{"pages":[{"title":"About","date":"2021-10-25T10:00:52.042Z","path":"about/index.html","text":"hello world"},{"title":"Categories","date":"2021-10-25T08:46:44.318Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2021-10-25T08:46:51.302Z","path":"tags/index.html","text":""}],"posts":[{"title":"Python数据处理-阅读笔记三——数据分析","date":"2021-12-16T07:58:56.459Z","path":"wiki/程序技术/Python/数据处理/python数据处理/Python数据处理-阅读笔记三/","text":"agate是Python中的一个数据分析库，这里将使用agate对数据进行分析。agate接受三个参数，表格数据，数据列标题，数据列类型。所以创建一个agate需要提供以上三个参数。例如：table = agate.Table(country_rows , titles, types)。其中country_rows就是数据，titles就是每列的标题，types就是每列的数据类型。 agate有如下几种数据类型：1234text_type = agate.Text()number_type = agate.Number()boolean_type = agate.Boolean()data_type = agate.Date()对于Excel，如果要获取列的数据类型，可以根据映射表进行获取，Excel的映射表为ctype_text。获取实例如下所示：1234567891011types = []for v in example_row: # 获取每列元素的类型 value_type = ctype_text[v.ctype] # 根据映射表获取类型 if value_type == &#x27;text&#x27;: types.append(text_type) elif value_type == &#x27;number&#x27;: types.append(number_type) elif value_type == &#x27;xldate&#x27;: types.append(data_type) else: # 其它类型都以文本类型处理 types.append(text_type) agate的常用方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#agate的内置方法#1. 获取表属性print(table.column_names) #获取列名称#2. 数据显示table.print_table(max_columns=7)table.print_json()table.print_csv()#3. 排序方法# 显示根据列Total (%) 最高值的前十个数据most_egregious = table.order_by(&#x27;Total (%)&#x27;,reverse=True).limit(10)for r in most_egregious: print(r)print(&#x27;The max rate in Female&#x27;)most_females = table.order_by(&#x27;Female&#x27;,reverse=True).limit(10)for r in most_females: print(r)#4. 条件搜索female_data = table.where(lambda r:r[&#x27;Female&#x27;] is not None)#通过where条件搜索most_females = female_data.order_by(&#x27;Female&#x27;,reverse=True).limit(10)for r in most_females: print(&#x27;&#123;&#125;: &#123;&#125;%&#x27;.format(r[&#x27;Countries and areas&#x27;], r[&#x27;Female&#x27;]))#查找,通过find方法,查找第一个匹配的数据has_por = table.where(lambda r:r[&#x27;Rural&#x27;] is not None)first_match = has_por.find(lambda x:x[&#x27;Rural&#x27;]&gt;50)print(first_match[&#x27;Countries and areas&#x27;])# 5. 统计学方法# 计算平均值的方法#agate.Mean() 统计学方法和列名称来返回列的数学均值。#has_por的作用（去除空列，计算剩下数据的平均值）has_por = table.where(lambda r:r[&#x27;Place of residence (%) Urban&#x27;] is not None)print(has_por.aggregate(agate.Mean(&#x27;Place of residence (%) Urban&#x27;)))#计算最大值print(has_por.aggregate(agate.Max(&#x27;Place of residence (%) Urban&#x27;)))#计算最小值print(has_por.aggregate(agate.Min(&#x27;Place of residence (%) Urban&#x27;)))#6. 计算新的排序列,计算排名# 通过compute和agate.Rank()方法#，compute 是一个非常好用的工具，它基于一个数据列（或多个数据列）来计算一个新的数据列。#这里创建新的表格，表格包含Total Child Labor Rank和Total (%)两个字段，根据Total (%)列进行排序ranked = table.compute([(&#x27;Total Child Labor Rank&#x27;,agate.Rank(&#x27;Total (%)&#x27;,reverse=True)),])for row in ranked.order_by(&#x27;Total (%)&#x27;,reverse=True).limit(20).rows: print(str(row[&#x27;Total (%)&#x27;])+&quot; &quot;+str(row[&#x27;Total Child Labor Rank&#x27;]))#自定义排序方法def reverse_percent(row): return 100-row[&#x27;Total (%)&#x27;]ranked = table.compute([(&#x27;Children not working (%)&#x27;,agate.Formula(number_type,reverse_percent)),])ranked = ranked.compute([(&#x27;Total Child Labor Rank&#x27;,agate.Rank(&#x27;Children not working (%)&#x27;)),])for row in ranked.order_by(&#x27;Total (%)&#x27;,reverse=True).limit(20).rows: print(str(row[&#x27;Total (%)&#x27;])+&quot; &quot;+str(row[&#x27;Total Child Labor Rank&#x27;]))","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"数据处理","slug":"数据处理","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python数据处理","slug":"Python数据处理","permalink":"http://example.com/categories/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"Python数据处理-阅读笔记二——标准化和脚本化","date":"2021-12-16T07:06:03.485Z","path":"wiki/程序技术/Python/数据处理/python数据处理/Python数据处理-阅读笔记二——标准化和脚本化/","text":"数据归一化和标准化数据归一化：归一化通常需要对数据集进行计算，使得数据都位于一个特定的范围。简单的说归一化达到的效果可以使得结果数据归一化到0-1区间。举个例子，要分析一个班级分数的分布情况，那么我们可以对成绩归一化，横坐标0-1，最高分100对应1，也就是说以(分数/100,得分人数)作为坐标，分析得分情况。 数据标准化： 借助数据归一化和标准化，能够使使用者确定数据的分布，明白该分布对后续研究或计算的含义。 数据标准化和归一化有时还需要删除离群值，这样你才能更好地发现数据的规律和分布。也就是说对于异常数据的去除。例如偶然因素出现的黑马，所以根据实际情况，剔除离群值的可以根据具体情况进行抉择。 数据清洗脚本化对于清洗代码具有固定规律，不太可能发生变化，则可以将整个清洗过程脚本化。 脚本化（scripting）的意思是，确定代码的结构，用于后续使用、学习和分享。Python 之禅不仅适用于编写代码，还适用于组织代码，函数、变量和类的命名，等等。最好在选择命名上花点时间，判断哪些名字可以让你和他人都一目了然。注释和文档可以帮助理解，但代码本身也应该具有较强的可读性。 通俗讲脚本化就是让代码能够长时间使用，过了若干时间后，依然奏效。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"数据处理","slug":"数据处理","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python数据处理","slug":"Python数据处理","permalink":"http://example.com/categories/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"Python数据处理-阅读笔记二——数据清洗之匹配与格式化","date":"2021-12-15T06:54:21.574Z","path":"wiki/程序技术/Python/数据处理/python数据处理/python数据处理-阅读笔记(三)/","text":"为什么要清洗数据数据清洗可以让数据更容易存储，搜索和复用。通常我们获取到的数据可能会存在格式不一致的问题，由其是当数据来源于多个数据集的时候，需要对数据进行格式化处理。 什么是数据清洗数据清洗也就是修改数据使其满足新的标准化数据格式。 数据清洗过程 简单的目视分析，观察文件的结构 观察数据字段，寻找不一致的地方。 数据格式化 将杂乱无章的数据转化为可读性较强的格式。 找出离群值和不良数据 离群值：也就是指和数据集不一致的数据。 判断数据的有效性，通过过数据的来源来判断数据的偏差。对于获取的数据集，检查数据是否有不一致的地方？也就是说数据值里面是否存在错误。可以通过过遍历数据集，判断数据类型是否匹配，是否缺失数据。 类型离群值（type outlier）：如果存在数据的类型不匹配，则出现了类型离群值。如果要判断数据集中的类型离群情况，则可以对对应的字段进行分析，然后统计各个类型出现的次数，用于分析比对。 重复值：重复的原始数据 对于重复的数据一般需要删除。删除的方法可以考虑如下方法： 通过set集合进行去重 模糊匹配 通过模糊匹配进行寻找和合并重复值。脏数据可以是用户输入时粗心所造成的拼写和语法错误。可以借助fuzzywuzzy库进行模糊匹配。 12345678910111213141516171819202122232425262728293031323334353637383940414243# 提供模糊匹配from fuzzywuzzy import fuzzfrom fuzzywuzzy import processmy_records = [&#123; &#x27;favorite_book&#x27;: &#x27;Grapes of Wrath&#x27;, &#x27;favorite_movie&#x27;: &#x27;Free Willie&#x27;, &#x27;favorite_show&#x27;: &#x27;Two Broke Girls&#x27;,&#125;, &#123; &#x27;favorite_book&#x27;: &#x27;The Grapes of Wrath&#x27;, &#x27;favorite_movie&#x27;: &#x27;Free Williy&#x27;, &#x27;favorite_show&#x27;: &#x27;2 Broke Girls&#x27;,&#125;]# 通过fuzz模块的ratio函数，接受两个字符串作比较，返回两个字符串序列的相似程度(1-100)。print(fuzz.ratio(my_records[0].get(&#x27;favorite_book&#x27;), my_records[1].get(&#x27;favorite_book&#x27;)))print(fuzz.ratio(my_records[0].get(&#x27;favorite_movie&#x27;), my_records[1].get(&#x27;favorite_movie&#x27;)))print(fuzz.ratio(my_records[0].get(&#x27;favorite_show&#x27;), my_records[1].get(&#x27;favorite_show&#x27;)))# 返回更加准确的匹配结果（根据子字符串进行匹配），建议采用第一种匹配方法print(fuzz.partial_ratio(my_records[0].get(&#x27;favorite_book&#x27;), my_records[1].get(&#x27;favorite_book&#x27;)))# token_sort_ratio函数，匹配字符串时，不考虑单词顺序。my_records = [&#123;&#x27;favorite_food&#x27;: &#x27;cheeseburgers with bacon&#x27;, &#x27;favorite_drink&#x27;: &#x27;wine, beer, and tequila&#x27;, &#x27;favorite_dessert&#x27;: &#x27;cheese or cake&#x27;, &#125;, &#123;&#x27;favorite_food&#x27;: &#x27;burgers with cheese and bacon&#x27;, &#x27;favorite_drink&#x27;: &#x27;beer, wine, and tequila&#x27;, &#x27;favorite_dessert&#x27;: &#x27;cheese cake&#x27;, &#125;]print(fuzz.token_sort_ratio(my_records[0].get(&#x27;favorite_food&#x27;), my_records[1].get(&#x27;favorite_food&#x27;)))print(fuzz.token_set_ratio(my_records[0].get(&#x27;favorite_food&#x27;), my_records[1].get(&#x27;favorite_food&#x27;)))#对于有限选项的数据的匹配,返回的是最有可能的选项列表#最佳匹配，通过extractone选择choice = [&#x27;Yes&#x27;,&#x27;No&#x27;,&#x27;Maybe&#x27;,&#x27;N/A&#x27;]print(process.extract(&#x27;ya&#x27;,choice,limit=2))print(process.extract(&#x27;ya&#x27;,choice))print(process.extract(&#x27;nope&#x27;,choice,limit=2))print(process.extract(&#x27;nope&#x27;,choice)) 正则匹配 \\w:匹配任意字母和数字和下划线。\\d:匹配数字。\\s:匹配任意一个空格。+:匹配一个或多个模式或字符。.:匹配.字符。*:匹配零个或多个字符或模式。|：匹配多个模式中的一个。[]或()：字符类和字符组。 1234567word = &#x27;\\w+&#x27;sentence = &#x27;Here is my sentence&#x27;print(re.findall(word,sentence)) #任意位置开始匹配search_result = re.search(word,sentence) #任意位置开始匹配print(search_result.group())match_result = re.match(word,sentence) #从字符串开头匹配print(match_result.group())","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"数据处理","slug":"数据处理","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python数据处理","slug":"Python数据处理","permalink":"http://example.com/categories/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"","date":"2021-12-15T06:34:25.387Z","path":"wiki/程序技术/Python/机器学习/深度学习/数据集网站/","text":"• UCI 机器学习数据集（http://archive.ics.uci.edu/ml/）","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"Python","slug":"程序技术/Python","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/"},{"name":"机器学习","slug":"程序技术/Python/机器学习","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"程序技术/Python/机器学习/深度学习","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Python数据处理-阅读笔记二——数据获取和存储","date":"2021-12-15T05:39:44.566Z","path":"wiki/程序技术/Python/数据处理/python数据处理-阅读笔记(二)/","text":"数据处理的先决条件就是找到要研究的数据集。 数据集因该具备如下几个性质。 可信性 找到的数据集因该是可靠的，可信的。 真实性 找到的数据因该是真实的。 可读性 数据容易获取。 数据寿命 了解数据更新的频率。 数据的获取方式 政府网站获取 如果需要分析研究某个城市或者某个国家的问题，可以通过政府网站寻找到相关的数据信息。例如：1234567• 开放城市项目（http://www.opencitiesproject.org/）• 开放尼泊尔（http://data.opennepal.net/）• 中国国家统计局（http://www.stats.gov.cn/english/）• 香港开放数据（https://opendatahk.com/）• 印尼政府开放数据（http://data.go.id/• 加拿大统计数据（http://www.rdc-cdr.ca/datasets-and-surveys）• 加拿大开放数据（http://open.canada.ca/en） 组织数据或非政府组织数据 如果需要获取某领域的数据，可以查看相关领域的组织机构是否提供了相应的数据。例如气候变化数据，国际商贸数据和全球运输数据等，可以通过国际组织进行信息数据的获取。例如：12345678• 联合国开放数据（http://data.un.org/）• 联合国发展计划署数据（http://open.undp.org/）• 开放知识基金会（https://okfn.org/）• 世界银行数据（http://data.worldbank.org/）• 维基解密（https://wikileaks.org/）• 国际援助透明度数据集（http://www.iatiregistry.org/）• DataHub（https://datahub.io/）• 人口资料局（http://www.prb.org/DataFinder.aspx） 教育数据和大学数据 通过世界各地大学和研究生部获取数据集。例如12345• Lexis Nexis（http://www.lexisnexis.com/）• 谷歌学术搜索（https://scholar.google.com/）• 康奈尔大学 arXiv 项目（http://arxiv.org/）• UCI 机器学习数据集（http://archive.ics.uci.edu/ml/）• 通用数据集倡议（http://www.commondataset.org/） 医学数据和科学数据 通过一些医学研究部门和组织可以获取优质的数据资源。123456789• 开放科学数据云（https://www.opensciencedatacloud.org/publicdata/）• 开放科学目录（http://www.opensciencedirectory.net/）• 世界卫生组织数据（http://www.who.int/gho/database/en/）• Broad 研究所开放数据（http://www.broadinstitute.org/scientific-community/data）• 人类连接组项目（神经通路映射）（http://www.humanconnectomeproject.org/）• UNC 精神病基因组协会（http://www.med.unc.edu/pgc/）• 社会科学数据集（http://3stages.org/idata/）• CDC 医学数据（http://www.cdc.gov/nchs/fastats/） 众包数据和API 一些社交网络和论坛能够提供大量的数据记录。 123456• Broad 研究所开放数据（http://www.broadinstitute.org/scientific-community/data）• 人类连接组项目（神经通路映射）（http://www.humanconnectomeproject.org/）• UNC 精神病基因组协会（http://www.med.unc.edu/pgc/）• 社会科学数据集（http://3stages.org/idata/）• CDC 医学数据（http://www.cdc.gov/nchs/fastats/） 数据的存储 关系数据库 MySQL和PostgreSQL 非关系数据库 NoSQL和MongoDB 云存储 通过云服务器进行数据存储 其他数据存储 层次型数据格式(HDF)：HDF是基于文件的可扩展数据解决方案，可将大型数据库快速存储至文件系统(本地或其他位置)。 Hadoop:Hadoop是一个大数据分布式存储系统，可以跨集群存储并处理数据。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"数据处理","slug":"数据处理","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python数据处理","slug":"Python数据处理","permalink":"http://example.com/categories/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"","date":"2021-12-15T02:55:56.559Z","path":"wiki/程序技术/BUG集锦/python/常见/","text":"xlrd读取xlsx文件失败,出现xlrd.biffh.XLRDError: Excel xlsx file； not supported错误原因是最近xlrd更新到了2.0版本，只支持.xls文件。所以读取xlsx需要下载旧版本在cmd中运行：pip uninstall xlrdpip install xlrd==1.2.0","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"BUG集锦","slug":"程序技术/BUG集锦","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"},{"name":"python","slug":"程序技术/BUG集锦/python","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/"}]},{"title":"Python数据处理-阅读笔记一——数据处理介绍","date":"2021-12-14T08:55:49.108Z","path":"wiki/程序技术/Python/数据处理/python数据处理/python数据处理-阅读笔记(一)/","text":"数据处理数据处理是指将杂乱的或未加工的数据源转换成有用的信息。先寻找原始数据源，并判断其价值：这些数据集的数据质量有多好？它们与你的目标是否相关？能否找到更好的数据源？在对数据进行解析与清洗后，数据集变得可用，这时你可以利用工具和方法（如Python 脚本）来帮你分析数据，并以报告的形式展示结果。这样你可以将无人问津的数据变得清晰可用。 数据处理的作用通过编程进行分析数据，得出结论，并将结论清楚的传达给别人。 数据处理的流程分析问题-&gt;收集数据，数据存储-&gt;数据清洗(研究，匹配，格式化，归一化和标准化，脚本化)-&gt;数据分析 常见格式化数据 CSV数据 CSV文件是指将数据列用逗号分隔的文件，文件的扩展名是.csv。1234567891011121314151617# CSV数据def csv(): import csv #读取csv文件 csvfile = open(&#x27;data-text.csv&#x27;,&#x27;rb&#x27;) reader = csv.reader(csvfile) # 以字典形式读取 reader = csv.DictReader(csvfile) for row in reader: print(row) #保存csv文件 header = [&#x27;colname&#x27;,&#x27;colname&#x27;] content = [(1,2),(2,3)] with open(&#x27;data-text.csv&#x27;,&#x27;w+&#x27;) as f: csvwrite = csv.writer(f) csvwrite.writerow(header) csvwrite.writerows(content) JSON数据 JSON数据是数据传输最常用的格式之一。1234567891011# JSON数据def JSON(): # JSON数据读取 json_data = open(&#x27;data-text.json&#x27;).read() data = json.loads(json_data) for item in data: print(item) #JSON数据保存 datas = [] with open(&#x27;data-text.json&#x27;,&#x27;w+&#x27;) as f: json.dump(datas,f) XML数据 XML 是一种标记语言，也就是说，它具有包含格式化数据的文档结构。XML 文档本质上只是格式特殊的数据文件。1234567891011121314#XML数据def XML(): #读取XML文件 from xml.etree import ElementTree as ET tree = ET.parse(&#x27;data-text.xml&#x27;) root = tree.getroot() data = root.find(&#x27;data&#x27;) all_data = [] for d in data: record = &#123;&#125; #获取标签属性 print(d.attrib.keys()[0]) print(d.attrib[&#x27;key&#x27;]) Excel文件 Python解析Excel文件使用的是xlrd库(读取Excel文件),xlwt(向Excel文件写入，并设置格式)。xlutils(一组Excel高级操作工具，需要安装xlrd和xlwt)。 Excel文件的读取，首先要确定在哪个表(sheet)。然后根据表可以获取表内的数据。 打开Excel文件book = xlrd.open_workbook()获取工作表对象sheet = book.sheet_by_name(表名称)获取数据 获取工作表的总行数print(sheet.nrows) 遍历工作表的每一行for i in range(sheet.nrows): print(sheet.row_values(i))实例代码：123456789101112131415161718192021222324#解析Excel文件def Excel(): import xlrd import xlwt import xlutils #读取xlsx数据 book = xlrd.open_workbook(&#x27;1.xlsx&#x27;) #Excel工作簿可以有多个标签(tab)或工作表(sheet) #所以需要找到包含目标数据的工作表 #获取工作表名称 for sheet in book.sheets(): print(sheet.name) #根据工作表名称，获取工作表对象 sheet = book.sheet_by_name(&#x27;Table 9 &#x27;) print(sheet) #获取工作表的总行数 print(sheet.nrows) #遍历工作表的每一行 for i in range(sheet.nrows): #print(sheet.row_values(i)) row = sheet.row_values(i) #读取列数据 for cell in row: print(cell) PDF文件 通过PyPDF2模块进行读取pdf，但是只能够读取文本。1234567891011121314def PDF(): import PyPDF2 #读取PDF pdffile = open(&#x27;&#x27;) pdfreader = PyPDF2.PdfFileReader(pdffile) print(&#x27;页数&#x27;+str(pdfreader.pages)) print(pdfreader.getPage(0))#获取第一页内容 print(pdfreader.getPage(0).extractText())#获取第二页内容 #写入PDF pdfwriter = PyPDF2.PdfFileWriter() pdfwriter.addPage(&#x27;adadad&#x27;) with open(&#x27;test.pdf&#x27;,&#x27;wb&#x27;) as wf: pdfwriter.write(wf)","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"数据处理","slug":"数据处理","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python数据处理","slug":"Python数据处理","permalink":"http://example.com/categories/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"}]},{"title":"CV2问题集锦","date":"2021-12-14T02:26:49.895Z","path":"wiki/程序技术/BUG集锦/python/cv2/","text":"1. 关于cv2.imshow不显示，一闪而过的问题我们在使用cv2显示图片的时候，一定要加上cv2.waitKey进行等待图片，否则会不显示。示例代码如下 1234567img = cv.imread(&quot;1.jpg&quot;) # 图像读取x,y = img.shape[:2] #获取图片的高和宽# 创建一个窗口cv.namedWindow(&#x27;test&#x27;,cv.WINDOW_NORMAL) # 通过cv.WINDOW_NORMAL可以完整显示图片cv.imshow(&#x27;test&#x27;,img) # 显示图片cv.waitKey(0) # 等待键盘输入，0表示无限等待，如果不调用waitKey窗口就会一闪而过，看不到任何图片cv.destroyAllWindows() # 销毁所有串口","tags":[{"name":"Bug","slug":"Bug","permalink":"http://example.com/tags/Bug/"}],"categories":[{"name":"Bug","slug":"Bug","permalink":"http://example.com/categories/Bug/"}]},{"title":"","date":"2021-12-14T00:54:21.517Z","path":"wiki/程序技术/BUG集锦/vsnode/","text":"","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"BUG集锦","slug":"程序技术/BUG集锦","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"}]},{"title":"","date":"2021-12-14T00:54:13.967Z","path":"wiki/程序技术/BUG集锦/idea/","text":"","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"BUG集锦","slug":"程序技术/BUG集锦","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"}]},{"title":"Pycharm问题集锦","date":"2021-12-14T00:54:06.688Z","path":"wiki/程序技术/BUG集锦/python/Pycharm/","text":"Pycharm误删代码如何恢复在误删代码的父文件夹上，点击鼠标右键，找到Local History，点击后即可找到之前的代码，根据情况进行恢复(Revert)。","tags":[{"name":"Bug","slug":"Bug","permalink":"http://example.com/tags/Bug/"}],"categories":[{"name":"Bug","slug":"Bug","permalink":"http://example.com/categories/Bug/"}]},{"title":"","date":"2021-12-14T00:53:54.425Z","path":"wiki/程序技术/BUG集锦/python/爬虫/","text":"","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"BUG集锦","slug":"程序技术/BUG集锦","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"},{"name":"python","slug":"程序技术/BUG集锦/python","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/"}]},{"title":"CV2的常用方法","date":"2021-12-13T07:33:05.449Z","path":"wiki/程序技术/Python/图像处理/CV2/","text":"1. 图片读取显示1234567img = cv.imread(&quot;1.jpg&quot;) # 图像读取x,y = img.shape[:2] #获取图片的高和宽# 创建一个窗口cv.namedWindow(&#x27;test&#x27;,cv.WINDOW_NORMAL) # 通过cv.WINDOW_NORMAL可以完整显示图片cv.imshow(&#x27;test&#x27;,img) # 显示图片cv.waitKey(0) # 等待键盘输入，0表示无限等待，如果不调用waitKey窗口就会一闪而过，看不到任何图片cv.destroyAllWindows() # 销毁所有串口 2. 图片的操作-修改图片大小通过cv2.resize可以修改图片的大小，指定尺寸size（宽，高）,并且可以指定插入方式，不同插入方式得到的缩放图片的效果不一样。 123456789101112131415161718192021222324252627img = cv.imread(&quot;1.jpg&quot;)x, y = img.shape[:2] # 获取图片的高和宽# 输出尺寸为(宽，高)&quot;&quot;&quot;cv2.resize(InputArray src, OutputArray dst, Size, fx, fy, interpolation)InputArray src： 输入图片InputArray src: 输出图片Size: 输出图片尺寸(宽，高)fx,fy:沿x轴，y轴的缩放系数interpolation:插入方式插入方式有如下几种：INTER_NEAREST：最近邻插值INTER_LINEAR：双线性插值（默认设置）INTER_AREA:使用像素区域关系进行重采样。INTER_CUBIC:4x4像素邻域的双三次插值INTER_LANCZOS4:8x8像素邻域的Lanczos插值&quot;&quot;&quot;img1 = cv.resize(img,(int(y/4),int(x/4))) # 修改图片大小，缩放为原来的1/4#cv.namedWindow(&#x27;test&#x27;,cv.WINDOW_NORMAL)cv.imshow(&#x27;test&#x27;,img1)cv.waitKey(0)cv.destroyAllWindows()img2 = cv.resize(img,(0,0),fx=0.25,fy=0.25,interpolation=cv.INTER_AREA)cv.imshow(&#x27;test&#x27;, img2)cv.waitKey(0)cv.destroyAllWindows() 3. 图片的操作-灰度化123456img = cv.imread(&quot;1.jpg&quot;)img_gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 灰度化cv.namedWindow(&#x27;test&#x27;, cv.WINDOW_NORMAL)cv.imshow(&#x27;test&#x27;, img_gray)cv.waitKey(0)cv.destroyAllWindows()","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"图像处理","slug":"图像处理","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}]},{"title":"滑块验证码破解","date":"2021-12-13T07:01:04.559Z","path":"wiki/程序技术/Python/爬虫/滑块破解/","text":"滑块验证码破解最近在爬虫开发的过程中，遇到了关于滑块验证码，需要进行滑块验证码破解。这里涉及到图像方面的技术，可以借助OpenCV进行解决。 通过CV2解决滑块验证这里通过CV2库进行滑块验证的解决。简单介绍一下滑块验证的几个步骤。 1. 获取图片（不带缺口的图片，带缺口的图片）根据网页，获取到滑块图片，一般来说分为两个图片，一个是缺口图，也就是缺少缺口的图片。一个是滑块图，也就是缺口图缺少的图片。假设缺口图为img1,滑块图为img2。img1: img2: 2. 识别缺口位置，计算滑动距离一般而言，我们通过网页获取到的图片和实际在网页上显示的图片大小是不一致的。读者可以根据自己需要破解的滑块验证码进行校验。查看在网页中的图片大小和下载的图片大小是否一致。如果不一致，那么需要跳转下载的图片的大小。读取图片，然后跳转图片的大小为网页中显示的大小。123456img1 = cv2.imread(缺口图的路径)img1 = cv2.resize(img1, (网页缺口图实际宽度, 网页缺口图实际高度))img2 = cv2.imread(缺块图的路径)img2 = cv2.resize(img2, (网页缺块图实际宽度, 网页缺块图实际高度)) 如果要识别缺口位置，可以借助cv2库的matchTemplate方法来获取缺块距离缺口的位置。实现代码如下所示，一般来说这种方法识别会带有误差，也就是存在失败的情况，这个时候可以在程序中进行设置，进行多次尝试解决滑块验证(可以采用循环处理的办法，重复多次滑块验证解决流程)，以达到解决滑块问题的效果。12345img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) # 对图片灰度化处理img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)# 对图片灰度化处理res = cv2.matchTemplate(img1_gray, img2_gray, cv2.TM_CCOEFF_NORMED) value = cv2.minMaxLoc(res) #获取缺口和缺块的距离value = value[3][0] #value值就是缺块距离缺口的距离 3. 模拟运动我们可以借助selenium进行模拟滑块拖动的过程，但是在模拟滑块的拖动过程中，需要模拟人的速度。人在拖动滑块的时候，正常是先快后慢的过程，这个时候我们可以设置一个先快后慢的过程（可以设置两个运动方程，一段加速度为a1,一段加速度为a2），用于模拟人拖动滑块。示例代码如下所示： 123456789101112131415161718192021222324252627282930# ditance为移动的距离，也就是上面获得的valuedef getTrack(self,distance): #用于存储每个过程中，拉动滑块的距离 track = [] # 当前位移 current = 0 # 当距离超过什么时候，进行减速 mid = distance * 4 / 5 # 计算间隔 t = 0.2 # 初速度 v = 0 while current &lt; distance: if current &lt; mid: # 加速度为正2 a = 2 else: # 加速度为负3 a = -3 # 初速度v0 v0 = v # 当前速度v = v0 + at v = v0 + a * t # 移动距离x = v0t + 1/2 * a * t^2 move = v0 * t + 1 / 2 * a * t * t # 当前位移 current += move # 加入轨迹 track.append(round(move)) return track 通过selenium模拟运动，可以借助ActionChains来拖拽滑块。首先我们需要获取到滑块元素。这个我们可以通过F12来查看拖拽滑块元素的属性，然后通过driver获取到元素。12345ActionChains(driver).click_and_hold(拖拽滑块的元素).perform() # 点击并且不释放鼠标for x in track: # 根据轨迹，移动元素 ActionChains(driver).move_by_offset(xoffset=x, yoffset=0).perform() #移动元素time.sleep(0.5) ActionChains(driver).release().perform() #释放元素","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Web项目部署","date":"2021-12-10T09:31:23.397Z","path":"wiki/程序技术/Java/JavaWeb/项目部署/","text":"安装Tomcat 通过 tar -zxvf 解压tomcat。 修改catalina.sh，在脚本开头增加export JAVA_HOME指定jdk路径。 修改Tomcat的端口：将8080，8005，8009修改其它端口即可。 下载jdk 下载jdk，然后通过tar -zxvf 进行解压jdk 打包项目 将项目打包war包，然后放入到tomcat的webapp目录下即可。 访问路径 将War包包放到webapp下之后，访问路径的名称就是war包的名称，假设war包为AAA.war,部署端口为8080.ip地址为xxx.xxx.xxx则访问路径为：https://xxx.xxx.xxx/AAA。 报错解决 Cannot find /usr/tomcat/tomcat9/bin/setclasspath.sh执行：unset CATALINA_HOME","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://example.com/tags/JavaWeb/"}],"categories":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://example.com/categories/JavaWeb/"}]},{"title":"反爬技巧","date":"2021-12-10T08:19:06.344Z","path":"wiki/程序技术/Python/爬虫/反爬技巧/","text":"简介这里主要总结了一些爬虫开发过程中的反爬技巧。 正确设置headers 通常我们简单设置一下User-Agent就能够获取到网页内容。但是对于一些网站，通过request获取到的网页内容，通常又和正常访问网页获取到的内容不一致。这里就需要根据网页的Headers来设置request内的headers属性，用于避免被检测。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"debug反爬","date":"2021-12-10T07:01:24.965Z","path":"wiki/程序技术/Python/爬虫/debug反爬/","text":"关于解决(function anonymous() {debugger})的问题1. 实现原理如何实现无限debugger呢？实现无限debugger就是不断的打断你，页面跳转到source页面，阻止你看内容。写一个不断调用debugger即可。123(function() &#123;var a = new Date(); debugger; return new Date() - a &gt; 100;&#125;()) 2. 问题解决","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python爬虫-Scrapy进阶","date":"2021-12-09T09:00:55.657Z","path":"wiki/程序技术/Python/爬虫/Python爬虫-Scrapy进阶/","text":"1.Spider模块2.Item Loader3.请求与响应深入Scrapy爬虫框架1. Spider模块Spider模块是定义爬虫的动作及分析网页结构的地方，我们容易看出，在这里给出了解析网页获取元素，并进行是否继续爬取下一个网页的操作(也就是爬虫的动作)。Spider的执行流程 从入口URL初始化Request并设置回调函数。这个Reuquest下载完毕返回Response，并作为参数传送给回调函数，Spider初始的Request是通过调用start_requests()方法获取。start_requests()读取start_urls中的URL，并以parse为回调函数生成Request。也就是说初始的URL，只需要在start_urls加入，系统会自动的获取response，并以parse()为解析函数。 在回调函数分析Response，返回Item对象，dict,ruquest或者一个包括三者的可迭代容器。其中返回的Request对象会经过Scrapy处理，下载相应内容，并调用设置相应的解析函数。例如： 12request = scrapy.Request(url=url,callback=self.parse_body) #调用Request方法，并设置解析函数 在解析函数内，可以使用页面解析技术，对页面元素进行解析，可以用BeautifuleSoup等等技术。通过response可以获取到响应的内容。将分析的数据生成item 由spider返回item,可以经过Item Pipeline被存到数据库或使用Feed exports存入到文件中。 Spider类的成员变量123456789101112131415161718192021222324&#x27;&#x27;&#x27; @:param name 定义spider名字的字符串，名字必须唯一。可以生成多个相同的spider实例 通常可以用网站域名命名spider @:param allowed_domains: 包含了spder允许爬取的域名列表。 当OffsiteMiddleware组件启用时，域名不在列表中的URL不会被跟进。 @:param statr_urls:URL列表，当没有配置statr_requests9）f方法的时候，spider会从该列表开始进行爬取。也就是说爬虫开始爬取的 URL就是从start_urls中获取。 @:param custom_setting：该设置是一个dict,当启动spider时，该设置将会覆盖项目级的设置。也就是 说可以在这里对spider单独定义。 @:param crawler 该属性在初始化class后，由类方法from_crawler()设置。并且链接了 本spider实例羽Crawler对象。 &#x27;&#x27;&#x27; name = &#x27;myspider&#x27; allowed_domains = [&quot;www.baidu.com&quot;] start_urls = [ &quot;https://www.baidu.com&quot; ] custom_settings = &#123;&#125; crawler = &quot;&quot; Spider类的方法123456789101112131415161718192021222324252627282930313233343536373839# 常用的Spider方法# 该方法必须返回一个可迭代对象，对象包含spider用于爬虫的第一个request。# 也就是说 start_requests是项目启动的开始，是根据start_url作为项目启动URL# 如果没有设置start_requests方法，就会默认从start_urls的url生成Request。# 如果需要定制最初爬取的Request对象，可以重写方法。# 例如通过POST登录# 总结来说：strt_request就是整个程序的入口，如果不指定就是直接从start_ruls中获取url，以parse()为回调函数进行解析。def start_requests(self): return [scrapy.FormRequest(&quot;http://www.example.com/login&quot;,formdata=&#123; &#x27;user&#x27;:&#x27;john&#x27;,&#x27;pass&#x27;:&#x27;secret&#x27; &#125;,callback=self.login)]# start_requests对url请求后的响应，会通过login进行处理def login(self,response): passdef make_requests_from_url(self, url): &#x27;&#x27;&#x27; 接受一个URL并返回用于爬取的Request对象 :param url: :return: &#x27;&#x27;&#x27; passdef parse(self, response, **kwargs): &#x27;&#x27;&#x27; 用于解析网页内容，一般作为初始URL解析的回调函数 :param response: :param kwargs: :return: &#x27;&#x27;&#x27;def close(spider, reason): &#x27;&#x27;&#x27; 当Spider关闭时，该函数被调用。可以用来在spider关闭时，释放占用的资源。 :param reason: :return: &#x27;&#x27;&#x27; Scrapy除了Spider类作为基类进行扩展，还提供了CrawlSpider，XMLFeedSpider,CSVFeedSpider和SitemapSpider等类来实现不同的爬虫任务。 CrawlSpiderCrawlSpider类常用于爬取一般的网站。其中定义了一些规则(rule)来提供跟进链接功能。CrawlSpider提供了新的属性rules。rules包含一个或多个Rule对象的集合。每个Rule对爬取网站的动作定义了特定的规则。如果多个Rule匹配相同的链接，则先定义的被调用。CrawlSpider提供的初始URL解析方法，parse_start_url(response)。该方法返回一个Item对象或者一个Request对象或者包含二者的对象。使用示例如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243class MyCrawlSpider(CrawlSpider): name = &#x27;crawlSpider&#x27; allowed_domains = [&quot;cnblogs.com&quot;]#域名 start_urls = [ &quot;http://www.cnblogs.com/qiyeboy/default.html?page=1&quot; ] # Rule原型 # scrapy.contrib.spiders.Rule(link_exactor,callback=None,cb_kwargs=None, # follow=None,process_links=None,process_request=None) rules = ( Rule(LinkExtractor(allow=(&quot;/qiyeboy/default.html\\?page=\\d&#123;1,&#125;&quot;,)), follow=True, callback=&#x27;parse_item&#x27; ), ) # LinkExtractor对象的构造 # allow: 用于匹配满足正则表达式的链接 # deny: 排除正则表达式匹配的链接，优先级高于allow # allow_domains：允许的域名，可以是list或str # deny_domains:排除的域名 # restrict_xpaths:提取满足Xpath选择条件的链接。 # restrict_css:xxxCSSxxx的链接 # tags: 提取指定标签下的链接。 # unique:链接是否去重 # process_value:值处理函数。 def parse(self, response, **kwargs): papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;) for paper in papers: url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0] title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0] content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] item = MyCrawlSpider(url=url, title=title, time=time, content=content) request = scrapy.Request(url=url, callback=self.parse_body) # 调用Request方法，并设置解析函数 request.meta[&#x27;item&#x27;] = item # 将item暂存 yield request def parse_body(self,response): item = response.meta[&#x27;item&#x27;] body = response.xpath(&quot;.//*[@class=&#x27;postBody&#x27;]&quot;) item[&#x27;cimage_urls&#x27;] = body.xpath(&#x27;.//img//@src&#x27;).extract()# 提取图片链接 yield item XMLFeedSpiderXMLFeedSpider被设计用于通过迭代各个节点来分析XML源。迭代器可以从Iternodes,XML,HTML中选择。在XMLFeedSpider中，需要定义下列类属性来设置迭代器及标记名称。 iterator 用于确定使用哪个迭代器string,默认为iternodes，可选项有(1. iternodes, 2. html , 3. html) itertag itertag为一个包含开始迭代的节点名string namespaces 称为命名空间，由(prefix,url),元组(tuple)所组成的list。这里定义了在文档中会被spider处理可用的namespace,prefix和url会被自动调用。由register_namespace()方法生成namespace。 示例代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839class MyXMLFeedSpider(XMLFeedSpider): name = &quot;myxmlfeed&quot; allowed_domains = [&#x27;cnblogs.com&#x27;] start_urls = [&quot;https://feed.cnblogs.com/blog/u/269038/rss&quot;] namespaces = [&#123;&#x27;n&#x27;,&#x27;http://www.sitemaps.org/schemas/sitemap/0.9&#x27;&#125;] iterator = &#x27;html&#x27; # 用于定义解析方式 itertag = &#x27;entry&#x27; #XMLFeedSpider方法 def adapt_response(self, response): &#x27;&#x27;&#x27; 这个方法在页面解析前和页面下载后之间被调用。可以用于修改Response内容，并再返回。 :param response: :return: &#x27;&#x27;&#x27; return response def parse_node(self, response, selector): &#x27;&#x27;&#x27; 当节点符合itertag时，该方法被调用。接收到的response以及相对应的Selector作为参数传递给该方法。 需要返回一个Item对象或Request对象，或包含二者的可迭代对象 :param response: :param selector: :return: &#x27;&#x27;&#x27; print(selector.xpath(&#x27;id/text()&#x27;).extract()[0]) print(selector.xpath(&#x27;title/text()&#x27;).extract()[0]) print(selector.xpath(&#x27;summary/text()&#x27;).extract()[0]) def process_results(self, response, results): &#x27;&#x27;&#x27; 在页面解析后，数据返回前进行处理。主要是对返回数据的最后处理。修改Item的内容 :param response: :param results: :return: &#x27;&#x27;&#x27; return [response,results] 2. Item Loader模块 Item Loader是什么？ Item Loader提供了一种边界的方式填充抓到的Items。Item Loader可以直接对Item分析，并提取出想要的数据保存到容器中，而Item则是机械的根据键值对对应，返回数据。所以Item Loader更加灵活，高效。 Item Loader负责数据的收集，处理和填充。Item Loader包含两个重要的组件：输入处理器(input processors)和输出处理器(output processors)。 Item Loader的每个字段都包含了一个输入处理器和输出处理器。 输入处理器接收到response后，通过add_xpath,add_css,add_value等方法提取数据，并将数据保存到Item Loader中。 收集完成数据之后，通过ItemLoader.load_item()方法来填充并返回Item对象。load_item()方法内部先调用输出处理器来处理收集到的数据，结果保存到最终的Item中。 Item Loader使用方法 在Item中声明输入输出处理器12345678910111213141516171819# 在Item中声明输入和输出处理器def filter_price(value): if value.isdigit(): return valueclass Product(scrapy.Item): name = scrapy.Field( input_processor=MapCompose(remove_tags), output_processor=Join(), ) price = scrapy.Field( input_processor=MapCompose(remove_tags,filter_price), output_processor=TakeFirst(), ) stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) 在Item Loader类中声明类似field_in和field_out的属性。 123456789101112131415# ItemLoaderclass ProductLoadr(ItemLoader): default_output_processor = TakeFirst() # 声明输入输出处理器 #输入处理器 name_in = MapCompose(unicode.title) #输出处理器 name_out = Join() price_in = MapCompose(unicode.price) price_out = Join() stock_in = MapCompose(unicode.stock) stock_out = Join() last_updated_in = MapCompose(unicode.last_updated) last_updated_out = Join() Item Loader Context Item Loader Context是一个任意的键值对字典。能够被Item Loader中的输入输出处理器所共享。可以用于调整输入输出处理器的行为。使用Item Loader Context就是为了能够提高代码复用，便于扩展。如下代码所示，可以在原有的Item Loader基础上，对属性进行解析。如果对于不同的解析，只需要设置解析方法即可，增加了可复用性。就不需要再单独设置一个Loader ，可以复用Loader。 对于输入处理器，我们可以借助方法进行扩展。对于输出处理器，通常是再Item字段元数据进行声明。 123456789101112# Item Loader Context# 通过接收loader_context,能告诉Item loader自己能够接收Item Loader context ,# 所以方法被调用的时候能将当前的active Context传递给该方法def parse_length(text, loader_context): unit = loader_context.get(&#x27;unit&#x27;, &#x27;m&#x27;) # 获取长度 parsed_length = len(unit) return parsed_lengthclass ProductLoadr(ProductLoader): # 定义修改context length_out = MapCompose(parse_length, unit=&#x27;cm&#x27;) Item Loader 内置的处理器 1. MapCompose输入处理器，将多个方法的执行结果按顺序组合产出最终的输出。1234567891011#MapCompose#和Compose类似，但是这个方法对输入是：每个元素单独传入第一个函数处理，然后将结果又作为#整个迭代对象传入到后面的函数进行处理。#如果输入是None会被自动忽略#相当于这个方法会对每个元素单独处理def filter_world(x): return None if x ==&#x27;world&#x27; else xproc = MapCompose(filter_world,unicode.upper)print(proc([u&#x27;hello&#x27;,u&#x27;world&#x27;,u&#x27;this&#x27;,u&#x27;is&#x27;,u&#x27;scrapy&#x27;])) 2. Identity最简单的处理器，不做任何处理，直接返回原来的数据，无参数。123#Identity ：简单的处理器，不做任何处理，直接返回原来的数据identity = Identity()print(identity([&quot;1231&quot;,&quot;321&quot;])) 3. TakeFirst返回第一个非空值，常用于单值字段的输出处理器，无参数。1234# TakeFirst:输出处理器# TakeFirst用于返回第一个非空值，常用于单值字段的输出处理。proc = TakeFirst()print(proc([&#x27;&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;])) 4. Compose123456#Compose#用于将给定的多个方法的组合构造处理器#每个输入值传递到第一个方法，然后将结果传递到第二个方法以此类推，最后一个方法的返回值作为输出#如果需要当遇到None值时候停止处理，可以通过传递stop_on_one=False设定。proc = Compose(lambda v:v[0],str.upper)print(proc([&#x27;hello&#x27;,&#x27;world&#x27;])) 5. Join12345#Join#返回用分隔符separator连接后的值，separator默认为空格，不接受loader contexts/proc = Join()print(proc([&#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;])) 6.SelectJmes指定json_path查询并返回值，需要jmespath的支持，每次只接收一个输入。1234#SelectJmes#指定json_path查询并返回值，需要jmespath的支持proc = SelectJmes(&quot;foo&quot;)print(proc(&#123;&#x27;foo&#x27;:&#x27;bar&#x27;&#125;)) 3.Item Pipeline模块完整的Item Pipeline的demo:1234567891011121314151617181920212223242526272829# 完整的Item Pipelineclass MongoPipeline(object): coolection_name = &#x27;scrapy_items&#x27; def __init__(self,mongo_uri,mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db #crawler是一个Crawler对象 #从Crawl属性中，创建一个pipeline示例，Crawler对象能够接触所有Scrapy的核心组件如seetings，singnals。 @classmethod def from_crawler(cls,crawler): return cls( mongo_uri=crawler.settings.get(&#x27;MONGO_URI&#x27;), mongo_db=crawler.settings.get(&#x27;MONGO_DATABASE&#x27;,&#x27;items&#x27;) ) # 参数spider表示一个Spider对象，表示被开启的Spider # 当spider被开启的时候，这个方法会被调用 def open_spider(self,spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] #当spider被关闭的时候，这个方法会被调用 def close_spider(self,spider): self.client.close() def process_item(self,item,spider): self.db[self.coolection_name].insert(dict(item)) return item 3. 请求与响应 1. Request对象 Request对象 Request对象相当于一个HTTP请求。通常在Spider产生，传递给下载器，最后返回一个响应。Request(url[,callback,method=’GET’,headers,body,cookies,meta,encoding=’utf-8’,priority=0,dont_filter=False,errback])。url:请求链接。callback:指定用于解析请求响应的方法，如果没有指定，默认使用spider的parse方法。method:HTTP请求方式。body:请求的body。headers:请求头。cookies（dict or list）：请求的cookie信息。encoding:请求的编码。priority:请求的优先级，默认为0。dont_filter:标明该请求不应由调度器过滤，适用于多次执行相同请求。errback:如果请求出现异常，该方法将被调用。meta的参数如下所示：示例代码：123456789101112request_with_cookies = scrapy.Request(url=&#x27;http://www.example.com&#x27;, cookies=&#123;&#x27;currency&#x27;:&#x27;USD&#x27;,&#x27;country&#x27;:&#x27;UY&#x27;&#125;)#使用字典列表发送request_with_cookies = scrapy.Request(url=&#x27;http://www.example.com&#x27;, cookies=[&#123;&#x27;name&#x27;:&#x27;currency&#x27;,&#x27;value&#x27;:&#x27;USD&#x27;, &#x27;domain&#x27;:&#x27;example.com&#x27;,&#x27;path&#x27;:&#x27;/currency&#x27;&#125;])#meta(dont_merge_cookies属性)可以用于当请求发送后，不将返回的cookie信息和现有cookie合并request_with_cookies = scrapy.Request(url=&#x27;http://www.example.com&#x27;, cookies=[&#123;&#x27;name&#x27;: &#x27;currency&#x27;, &#x27;value&#x27;: &#x27;USD&#x27;, &#x27;domain&#x27;: &#x27;example.com&#x27;, &#x27;path&#x27;: &#x27;/currency&#x27;&#125;], meta=&#123;&#x27;dont_merge_cookies&#x27;:True&#125;) FormRequest 123456789101112#FromRequest提供了一个类方法from_response#from_response(response[,formname=None,formnumber=0,formdata=None,formxpath=None,# clickdata=None,dont_click=False])# response: 一个包含HTML表单的响应页面，formname:表单的name属性，formnumber:用于指定第几个表单# formdata(dict):用于填充表单中属性的值# formxpath:通过xpath定位表单，第一个匹配的将会被操作# 示例scrapy.FormRequest.from_response( response, formdata=&#123;&#x27;username&#x27;: &#x27;john&#x27;, &#x27;password&#x27;: &#x27;secret&#x27;&#125;, callback=self.after_login) 使用实例如下所示：12345678910111213141516class LoginSpider(scrapy.Spider): name = &#x27;example.com&#x27; start_url = [&#x27;http://www.example.com/users/login.php&#x27;] def parse(self, response, **kwargs): return scrapy.FormRequest.from_response( response, formdata=&#123;&#x27;username&#x27;:&#x27;john&#x27;,&#x27;password&#x27;:&#x27;secret&#x27;&#125;, callback=self.after_login ) def after_login(self,response): if &quot;authentication failed&quot; in response.body: self.logger.error(&quot;Login failed&quot;) return","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python常见问题","date":"2021-12-08T06:36:41.000Z","path":"wiki/程序技术/Python/常见问题/","text":"Python：常遇见的字符编码问题 TypeError: a bytes-like object is required, not ‘str’需要将写入的数据进行编码转换(通过encode转化)。例如:1file.write(line.encode(&#x27;utf-8&#x27;))","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"}],"categories":[{"name":"Python","slug":"Python","permalink":"http://example.com/categories/Python/"}]},{"title":"JavaWeb后端数据导出","date":"2021-12-06T07:07:34.071Z","path":"wiki/程序技术/Java/JavaWeb/数据导出/","text":"简介我们在进行页面开发的过程中，经常需要获取数据，以及下载数据。通常从页面下载数据有多种方式，可以生成PDF下载，可以生成Excel表格下载。这里介绍这几种数据下载方式。 PDF下载Excel下载Java比较常用的Excel导入和导出技术有两种，Jakarta POI和Java Excel。Jakarta POI 是一套用于访问微软格式文档的Java API。Jakarta POI有不少组件组成，其中有用于操做Excel格式文件的HSSF和用于操做Word的HWPF，在各类组件中目前只有用于操做Excel的HSSF相对成熟。官方主页http://poi.apache.org/index.html，API文档http://poi.apache.org/apidocs/index.htmlapi。这里主要介绍Jakarta POI的用法。 首先是maven依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi&lt;/artifactId&gt; &lt;version&gt;3.17&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;3.17&lt;/version&gt;&lt;/dependency&gt;","tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JavaWeb","slug":"JavaWeb","permalink":"http://example.com/tags/JavaWeb/"}],"categories":[{"name":"JavaEE","slug":"JavaEE","permalink":"http://example.com/categories/JavaEE/"}]},{"title":"Java中对JSON的操作","date":"2021-12-01T06:56:29.125Z","path":"wiki/程序技术/Java/JAVA-Json/","text":"JSON格式的数据在Web开发中经常作为数据请求格式或数据响应格式，所以对JSON格式的数据的操作十分重要。 常见的JSON操作主要有以下几种方式，JSON对象转化JSON字符串，JSON字符串转化JSON对象，普通数据对象转换JSON字符串，普通数据对象转换JSON对象。 常见的JSON第三方库有fastjson,gson等。这里主要以fastjson作为介绍。","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"Java","slug":"程序技术/Java","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Java/"}]},{"title":"抖店开发--订单导出之信息获取","date":"2021-12-01T04:38:34.647Z","path":"wiki/抖店API开发/订单导出/","text":"我们可以通过订单列表查询API获取到店铺的订单列表。根据需要获取的信息和抖店API查询到的结果，做如下表格，用于获取对应参数。响应格式如下所示： 名称 获取方式 获取结果 主订单编号 order_id 可获取 选购商品 product_name 可获取 商品规格 order_type_desc 可获取 订单应付金额 order_amount 可获取 收件人 encrypt_post_receiver 加密 收件人手机号 encrypt_post_tel 加密 详细地址 post_addr省，市，区县，街道可获取，详细地址（encrypt_detail） 部分加密 买家留言 buyer_words 可获取 订单提交时间 create_time 可获取 订单状态 order_status 可获取 承若发货时间 appointment_ship_time 可获取 商家备注 seller_words 可获取 身份证姓名 user_id_info.encrypt_id_card_name 加密 身份证号 user_id_info.encrypt_id_card_no 加密","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"}],"categories":[{"name":"抖店","slug":"抖店","permalink":"http://example.com/categories/%E6%8A%96%E5%BA%97/"}]},{"title":"","date":"2021-11-26T01:21:47.277Z","path":"wiki/程序技术/Python/爬虫/Untitled-1/","text":"< div class = \"v-image__image v-image__image--cover\" style = \"background-image: url(&quot;https://doge.zzzmh.cn/wallpaper/origin/a58cbbffd2aa49c1b1e99990be912f30.jpg/thumbs?auth_key=1642176000-d1e0a42b4ad44111e5c25911da43d23e-0-3cd3282f8ac059d4a3573b0104c631c3&quot;); background-position: center center;\" > < /div> < a href = \"https://doge.zzzmh.cn/wallpaper/origin/a58cbbffd2aa49c1b1e99990be912f30.jpg?response-content-disposition=attachment&amp;auth_key=1642176000-d1e0a42b4ad44111e5c25911da43d23e-0-0c38988dbe7d8b1203a85814583d2315\" > < div title = \"下载\" class = \"tool\" > < i aria - hidden = \"true\" class = \"v-icon notranslate mdi mdi-download theme--light\" style = \"color: rgb(47, 146, 150); caret-color: rgb(47, 146, 150);\" > < /i> < /a>","tags":[],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"Python","slug":"程序技术/Python","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/"},{"name":"爬虫","slug":"程序技术/Python/爬虫","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/"}]},{"title":"selenium+Chrome(79版本以上)反爬","date":"2021-11-25T08:16:51.544Z","path":"wiki/程序技术/Python/爬虫/selenium反爬/","text":"简介&nbsp;&nbsp;&nbsp;最近在学习爬虫的过程中碰到一个奇怪的现象，当我在正常的浏览器页面访问网站的时候，能够正常访问到网页的数据。然而，当我通过selenium进行爬取网页数据的时候，出现服务器异常的提醒。正常访问能够访问，而通过selenium访问的时候却报错，这是为什么呢？通过查阅相关资料，可以得出，碰上反爬虫了。 分析 为什么可以得出碰上反爬虫了呢？通常我们通过selenium进行爬取网页的时候，可以通过 window.navigator.webdriver检测是否使用了webdriver。我们可以试一下。在正常访问的网页中输入 window.navigator.webdriver，通常返回的是false或undifine，而当我们通过selenium访问浏览器的时候，在网页控制台输入 window.navigator.webdriver，会返回true。假如我是网页的设计者，我就会先在网页加载的时候写下这么一行代码 : 12345if (window.navigator.webdriver)&#123; alert(&quot;爬虫爬的好，牢饭吃到饱&quot;)&#125;else&#123; alert(&quot;正常页面&quot;)&#125; 所以，当我们爬取网页出现上述情况的时候，很大可能浏览器存在反selenium。下面介绍一下如何解决反爬，常见的反反爬方案包含：设置参数 excludeSwitches、mitmproxy 拦截过滤、cdp 命令。 2. 解决方案&nbsp;&nbsp;&nbsp;&nbsp;要想解决上述的问题，我们可以让window.navigator.webdriver返回false即可。12345678options = webdriver.ChromeOptions()options.add_experimental_option(&#x27;excludeSwitches&#x27;, [&#x27;enable-automation&#x27;])driver = webdriver.Chrome(executable_path=&#x27;E:\\codeEverment\\python\\chromedriver.exe&#x27;, options=options) # chrome驱动 script = &quot;Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123; get: () =&gt; undefined&#125;)&quot; driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;&quot;source&quot;: script&#125;)要见检查是否避免浏览器对webdriver的检测，可以通过selenium访问https://intoli.com/blog/not-possible-to-block-chrome-headless/chrome-headless-test.html,如果页面显示全绿，那么就表明避免成功，反之失败。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python爬虫-中级","date":"2021-11-22T06:44:03.511Z","path":"wiki/程序技术/Python/爬虫/Python爬虫-中级/","text":"1.数据存储2.动态文件抓取3.Web端协议分析4.数据存储1.数据存储1. 数据存储 前面介绍了关于数据存储的csv,txt,json方式，这里介绍如何采用数据库保存数据，主要是了解两个数据库，关系数据库和分布式数据库。即MySQL和MongoDB。 1.1 MySQL Python对MySQL的操作通过pymsql模块支持。Python操作MySQL的代码如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&quot;&quot;&quot; Python对MySQL的操作主要是由pymysql模块进行支持。&quot;&quot;&quot;# 主机ip(host),用户名(user),密码(passwd),数据库名称(db),端口(port),编码(charset)# 打开数据库， pymysql.connect(host=,user=,passwd=,db=,port=,charset=)db = pymysql.connect( host=&#x27;localhost&#x27;, user=&#x27;test&#x27;, passwd=&#x27;123456&#x27;, db=&#x27;votemsy&#x27;, charset=&#x27;utf-8&#x27;)# 连接数据库成功后就可以操作数据库# 创建一个游标对象# 游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果# 游标对象支持的数据库的操作cursor = db.cursor()#需要执行的sql语句sql = &quot;&quot;try: #执行一条SQL语句 cursor.execute(sql) #执行多条SQL语句 cursor.executemany(sql) #用来从结果中取一条记录，并将游标指向下一条记录 result = cursor.fetchone() #用来从结果中取多条记录 result = cursor.fetchmany(5) #获取所有记录列表 results = cursor.fetchall() # 插入数据 data = &quot;&#x27;qiye&#x27;,20&quot; cursor.execute(&#x27;INSERT INTO person (name,age) VALUES (%s)&#x27;%data) # 插入数据，占位符法 cursor.execute(&#x27;INSERT INTO person(name,age) VALUES (%s,%s)&#x27;,(&#x27;qiye&#x27;,20)) #执行多条插入语句 cursor.executemany(&#x27;INSERT INTO person(name,age) values &#x27;,[(&#x27;qiye&#x27;,20),(&#x27;jack&#x27;,20)]) #查询数据 cursor.execute(&#x27;SELECT * FROM person&#x27;) res = cursor.fetchall() #获取所有结果 for line in res: print(line) cursor.execute(&#x27;SELECT * FROM person&#x27;) res = cursor.fetchone() #只获取一个结果 print(res) #修改和删除数据 cursor.execute(&#x27;UPDATE person SET name=%s WHERE id=%s&#x27;,(&#x27;rose&#x27;,1)) cursor.execute(&#x27;DELETE FROM person where id=%s&#x27;,(0,)) # 用来事务提交，只有commit之后，才会提交到数据库进行一系列的操作 db.commit()except Exception as e : # 由于在执行事务的过程中，出现错误，所以回滚，恢复原来的状态，不执行操作 db.rollback()finally: cursor.close() #关闭游标 db.close() #关闭一个数据库连接 1.2 MongoDBMongoDB是基于分布式文件存储的数据库，用于为Web应用提供可扩展的高性能数据存储解决方案。MongoDB属于非关系数据库。 MongoDB的基本概念是文档，集合，数据库。 MongoDB中的表通过collection替代，MongoDB中的行通过document替换。MongoDB中的列通过field替换。MongoDB自动将_id字段设置为主键。 文档：文档是MongoDB中数据的基本单元（即BSON）,类似于关系数据库中的行。文档具有唯一标识_id.数据库可以自动生成。文档以key/value形式。例如: {“name”:”qiye”,”age”:24} 文档具有如下三个特性： 文档的键值对是有序的，顺序不同文档亦不同。 文档的值可以是字符串，整数，数组以及文档等类型。 文档的键是用双引号标识的字符串。以——开头的键是保留的，建议不要使用。 文档区分大小写以及值类型 集合：集合也就是一组文档，类似于数据表。集合没有固定的结构，可以在集合中插入不同格式和类型的数据（和关系表的差异）。 集合命名不能是空字符串，不能有’\\0’字符，不能以system.开头，不要包含$。 数据库：一个MongoDB可以创建多个数据库，默认数据库是db，数据库存储在data目录。MongoDB的单个实例可以容纳多个独立的数据库。 MongoDB的数据类型如下所示： 插入语法：db.集合.insert(JSON格式数据) 查询语法：db.集合.find() 对于多条件查询(and和or).and通过逗号隔开。例如：db.集合.find({“key1”:{条件1},”key2”:{条件2}}) or通$or来实现。例如：db.集合.find({ $or:[ {key1:value1},{key2:value2} ]}) 更新文档：MongoDB通过update(),save()方法来更新集合中的文档。update: db.集合.update( query, update，{ upsert:boolean, multi: boolean, writeConcern:document })其中：query为update的查询条件，update:update的对象和一些更新的操作符等（类似于set后面的内容），upsert（可选，如果不存在update记录，是否插入新文档）, multi（可选，是否更新全部查找出来的记录），writeConcern(可选，异常抛出级别)。eg:db.python.update( {‘title’,’python’},{$set:{‘title’,’python爬’}})db.python.update( {‘title’,’python’},{$set:{‘title’:’python爬’}},{multi:true}) 对于save()方法，通过传入的文档替换已有的文档。db.集合.save({ document{ writeConcern:document }}) 删除文档：MongoDB提供了remove()方法来删除文档。 db.集合.remove( query, #删除的文档的条件 { justOne:boolean, # 如果设置为true，则只删除一个文档。 writeConcern:document }) 删除所有title等于mongodb的文档db.python.remove({‘title’:’Mongodb’})如果没有查询条件，就相当于删除所有的文档。python操作mongodb代码如下所示：12345678910111213141516171819202122232425262728293031323334353637383940#1. 建立连接#pymogo模块使用MongoClient对象描述一个数据库客户端，创建对象的主要参数是host和port#如下三种方式创建client = pymongo.MongoClient() # 连接默认的主机IP和端口# client = pymongo.MongoClient(&#x27;localhost&#x27;,27017) #显示指定IP和端口# client = pymongo.MongoClient(&#x27;mongodb://localhost:27017/&#x27;) #采用URL格式进行连接#2. 获取数据库,通过MongoClient的属性方式来访问数据库db = client.test #方式一# db = client[&#x27;pa-pers&#x27;] #方式二#3. 获取一个集合# collection = db.books #方式一collection = db[&#x27;test_one&#x27;] #方式二#插入文档操作data = &#123; &#x27;author&#x27;:&#x27;mike&#x27;, &#x27;text&#x27;:&#x27;My first book&#x27;, &#x27;tags&#x27;:[&quot;爬虫&quot;,&quot;python&quot;,&quot;网络&quot;], &#x27;date&#x27;: datetime.datetime.utcnow()&#125;data_id = collection.insert_one(data) # 插入一条语句,返回数据的_id值,如果文件内没有_id值，则会自动添加到一文件里# data_id = collection.insert_many(data) # 插入多条,数据以列表形式[&#123;&#125;,&#123;&#125;]print(data_id)# 查询语句，find_oneprint(collection.find_one(&#123;&#x27;author&#x27;:&#x27;mike&#x27;&#125;))# 通过_id查询print(collection.find_one(&#123;&#x27;_id&#x27;: ObjectId(&#x27;619c4dc19c281df292e7e0dd&#x27;)&#125;))# 通过find可以查询多个符合条件的文档,并且可以在括号中加入限制条件，查询多个符合条件的文档for book in collection.find(): print(book)# 统计符合条件的数目print(collection.count_documents(&#123;&#x27;author&#x27;:&#x27;mike&#x27;&#125;))# 修改文档collection.update_one(&#123;&#x27;author&#x27;:&#x27;mike&#x27;&#125;,&#123;&quot;$set&quot;:&#123;&quot;text&quot;:&quot;python book&quot;&#125;&#125;)#删除文档collection.delete_one(&#123;&#x27;author&#x27;:&#x27;mike&#x27;&#125;) # 如果要删除多个，delete_many 2. 动态网站抓取动态网页主要涉及到的技术是Ajax（Asynchoronous JavaScript and XML）和动态Html。 Ajax技术用于网页的局部刷新，不必刷新整个页面，只需要调整局部内容，达到想要的效果，用户体验得到提升。AJAX使用SOAP,XML或者支持JSON的WebService接口，在客户端利用JavaScript处理来自服务器的响应。 SOAP:简单对象访问协议是交换数据的一种协议规范，是一种轻量的、简单的、基于XML（标准通用标记语言下的一个子集）的协议，它被设计成在WEB上交换结构化的和固化的信息。 动态html（DHTML，Dynamic Html）,由HTML+CSS+JavaScript。 如何从动态html页面爬取数据？有如下两种方法： 直接从JavaScript中采集加载的数据 爬取影评信息网页地址(www.mitime.com) 直接采集浏览器中已经加载的数据 对于直接加载渲染后的页面，可以通过PhantomJS,Selenium进行爬取。PhantomJS是基于WebKit的服务端JavaScript API，全面支持Web而无需浏览器支持，运行快，支持各种Web标准，DOM处理，CSS选择器，JSON，Cancas和SVG。PhantomJS可以用于网页自动化，网络检测，网页截屏，无界面测试等。可以把PhantomJS看成一个无界面的浏览器。 Selenium: Selenium是一个自动化测试工具，支持各种浏览器，Selenium支持浏览器驱动，可以对浏览器进行控制。 Selenium可以说是网页爬取的大杀器，可以直接模拟操作浏览器页面。下面介绍关于Selenium的使用方法。 安装配置 对于Selenium的安装配置教程可以自行百度。这里我采用的是Firefox,所以只需要两步，1. 下载selenium,通过pip指令就行。2. 下载驱动器geckodriver。通过如下代码即可使用：123456from selenium import webdriverfrom selenium.webdriver.common.keys import Keysimport time# executable_path就是下载的geckodriver所在的文件路径driver = webdriver.Firefox(executable_path=&#x27;E:\\codeEverment\\python\\geckodriver\\geckodriver-v0.14.0-win64\\geckodriver.exe&#x27;)driver.get(&#x27;http://www.baidu.com&#x27;) 元素查找 selenium的元素定位方法如下图所示： 页面操作 如何给表单填写内容？我们可以定位到表单元素，然后通过元素.send_keys填入内容。找到按钮或链接通过元素.click()模拟点击事件。如果要清除填入的内容，通过元素.clear()可以清除内容。 对于下拉选项，可以通过WebDriver提供的一个叫Select方法进行选择。 对于元素拖拽，首先要找到源元素和目的元素，然后用ActionChains类可以实现。 12345678910111213141516from selenium import webdriverfrom selenium.webdriver.common.keys import Keysimport timedriver = webdriver.Firefox(executable_path=&#x27;E:\\codeEverment\\python\\geckodriver\\geckodriver-v0.14.0-win64\\geckodriver.exe&#x27;)driver.get(&#x27;http://www.baidu.com&#x27;)print(driver.title)assert u&#x27;百度&#x27; in driver.titleelem = driver.find_element_by_name(&#x27;wd&#x27;)elem.clear()elem.send_keys(u&#x27;网络爬虫&#x27;) # 给控件填写内容elem.send_keys(Keys.RETURN) #这里是回车按钮time.sleep(3)driver.close()# 执行js代码# 将页面拉到最低端driver.execute_script(&quot;window.scrollTo(0,document.body.scrollHeight);&quot;) 显示等待的API： 3. Web端协议分析这里主要是关于网页登录POST分析，和验证码的解决方法。一般通过form表单填写账号密码，然后进行获取更多有效数据。 通过POST请求登录 一般我们都会通过构造表单数据，进行post请求。但是通常我们不仅仅提交的是账号，密码，还需要分析页面的具体提交数据，’然后分析对应的数据，进行构建 1234567891011121314151617181920212223242526272829303132&quot;&quot;&quot;这里主要演示了，如何通过session构建表单，然后进行表单提交。&quot;&quot;&quot;# coding: utf-8import reimport requestsdef get_xsrf(session): &#x27;&#x27;&#x27; _xsrf是一个动态参数从网页中获取 :param Session: :return: &#x27;&#x27;&#x27; index_url = &#x27;http://www.baidu.com&#x27; index_page = session.get(index_url) html = index_page.text return &quot;&quot;session = requests.session()_xsrf = get_xsrf(session)post_url = &#x27;htttp://www.zhihu.com/login/phone_num&#x27;postdata = &#123; # 构造post参数，这需要分析登录过程传递的参数，然后进行构建，通常不仅仅包含 #账号密码选项，还有许多附加项 &#x27;_xsrf&#x27;:_xsrf, &#x27;password&#x27;:&#x27;xxxxxxx&#x27;, &#x27;phone_no&#x27;:&#x27;xxxxxxx&#x27;, &#x27;remember_me&#x27;:&#x27;true&#x27;&#125;login_page = session.post(post_url,data=postdata)login_code = login_page.textprint(login_page.status_code)print(login_code) 加密数据分析 通常在网页传输的数据都会进行数据加密，然后添加一系列附加的参数到POST请求中，而且还有验证码。所以这时候需要对网页进行分析。 监听网络数据，分析传送参数 通常这一过程，我们会反复登录，然后记录传送的参数以及cookie中值的变化。 分析参数的获取方式 当我们完成传送参数的分析过程的时候，就需要进一步解析参数的生成方法，然后在程序中进行生成参数，然后构建data,进行post请求。通过我们获取参数的方法有两种。一是根据网络请求分析，查看是否有参数通过API进行获取，例如有的网页的Token，Public_key等是通过API进行获取，通常我们可以在网络请求中查看。二是根据JS文件获取生成方式，通常通过API获取参数还是需要通过JS获取API需要传递的参数，通常我们从JS中获取参数是通过网页文件中搜索参数名，然后从众多包含参数名的JS文件中，分析出参数在哪个JS文件中生成，并提取出相应的生成方法。 实例代码如下(以百度网盘为例)：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217&quot;&quot;&quot;分析百度网盘的登录：https://passport.baidu.com/v2/api/?login #账号验证链接staticpage: https://pan.baidu.com/res/static/thirdparty/pass_v3_jump.htmlcharset: UTF-8tpl: netdisksubpro: netdisk_webapiver: v3codestring:safeflg: 0u: https://pan.baidu.com/disk/homeisPhone:quick_user: 0logintype: basicLoginlogLoginType: pc_loginBasicidc:loginmerge: truecrypttype: 12mkey:countrycode:fp_uid:fp_info:loginversion: v4supportdv: 1mem_pass: ondetect: 1alg: v3tt: 时间戳POST1：gid: A7A60F7-2233-48D5-9CD2-B51EB721ADDFusername: 2891112980@qq.compassword: OcYb7XL/QONk5nADRh+a8cPYNbCvxlMxmncxXomf94YxHenzYWxVJUnM60bNhLt6mYmL+oXC4QvAVw9ujoREPP/YrFDvJNI87jgBwE6doPvn8/9+H8gATohKY68SaIoF+G9tMh/9VIOk5OuwrmpwzvZNQAY/V7gPAjCGmTKWPu4=rsakey: F6qaltAhSGSTrr3LYZoF0fDJkX11XTI6ppui_logintime: 33785# ds通过viewlog可以获取#https://passport.baidu.com/viewlog?callback=jQuery11020116633250707733_1637907449065&amp;ak=1e3f2dd1c81f2075171a547893391274&amp;as=b052f9ad&amp;fs=aJ2zH%2BxmPIQryyPrpIPqtXmH72tk2XARV%2F09o05kx%2FNTGdkoP%2Bzms8NSTm0jdEzP%2BhyPK1KdjXobjyUVxQ9OAuiZZmmEtQpubsfgTL8NYZWy5Sj0%2Bp5a20bTVZFZ70YbQcqZadFjrJ3ompkIg8E%2BYKbzQiEl0HRdRM6o429%2BhEeW3SwAMvEl04XHGbic4703ZlUREQzEeQ5v4b840qnfHziBVVGS9jihhHliV1iPFo6%2Brdl0G1YGREEm%2FvCvr3H15pDBP346ucrwUvqF23DN2WxkjdG68I%2FlW8rMEoTmx2sRpgiVDW4FwR7GYgOinomA7XKjKIV44rACzwS5Cd53wrP%2FCxFhAH91X%2BlFr%2F5Nye6OxX7u7B5FeVe4OnMt%2BrE0cYUqWLzqAls1loOLNoKaGNKpxIjOup%2BA%2B4sulrz1K89itYMKw7MQeNERfCI32hPmJ%2FSAJz36rVBtWO1aMUq66Vu7D8QrNIQ6Ko2L1vq3UQGqO0PRKSObWKZ4leHIf7wzJAMMD2K5cIVq1iLD2k7u5RmP8TI8EYsbxA0V4X8rwdAjAnFOgE4ch%2ByNyJfCeB0FPWs6VPPb9Opd37f6ofMcxWxkjdG68I%2FlW8rMEoTmx2smMDXKV2I%2F%2FSoxAYPYNoZT7XKjKIV44rACzwS5Cd53wuq17oB3hRDHI%2F5FD8ApIUyOxX7u7B5FeVe4OnMt%2BrE0pO2CUHf3wzY2O1nRsJt429KpxIjOup%2BA%2B4sulrz1K8%2Fgsgko9%2FybfRXZevgkJ2FTJ%2FSAJz36rVBtWO1aMUq66TWu3mrKMIRru7O6yI6%2B2TDqQXhdGcWeeWbzEILv75p3aopf9L%2Fmg3TG32fE46ACdLe9EtBCfLdqwpsieMHokZYjAnFOgE4ch%2ByNyJfCeB0FTmTqlzbiE0dSKyXoaNVBrXPmvUO8LF0TrlU912rIKlNqHIWEzcVbD5iGbnqKODdJh2NjhpbaJhRLDM6hXKbFY1SOq8SbKq03JqsbsJTIds6txmXY3cvIjmhaLph06iufE2oUGZsm4FwM9L7uTYsCXEhwGeYx1aILjYgChhgGUkTgVEgyn9h0l5p8hDZ7MKqOqYtXA1fmKQ0NEKMmzZd8eYFcgQPNQxncOm63JKnGItlgXNqFT31JBhv5lC7ybrYm5BE4sZtiEjgFwbXnaseNyqG1B2V5SJLYgUj69M46RJ09vxsCO54v7u3N3p1RjZlHwgU0OC16voeLsV3dm54Q2686dcWl%2BMBcvN6nLD3oN4wZ0sE582gll4wCKKEU3HrI&amp;tk=4006KvNubQMyUKPA8V02ndLZ4ZdO%2BQjmAT7GxVnjL4Rwk6ewi356PNOQKePl0ypYx1c7Dpi5RQawPXeIhjtrwJmhbK%2B7%2FP98piTEVpjpMwlZ0Es%3D&amp;_=1637907449068ds: II4Fg+RtgS2K3+RJnfEduZufjjdwD4QhYQhW04uf/VRDc+fFv1h7ViPEKzfVS15/YEt/aPddzytkFKhGf1Uo/XJm6pthogRlVaD8YeC68vvNq44fKPFACQkFXnHXZ8IGNQ8yrxON2Up0DT3jOOlmAxcKlI5PS3IaOp/jshcyHK+rJaRTTxZ4IXSCKZA+yQAEcAjLtuNMmB1RjOGr5SoG6fPAF+/5zUe4JogUCHg4IFDAaLsFP9Nev6IkG0mnTXJlwdIJSe6ZaTCOplnmGC0mouUychSC9/KIyV2TJiN7kJCUQKyqbp3VAbpz+e9xfaFeqtyPv+wOE1buN6QpyyJ8NvJ4edqWVwch1ebshP8gVPM5tNlcd/YWPT2n6VFtxn5ejme5wI1wDnpCBtbXY3C0+hIJaqJ9M94QNQvK4Swn6kGkrLKCgBbkaOfOdnfdMXmUrIOkIn8vCcbx8htOM9d5kNn8DmHNt4rtCcXphfQSNvf4didhsiAzwUU0ZQ1G53CTs7xMe9NqmzIRYgs/OVr4WIqK7r5cRjuBtcWtZAGKqrOWEO9QokQM225kP47RB25Vap6P4obUOTl88p4eNEGefEhqfDr7ZBqZ8p65Ht76ndP1+q8N6dpCh7CR5hv96mkIw1jfYWlQaZvgjNF/3uxqDRSnLLf9LGJ6O2qdR6IdN3XHB5DGmpdnOiu9PWA6J1qSAfeZOiPBXIsoDM+5yTqMwthRIWZK7zJ4B2vfQrqcqYgW3C9w6KglvuIEaNBZBnjrA/QuA1PU8ZMCijyrRN+3jnS1VX6lBoxVFG9oQnRLdyTaNFQbcq1o6zdcRh/zCQMcBSvZK8tKboXFXDK6oN0zYaSBdx3rJVzzEdkAL1gA8t0nqYsFbdQIQjeFi5RjIrjvDknLZ82xw0zoid7DyZ7qYTbIGxjdYK0QdKXH/u6GM446LCkG2wnSgTEHtBWSJIacz3haMH0277hPZeyGtjstPhqrm3v6wFYqSqa0feecGEGXUlFkq1vfEb/q6CQUEoRzZREiW6pEVnFUIzNG/8mqfISDRwYFqTOjNn5HAEho8qf0G9G3zi6FZgGQ8Jq4HJ9F6I4QEagJYde/A5KmPqZNgSxJc/j/vXzQ7I565b/1F6A=# tk 通过viewlog可以获取tk: 3806qsn4na7kF+g6FX58o+wfHp0rkicKoGxH7zGNq7/Nc12RpVbL51wDca2EsymyAXLwDS2oDrrP90vY/QvHD4ykyS18NmHsVx9q95E2iL5h0T4=dv: tk0.53300068179592071637896306454@ssr0MBvDUOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COtyEyvJzgnkEOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COtyUitizgnkCOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COvkrgn2zwtHzzupohD3Q9EClFJlhDCpX~JldlI-AHDRpKG9zjny7gFk3iukqOArpvol8hDAdFCptEJyoFJiAyGJ8vB6lVukU-nSAXtr0DBvD7gukn~t9zyvkrlupohD3Q9EClFJlhDCpX~JldyQ68KLJoXvDUzukn-vmz~tkIlupohD3Q9EClFJlhDCpX~JldyQ68KLJoXnDq-nHzytDIOtDrgnmgCECw2C3ptJldEClhFtpdFIiA1M6V~FkrznkrOnyB~ukC~nSrOArpvol8hDAdFCptEJyoFJitlBRleQ2zwtSIOny7yukClvDUOArpvol8hDAdFCptEJyoFJitlBRleQpQHBJhzGJ8XBggLzgFHhpZWhFrhn9zHuk3-CrYQ5OzuSCynyqznkBjnDIwtD3HnkIgtSnivk3-nyq-tkC~hrYL2o~I2nbuHdzB6jNBRpeG2CNB-dKuHglMRoVGRVNG6E_ArnnmzzukrHvkqOtyr-ukrlnyBOvkB~ukrlnyBOnDCyt1zjnSE_fuid: FOCoIC3q5fKa8fgJnwzbE67EJ49BGJeplOzf+4l4EOvDuu2RXBRv6R3A1AZMa49I27C0gDDLrJyxcIIeAeEhD8JYsoLTpBiaCXhLqvzbzmvy3SeAW17tKgNq/Xx+RgOdb8TWCFe62MVrDTY6lMf2GrfqL8c87KLF2qFER3obJGn1imUD9LtLDAxtUQVt00ywrFuY99+6ElzcETNlw/C1xZYD5SLH8W4d36DzPMGV4JvslIL5jj8EphieYalb6sMBbKVYW5I2LDDIlDR71IUk1mpk3Y5oUBZH/dZ3rKHJ0IrTnF2Wvu65NpvARErKLH28zZNnk8BWIbmbBo8AkexVQD4TDTFi/kJX2nM5fjwUAQ7erld8wjGrZonj8gE2rhKeVRKCfkBUbih20ajKSj+uMVvFor2HqteRD2yQbHfKX9I9NLYjYxH1YiKtJ/LZAt+jDVAzZK7r2I/XoTgOvpK/GRXi7OMMoCL+ptOvZYbHZs2EmNX6yJZWrVbkhpE/nUJFiBoWKq2H4RxmaiTcdrlpoJQCEBGWNtTsmA4KXyjd88UZ+jjahyvNzJARDVgTFkgMHmv/tVUjVExnyBvjSrtrPoNvLDs45y1J1BPSrpDADEz2WH0+/bGxWLp7WNEcuhl9zrEQCVXVz1ypT0UIooITdod7rWTX1LABi9llmt7bm09SsvyWN9Io4FIQt/daeMGulvDHd3RTujFKqjbyri7OLazNV00oTLvGOs+5nIyBMdEkYaTFoTKPDDVtdCO/a2IhGao+HBbcA3Sf1GWmuogDLSCOxH8nX4hJuc0/kKmVlWYouh6qHeKBM7Z7nT+yF0MXReV/blmoigtQ9A6GiwCQZffiSKUOp1VsS+Ly9+Iupn8y+IS13BgMJv25Rgmo9qqpVhLAbfjO5WTzIDI7m0pWOYtgRQxRCPxURvzJkLg7aW9Jm4l4Bb/STxNSv23Ru6jHCU7CX0YAw0dSweJL1vvynMwJmCcF9FlItQPQ5C9mG3VH8xRyN02ywrifxP8IF10Hg3b+Vi67h2TKPZAaOJzerzLANGXGTGd0Djy9B9kQWIoYRDDc4ujl2xJR5AN3q8OeLV68X8zvK+Yv4VQDA+ZZkVu82rr07bPfAFb6iDElQPL0gOE5uGejgPswFkbgH6k2WEZafSOKRDnPOYABKWeLHdXZe2bA0DO+1FlR8qb+PmiFcZVouo5DCvlQhjRhvpItjUh2/yNfyHjnbZ1A/hRRI7BFVDtu1KaJzjx51nTN1+yRNkC71fasq+HI3zFAuiUsfI5v5JSd2kRNZE7s6bQrA5yMI31SfUDgxDrsd6lPtUU=traceid: F3DFF901callback: parent.bd__pcbs__7pdpaetime: 1637896340sig: NHhDWmhlSkpXWlpoVlpTb1hZajZwaXhyYlRtSmVUM1lCRzBXa1dhOFRPWmtMZnRHd2Vmd29rdTZlUzhHKzl6eA==elapsed: 2shaOne: 00b653720afbeae6ff66615907b0e59e85a35757rinfo: &#123;&quot;fuid&quot;:&quot;1d5264f920930df8d682b5cbac99c9e5&quot;&#125;POST2：#通过gid获取token#https://passport.baidu.com/v2/api/?getapi&amp;token=&amp;tpl=netdisk&amp;subpro=netdisk_web&amp;apiver=v3&amp;tt=1637908334880&amp;class=login&amp;gid=4D970C9-672F-4002-BCD8-D71F629D9C8A&amp;loginversion=v4&amp;logintype=basicLogin&amp;traceid=&amp;time=1637908335&amp;alg=v3&amp;sig=ZHpJazJocTlTeHNMNG5LUlZjdE8yTXNLMUNjMGk3QmVTUzdDajF2Y2Q0M3pjSDBNL1k4ZHN1VHJzY0g0QWdrbw%3D%3D&amp;elapsed=10&amp;shaOne=00c2bc34cb293b92b355a01af5b33dcb0c7c0e19&amp;callback=bd__cbs__elghk8token: 134400c212148a1ed97cd3e14a64dec4##通过loginv4_tangram_c32acce.js可以 看到gid的生成方法# 通过F12查找可以得到gid的生成方法，如下所示# &quot;xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&quot;.replace(/[xy]/g, function(e) &#123;# var t = 16 * Math.random() | 0,# n = &quot;x&quot; === e ? t : 3 &amp; t | 8;# return n.toString(16)# &#125;).toUpperCase()gid: 14893FE-D2FB-4BBC-9D6F-353A905BD43Dusername: 18370446979#可以通过rsa加密#通过F12找到password的生成方法# password: function() &#123; e._getRSA(function(t) &#123; e.RSA = t.RSA, e.rsakey = t.rsakey &#125;) &#125;password: qnmL7K9NK7pvaHhlmV/FVFjJEJsUUCmPs+aRr0jWLPSe9y033E9268hTdWCZoGGSz3fV2Q5BN7szAXd8vpH/I4BzNI0Jd72MXb1pkNEXeUnuxEy7xDPGshCWThTIID2PcCydW0dWrs/GrVjTsiwKFrOuig8uWSeJ5F5RVS2ioMc=rsakey: kC02XvnT7I6uMux9cYbgVbsVqcat7gCK#在loginv4_tangram_c32acce.js文件可以找到ppui的生成方法，没啥影响，可以用相同的ppuippui_logintime: 59338ds: DJv1I2ZMYFt4VNJGrEieRzX+fTxEGqp5KEKwP82Eqs0OwC/0nDO0AaGbSnOx4gLBgEwFUlmknWtM8w70bR6651jBTWR3RrN6M8athvLv0qjIh/uqJXSiudqxzvNbI8JrbRQi/L6z0yapH8OzFzgtjkCbQnKIIseJ5YN7pXQNzzfsLmwRSboSxSIMh7H+mNWlm7Jv6bipWb2Ef+mULPz+HJcsYAuYX7ZSRej5qKjg4LilTqk77vHywOzTdYX43QwjBa95tTsSQl3UdoyTyTSoP3+MI39NMnpfvc31ubL1aYBzIwtGTKBGQ6r9IMPO1/apOChCY8Ukvh5YsbuEPVZhHNrWGbwQxmPZGfwnHLxk+a4fe8Swz2GQYZhJWXhGX1zomphghvSfI0Xr0C3ofNTFL5YQ71CiRAzbbmQ/jtEHblVptfsMcO06PhUuRCioIIrvPK3f5pSnKweaOreL/stiDNRDmTc/TFc/zLbPufGyD5xEZm3ptEBPg9JO1kZVGX9hEjqGlJPTvnmgyiTZbqqYMGuU+teFEfSezySS7QbzHrNdJWklVjPbX4MaXF0vQwV3QvJ2BYaIJXbUbmi2N+MCb6uuhcCUzRXcvQcCFN+nCi8JV0rZbi8qRA6qogqITSmcoX+C2ebWRCqd1Bta5oZQDFnlNY8P1//P9TI67UuT8f3rpPwKHtCV94QXnYOPyXhxoFShURm0wEXyWfe6k74eCKJzF9WHKgqjvWNBU4oxh51ptfsMcO06PhUuRCioIIrvRnbBncAl/l2oYpW1Tl+s9vX6rw3p2kKHsJHmG/3qaQjDWN9haVBpm+CM0X/e7GoNFKcst/0sYno7ap1Hoh03dccHkMaal2c6K709YDonWpJOuWUh5Btk18pp2ofQaQue++3sNK8B3l22xvtu69xAcBbcL3DoqCW+4gRo0FkGeOsD9C4DU9TxkwKKPKtE37eOdLVVfqUGjFUUb2hCdEt3JNo0VBtyrWjrN1xGH/MJAxwFK9kry0puhcVcMrqg3TNhpIF3HeslXPMR2QAvWADy3SepiwVt1AhCN4WLlGMiuO/EHjpcfKHlM/uGqUpTxlzFg6wRAvrcujI33KeY139x4zosKQbbCdKBMQe0FZIkhpybzWnTPAJbZ2HlksG4LzZfZZxOv3Rz8OzebBbGZY2cKHzOIWo49PxA+Q/fgOWSMCg+0HmDAKY5njRPYRTBGiWmrU3+ErtPH77j3x0d0oaItNhgp6PrWJ92dgGBZTxabg4LrS+iGvesD1QIbG1EpOUu6EGco5bp1s9+m48R0KveFA==tk: 5886+r0RsngU7eH85qinjvKq/Ov8cTqi3758E9sFclzWzRrsTzYvBtjqRuohenHcu9EkNtVwd1ZSl1bUdta7LJyUF+N0t3lElPs4Qk2Y4E3v6mM=dv: tk0.076573147928699071637897079952@ssm0MCo4JL8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGCLvj6ZvT-c8k6L8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGCLvjKeo9-yoH--0tDh4SRH6CXU9Xh4CtlZ9XwXJiAe4atYFH-~ok6cUk2-8e--0tDh4SRH6CXU9Xh4CtlZ9XwXJiAe4atYFH-x8k6yUkSi0kqLA2toDXph4AwUCtv69jDU9~AjF9poKGX30k7eo4hltm0otok7-0k8~ve-Xv12e0tDh4SRH6CXU9Xh4CtlZ9XwjRGpYI9Dlo46e0k8Xve-Xvj8Z0tDh4SRH6CXU9Xh4CtlZ9XwjRGpYI9Dlo4KX0k5x8e-Xok6y0tDh4SRH6CXU9Xh4CtlZ9XwaO~pYUkSi8e-e8j8Lv16e8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGAlokKe0kSX0k2e812e0r-_CllI~lI~vtMEhImi8H-e0kSiBmpRsL-01q~v1C~8j2ZvjSeokKyo4q~84Kjvj7yvjq~o4SX87__imVIrDZJr8P0ew-KGxgKatWFrCgKiwY0ecXOaD3Fa3gFG6_zmp8m--0k2ev18Lvj2i0k2X8jKLokKZ0k2X8jKL84CjvB-x816_fuid: FOCoIC3q5fKa8fgJnwzbE67EJ49BGJeplOzf+4l4EOvDuu2RXBRv6R3A1AZMa49I27C0gDDLrJyxcIIeAeEhD8JYsoLTpBiaCXhLqvzbzmvy3SeAW17tKgNq/Xx+RgOdb8TWCFe62MVrDTY6lMf2GrfqL8c87KLF2qFER3obJGn1imUD9LtLDAxtUQVt00ywrFuY99+6ElzcETNlw/C1xZYD5SLH8W4d36DzPMGV4JvslIL5jj8EphieYalb6sMBbKVYW5I2LDDIlDR71IUk1mpk3Y5oUBZH/dZ3rKHJ0IrTnF2Wvu65NpvARErKLH28zZNnk8BWIbmbBo8AkexVQD4TDTFi/kJX2nM5fjwUAQ7erld8wjGrZonj8gE2rhKeVRKCfkBUbih20ajKSj+uMVvFor2HqteRD2yQbHfKX9I9NLYjYxH1YiKtJ/LZAt+jDVAzZK7r2I/XoTgOvpK/GRXi7OMMoCL+ptOvZYbHZs2EmNX6yJZWrVbkhpE/nUJFiBoWKq2H4RxmaiTcdrlpoJQCEBGWNtTsmA4KXyjd88UZ+jjahyvNzJARDVgTFkgMHmv/tVUjVExnyBvjSrtrPoNvLDs45y1J1BPSrpDADEz2WH0+/bGxWLp7WNEcuhl9zrEQCVXVz1ypT0UIooITdod7rWTX1LABi9llmt7bm09SsvyWN9Io4FIQt/daeMGulvDHd3RTujFKqjbyri7OLazNV00oTLvGOs+5nIyBMdEkYaTFoTKPDDVtdCO/a2IhGao+HBbcA3Sf1GWmuogDLSCOxH8nX4hJuc0/kKmVlWYouh6qHeKBM7Z7nT+yF0MXReV/blmoigtQ9A6GiwCQZffiSKUOp1VsS+Ly9+Iupn8y+IS13BgMJv25Rgmo9qqpVhLAbfjO5WTzIDI7m0pWOYtgRQxRCPxURvzJkLg7aW9Jm4l4Bb/STxNSv23Ru6jHCU7CX0YAw0dSweJL1vvynMwJmCcF9FlItQPQ5C9mG3VH8xRyN02ywrifxP8IF10Hg3b+Vi67h2TKPZAaOJzerzLANGXGTGd0Djy9B9kQWIoYRDDc4ujl2xJR5AN3q8OeLV68X8zvK+Yv4VQDA+ZZkVu82rr07bPfAFb6iDElQPL0gOE5uGejgPswFkbgH6k2WEZafSOKRDnPOYABKWeLHdXZe2bA0DO+1FlR8qb+PmiFcZVouo5DCvlQhjRhvpItjUh2/yNfyHjnbZ1A/hRRI7BFVDtu1KaJzjx51nTN1+yRNkC71fasq+HI3zFAuiUsfI5v5JSd2kRNZE7s6bQrA5yMI31SfUDgxDrsd6lPtUU=traceid: B67AC501#baidu.phoenix._setIconsStatus#只是标识，不参与校验&quot;bd__cbs__&quot; + Math.floor(Math.random() * 2147483648).toString(36)callback: parent.bd__pcbs__kk48futime: 1637897139alg: v3sig: cFJLQ0ZzMWo2VmFaaUVXMFJQbk94QUhuUWFLeEVkNFllM2tjOVhFQ1hOcEg2UC9PUUltR2l2SnZOcFpQNi9FTw==elapsed: 5shaOne: 00a72749f19999931e2653a554991baf3f874f7brinfo: &#123;&quot;fuid&quot;:&quot;1d5264f920930df8d682b5cbac99c9e5&quot;&#125;&quot;&quot;&quot;&quot;&quot;&quot; 一般我们对登录附加信息的获取，都会通过多个POST请求进行比较，得出不变的数据和变动的数据， 然后进行分析处理。 1. 获取表单提交参数 2. 确定参数获取方式-&gt;可以通过F12的内容查找，然后搜索指定参数名，找到对应的代码，分析数据如何生成的 3. 构建post请求，进行登录&quot;&quot;&quot;import base64import jsonimport refrom Crypto.PublicKey import RSAfrom Crypto.Cipher import PKCS1_v1_5import PyV8 #pyv8引擎，可以直接执行js代码，不用转换为python语言from urllib import quoteimport requestsimport timeif __name__ == &#x27;__main__&#x27;: s = requests.Session() s.get(&#x27;http://yun.baidu.com&#x27;) js = &#x27;&#x27;&#x27; function callback()&#123; return &quot;bd__cbs__&quot; + Math.floor(Math.random() * 2147483648).toString(36) ; &#125; function gid()&#123; return &quot;xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&quot;.replace(/[xy]/g, function(e) &#123; var t = 16 * Math.random() | 0, n = &quot;x&quot; === e ? t : 3 &amp; t | 8; return n.toString(16) &#125;).toUpperCase() &#125; &#x27;&#x27;&#x27; ctxt = PyV8.JsContext() ctxt.enter() ctxt.eval(js) ### 获取gid gid = ctxt.locals.gid() ### 获取callback callback = ctxt.locals.callback() ### 获取token,根据需要的参数，传入相应参数，返回对应的token tokenUrl = &quot;#https://passport.baidu.com/v2/api/?getapi&amp;token=&amp;tpl=netdisk&amp;subpro=netdisk_web&amp;apiver=v3&amp;tt=1637908334880&amp;class=login&amp;gid=4D970C9-672F-4002-BCD8-D71F629D9C8A&amp;loginversion=v4&amp;logintype=basicLogin&amp;traceid=&amp;time=1637908335&amp;alg=v3&amp;sig=ZHpJazJocTlTeHNMNG5LUlZjdE8yTXNLMUNjMGk3QmVTUzdDajF2Y2Q0M3pjSDBNL1k4ZHN1VHJzY0g0QWdrbw%3D%3D&amp;elapsed=10&amp;shaOne=00c2bc34cb293b92b355a01af5b33dcb0c7c0e19&amp;callback=bd__cbs__elghk8&quot; token_response = s.get(tokenUrl) pattern = re.compile(r&#x27;&quot;token&quot;\\s*:\\s*&quot;(\\w+)&quot;&#x27;) match = pattern.search(token_response.text) if match: token = match.group(1) else: raise Exception #### 获取callback callback2 = ctxt.locals.callback() #### 获取rsakey和pubkey rsaUrl = &quot;&quot; response = s.get(rsaUrl) key = response.text #匹配搜索key pubkey = response.text ####加密password password = &#x27;&#x27; #自己的密码 pubkey = pubkey.replace(&#x27;\\\\n&#x27;,&#x27;\\n&#x27;).replace(&#x27;\\\\&#x27;,&#x27;&#x27;)#处理pubkey rsakey = RSA.importKey(pubkey) #百度网盘通过的是RSA加密 cipher = PKCS1_v1_5.new(rsakey) password = base64.b64decode(cipher.encrypt(password)) #加密 ###获取callback callback3 = ctxt.locals.callback() data = &#123; &quot;charset&quot;: &#x27;UTF - 8&#x27;, &quot;tpl&quot;: &quot;netdisk&quot;, &quot;subpro&quot;: &quot;netdisk_web&quot;, &quot;apiver&quot;: &quot;v3&quot;, &quot;codestring&quot;:&quot;&quot;, &quot;safeflg&quot;: 0, &quot;u&quot;: &#x27;https: // pan.baidu.com / disk / home&#x27;, &quot;isPhone&quot;:&quot;&quot;, &quot;quick_user&quot;: 0, &quot;logintype&quot;: &quot;basicLogin&quot;, &quot;logLoginType&quot;: &quot;pc_loginBasic&quot;, &quot;idc&quot;:&quot;&quot;, &quot;loginmerge&quot;: True, &quot;crypttype&quot;: 12, &quot;mkey&quot;:&quot;&quot;, &quot;countrycode&quot;:&quot;&quot;, &quot;fp_uid&quot;:&quot;&quot;, &quot;fp_info&quot;:&quot;&quot;, &quot;loginversion&quot;: &quot;v4&quot;, &quot;supportdv&quot;: 1, &quot;mem_pass&quot;: &quot;on&quot;, &quot;detect&quot;: 1, &quot;alg&quot;: &quot;v3&quot;, &quot;gid&quot;:gid, &#x27;callback&#x27;:callback3, &#x27;token&#x27;:token, &#x27;password&#x27;:password &#125; #构建好了表单数据，然后就可以进行页面登录了 post1_response = s.post(&#x27;https://passport.baidu.com/v2/api/?login&#x27;,data=data) #如果页面还有其它信息需要验证，在后面进行处理即可 #比如需要滑块验证等 验证码问题 从上面的POST表单构建引出的一个新的问题，就是关于验证码的问题，通常验证码有三种形式，一种是在post表单中的图片的验证码，一种是根据图片点击相应的区域，一种是滑块验证。 IP代理 同一个IP频繁访问网页，可能会导致IP被封禁或者需要输入验证码验证是否是真人。解决的方法可以通过加大爬虫的延时，让网页觉得你就是真人在浏览网页，毕竟通过爬虫访问网页的频率过快，容易被识别，然而通过减慢爬虫时延带来的另一个问题就是效率的降低。更好的一种做法的更换IP进行访问。 IP代理的方式可以通过设置request的proxies参数进行设置。1234567# 代理设置，使用代理Proxy,可以为任意请求方法通过设置proxies参数来配置单个请求 proxies = &#123; &quot;http&quot;: &quot;http://0.10.1.10:3128&quot;, &quot;https&quot;: &quot;http://10.10.1.10:1080&quot;, # &quot;http&quot;:&quot;http://user:pass@10.10.1.10:3128&quot; #这是代理中身份认证的用户名和密码，来设置代理 &#125; requests.get(&quot;www.baidu.com&quot;, proxies=proxies) # 设置代理ip我们已经知道通过request可以设置代理IP，那么如何获取代理IP？有如下几种方式： VPN国内和国外很多厂商提供VPN服务，可以分配不同的网络线路，并可以自动更换IP，实时性很高，速度很快。但是价格一般较高，适合商用。 IP代理池一些厂商将很多IP做成代理池，提供API接口，允许用户使用程序调用。稳定的IP代理池费用也是较高的，所以不适合个人学习使用。 现在很多领域都需要用到代理IP，用到的领域越来越广，如爬虫、投票、抢购等等，那么具体代理ip有什么用?能做些什么呢?代理ip具体的作用：1.可以冲破原始IP的访问限制，可以访问国外站点。2.可以访问一些单位或团体的内部资源，如某大学FTP(但是前提是，该代理地址在该资源允许访问的范围时可以进行访问)，使用教育网内地址段免费代理服务器，就可以用于对教育 网开放的各类FTP下载上传，以及各类资料查询共享等服务。3.可以突破中国电信的IP封锁：中国电信用户有很多网站是被限制访问的，这种限制是人为的，所以不能访问时可以换一个国外的代理服务器进行访问。4.可以提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区,当外界有信息通过的时候,将其保存到缓冲区内,当其他用户也访问相同信息的时候， 可以直接将缓冲区中的信息传给用户，提高访问的速度。5.隐藏真实IP：上网者也可以通过这种方法隐藏自己的IP，免受攻击。代理ip又分透明代理，高匿代理，S5代理等等举例：1.突破访问限制如：代理到国外的IP即可访问受限制的Google!2.用代理IP做网络爬虫!3.刷访问量，刷点击，优化网站的流量!4.做百度SEO,提升网站的排名!5.刷网络投票!6.批量挂机!如：QQ或YY!7.批量注册，如：注册邮箱，论坛账号之类的! ADSL宽带拨号也就是拨号上网。ADSL每次断开重新连接时会分配新的IP地址，爬虫可以利用这个原理更换IP。但是更换IP需要断开重连，所以效率并不高，适用于实时性不高的场景。Windows下通过rasdial操作可以进行拨号，Python通过os.system来执行命令行语句，进行拨号操作。代码如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243&#x27;&#x27;&#x27;&#x27;作为示例进行演示，如何通过ADSL进行拨号和断开&#x27;&#x27;&#x27;import osimport timeg_adsl_account = &#123; &quot;name&quot;:&quot;adsl&quot;, &quot;username&quot;:&quot;xxxxxxxx&quot;, &quot;password&quot;:&quot;xxxxxxxx&quot;&#125;class Adsl(object): def __init__(self): self.name = g_adsl_account[&#x27;name&#x27;] self.username = g_adsl_account[&#x27;username&#x27;] self.password = g_adsl_account[&#x27;password&#x27;] #修改asdl def set_adsl(self,account): self.name = account[&#x27;name&#x27;] self.username = account[&#x27;username&#x27;] self.password = account[&#x27;password&#x27;] &#x27;&#x27;&#x27; 拨号连接 &#x27;&#x27;&#x27; def connect(self): cmd_str = &quot;rasdlal %s %s %s&quot;%(self.name,self.username,self.password) os.system(cmd_str)#调用windows的控制台，通过rasdlal进行拨号 time.sleep(5) &#x27;&#x27;&#x27; 断开连接 &#x27;&#x27;&#x27; def disconnect(self): cmd_str = &quot;rasdial %s / disconnect&quot;%self.name os.system(cmd_str) time.sleep(5) &#x27;&#x27;&#x27; 重新拨号 &#x27;&#x27;&#x27; def reconnect(self): self.disconnect() self.connect() 如果我们想要找到一些免费的IP代理，可以在各个网站寻找免费的IP，然后对IP去重，检测代理有效性等操作，存放到数据库中，通过接口获取免费的IP，构建自己的IP池。 图片验证码识别 我们在浏览器登录的时候，常常需要输入图片验证码，这里给出几种方法解决图片验证码的问题。 Cookie绕过登录，我们在浏览网站的时候，常常会保留登录信息，下次登录就不要进行登录，通过Cookie即可登录。所以我们可以获取登录后的Cookie，然后设置Cookie，绕过登录。可以获取多个Cookie，构建Cookie池，绕过登录验证。示例代码如下。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&quot;&quot;&quot; 通过已经登陆过的Cookie进行通过Cookie进行登录。 我们可以在模拟登录成功后，将session的值保存到本地，后续可以通过cookie进行登录。&quot;&quot;&quot;import pickleimport requestsfrom requests.cookies import cookiejar_from_dictdef save_session(session): #将session写入文件,session.txt with open(&#x27;session.txt&#x27;,&#x27;wb&#x27;) as f: pickle.dump(session.headers,f) #写入头 pickle.dump(session.cookes.get_dict(),f)#写入Cookie值 print(&#x27;将sesssion写入文件:session.txt&#x27;)def load_session(): #加载session with open(&#x27;session.txt&#x27;,&#x27;rb&#x27;) as f: headers = pickle.load(f) cookies = pickle.load(f) return headers,cookiessession = requests.Session()session.headers = &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#x27;dicts = dict()dicts[&#x27;BDORZ&#x27;] = &#x27;FFFB88E999055A3F8A630C64834BD6D0&#x27;dicts[&#x27;PSINO&#x27;] = &#x27;7&#x27;dicts[&#x27;BDRCVFR[hpwYTbhBfiY]&#x27;] = &#x27;9xWipS8B-FspA7EnHc1QhPEUf&#x27;dicts[&#x27;BDUSS_BFESS&#x27;] = &#x27;XFOcmlGQVhEdUZjSnJiWmJmbVgza35YV0RvZktEUEdkUXgyYkJ4S01uOER0OHRoSVFBQUFBJCQAAAAAAAAAAAEAAADprZrnsrvLxrnp1MYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMqpGEDKqRhW&#x27;dicts[&#x27;BDUSS&#x27;] = &#x27;XFOcmlGQVhEdUZjSnJiWmJmbVgza35YV0RvZktEUEdkUXgyYkJ4S01uOER0OHRoSVFBQUFBJCQAAAAAAAAAAAEAAADprZrnsrvLxrnp1MYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMqpGEDKqRhW&#x27;dicts[&#x27;BAIDUID_BFESS&#x27;] = &#x27;E8BBAF114F9097C4849493D68A677EC2:FG=1&#x27;dicts[&#x27;HMACCOUNT_BFESS&#x27;] = &#x27;13391551711E4651&#x27;dicts[&#x27;MCITY&#x27;] = &#x27;-%3A&#x27;dicts[&#x27;BAIDUID&#x27;] = &#x27;A02BD18D5032DA5E3DA0339468C7AE42:FG=1&#x27;dicts[&#x27;__yjs_duid&#x27;] = &#x27;1_2b5acc51d9eef8efeada14364c2710b71633588836061&#x27;dicts[&#x27;c_dl_fref&#x27;] = &#x27;https://blog.csdn.net/kaida1234/article/details/89553115&#x27;dicts[&#x27;Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac&#x27;] = &#x27;1638500494&#x27;dicts[&#x27;c_page_id&#x27;] = &#x27;default&#x27;dicts[&#x27;c_first_page&#x27;] = &#x27;https%3A//i.csdn.net/&#x27;dicts[&#x27;log_Id_pv&#x27;] = &#x27;1556&#x27;dicts[&#x27;delPer&#x27;] = &#x27;0&#x27;dicts[&#x27;TY_SESSION_ID&#x27;] = &#x27;1e66f339-636d-4633-845e-6afde0a1d0f8&#x27;dicts[&#x27;PSTM&#x27;] = &#x27;1633401440&#x27;dicts[&#x27;dc_sid&#x27;] = &#x27;2cee0d889d6dbf50e0ba642d35372f04&#x27;dicts[&#x27;dc_session_id&#x27;] = &#x27;10_1638499930521.206489&#x27;dicts[&#x27;log_Id_click&#x27;] = &#x27;664&#x27;dicts[&#x27;c_segment&#x27;] = &#x27;12&#x27;dicts[&#x27;c_first_ref&#x27;] = &#x27;cn.bing.com&#x27;dicts[&#x27;ab_sr&#x27;] = &#x27;1.0.1_MzcxMWQ4NzQzNGYyZWRmZmJhNTA2NTdiNDY5Yjc2M2I3NTg2MThlZjg4OGRhZWVjNGExYmEwYzljODc5ZmExNmJhM2RkYWQxOTI5NzcyZjhiZWM5MDExNmU3ODZjNTcxMGY1MDg0ZTA1MmE3MTc4MWZjYTcxYjQ0ODg5OGVjY2JiZDgyZjlkYjk5Nzg4M2Q4OTVkYzRiMDIwZWJkM2FmOQ==&#x27;dicts[&#x27;firstDie&#x27;] = &#x27;1&#x27;dicts[&#x27;log_Id_view&#x27;] = &#x27;4444&#x27;dicts[&#x27;c_dl_um&#x27;] = &#x27;-&#x27;dicts[&#x27;AU&#x27;] = &#x27;5E7&#x27;dicts[&#x27;c_dl_rid&#x27;] = &#x27;1638195457309_383576&#x27;dicts[&#x27;H_PS_PSSID&#x27;] = &#x27;&#x27;dicts[&#x27;uuid_tt_dd&#x27;] = &#x27;10_37481785600-1633532465546-630652&#x27;dicts[&#x27;Hm_up_6bcd52f51e9b3dce32bec4a3997715ac&#x27;] = &#x27;%7B%22islogin%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isonline%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isvip%22%3A%7B%22value%22%3A%220%22%2C%22scope%22%3A1%7D%2C%22uid_%22%3A%7B%22value%22%3A%22weixin_43387852%22%2C%22scope%22%3A1%7D%7D&#x27;dicts[&#x27;c_dl_prid&#x27;] = &#x27;1638100826971_946707&#x27;dicts[&#x27;Hm_lvt_e5ef47b9f471504959267fd614d579cd&#x27;] = &#x27;1637833430&#x27;dicts[&#x27;Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac&#x27;] = &#x27;6525*1*10_37481785600-1633532465546-630652!5744*1*weixin_43387852&#x27;dicts[&#x27;BT&#x27;] = &#x27;1633859763611&#x27;dicts[&#x27;HMACCOUNT&#x27;] = &#x27;13391551711E4651&#x27;dicts[&#x27;Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac&#x27;] = &#x27;1638500140,1638500349,1638500394,1638500403&#x27;dicts[&#x27;p_uid&#x27;] = &#x27;U010000&#x27;dicts[&#x27;UN&#x27;] = &#x27;weixin_43387852&#x27;dicts[&#x27;__gads&#x27;] = &#x27;ID=20d2edb2ecff0dd8-2294b66f51cc00dd:T=1633682976:RT=1633682976:S=ALNI_MZ4Wd5yAdGLxFPr5rHe4LQE0aLwJw&#x27;dicts[&#x27;BIDUPSID&#x27;] = &#x27;FBAA203ACF6D45F80BC31F3396736AD0&#x27;dicts[&#x27;dc_tos&#x27;] = &#x27;r3ir2m&#x27;dicts[&#x27;c_ref&#x27;] = &#x27;https%3A//i.csdn.net/&#x27;dicts[&#x27;ssxmod_itna&#x27;] = &#x27;Yq+xBDuQG=YUGkDzxAhYo=mxWqY5Y3K3qoiQD/YBmDnqD=GFDK40oE7bD7mKn5BAuDeWxrGhuqWKFixPmaAiDOh31A+=0iEdDU4i8DCTrPoD4fKGwD0eG+DD4DWDmW7DnxAQDjxGpycuTXBDi3Dbg=Di4D+zd=DmqG0DDUH/4G2D7Uy8ivyWld52ubMiir4YB=DjqTD/+qFMWRFsa5VWjnTu44DC2v1oi519poqYAiWbqGybKGunqXV4uR1pq0Z3j5vKG4oBrdoBiPet0q3ngxU6GDPARCztP4KD++P7g+dBr5DGRwx3wDxD&#x27;dicts[&#x27;UserToken&#x27;] = &#x27;885ba7040ab64148b8d10b4450fcc279&#x27;dicts[&#x27;UserNick&#x27;] = &#x27;%E5%BD%92%E6%9D%A5%E7%A9%BA%E7%A9%BA&#x27;dicts[&#x27;BDRCVFR[C0p6oIjvx-c]&#x27;] = &#x27;Ble67U-OKLffjRLnjc3nW6kg1IxpA7E&#x27;dicts[&#x27;ssxmod_itna2&#x27;] = &#x27;Yq+xBDuQG=YUGkDzxAhYo=mxWqY5Y3K3qoiG9toFxBwm47pxUxBa4Hq7GFGfoiDhPizeG7q5h7Mt7khdzCG2O7=IYjr6oTN9ebmgXvejbDDQt5Uljyh7yEuBiEMSIhcGRVeRAB+Er04T2YpCHxTHtYBrV15C0eviq+WkT8vmqGKrtFG=Mo7sk/ntYiGQI87s1SYx1=TpeYcD5OALP3OQgpKTRGpC8bLiPSMTSCodV1ddSeu1P99Niqq8dQDd8+mcI+I5FHuv=yFBy=Ui7ZguyP6CkyK9C79iTqo4RqQd7EgTbEExB=XSjr88yXeYel5TGD1iDPnnD87ZjDQAGXQH+juTKGtnYnRahRAEKiPL+T8qkYT8m+Rfrue493By+ydLytmbuKaH8EG7wRfwD1eVGar8usImSzTL8TEB2qTwYBx4U5lGftyDTa9bB0sRWl748nyt6uW0xEDBAYAm8Hxm5fDx//oaf2ZYB4txDKdNdYzlm+0Ghg+xMcWCAXV138h8iDaP8DzG482Wr72i44D7=DYIOeD=&#x27;dicts[&#x27;c_pref&#x27;] = &#x27;https%3A//cn.bing.com/&#x27;dicts[&#x27;c_utm_medium&#x27;] = &#x27;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&#x27;dicts[&#x27;UserName&#x27;] = &#x27;weixin_43387852&#x27;dicts[&#x27;c_dl_fpage&#x27;] = &#x27;/download/weixin_38695452/12856957&#x27;dicts[&#x27;UserInfo&#x27;] = &#x27;885ba7040ab64148b8d10b4450fcc279&#x27;#不能直接将字典对象设置为cookies ，需要通过requests库的cookiejar_from_dict()方法#把字段对象转换为cookiejar对象session.cookies = cookiejar_from_dict(dicts)r = session.post(&#x27;https://blog.csdn.net/&#x27;)print(r.status_code)print(r.text)save_session(session)# print(session.headers)# print(session.cookies.get_dict())# session.headers,session.cookies = load_session() #将cookie设置为保存的cookie 借助图片识别技术识别图片文字。一种常用的是通过tesseract-ocr进行识别，一种借助百度提供的文字识别技术进行识别处理。 通过图像识别技术来获取文字信息，通常需要对图像进行预处理来提高识别准确率。这里给出一些处理流程。 graph LR 原始图片 --> 图片放大 图片放大 --> 灰度化 灰度化 --> 二值化 二值化 --> 去除边框 去除边框 --> 降噪 降噪 --> 结束示例代码如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#图像处理方法import cv2import matplotlib.pyplot as pltimport numpy as np#1.放大图片#DPI≥ 300的图片有更好的识别效果img_org = cv2.imread(&#x27;R-C.png&#x27;)h,w = img_org.shape[:2]h,w = h*2,w*2 #按比例放大两倍img_l = cv2.resize(img_org,(w,h))plt.subplot(121),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))plt.subplot(121),plt.imshow(cv2.cvtColor(img_l,cv2.COLOR_BGR2RGB))plt.show()#2. 图像灰度化，通过cvtColor灰度化图像img_org = cv2.imread(&#x27;R-C.png&#x27;)img_gray = cv2.cvtColor(img_org,cv2.COLOR_BGR2GRAY) # 灰度化plt.subplot(121),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))plt.subplot(121),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))plt.show()#3.图像二值化#二值化将图片转换为黑白图像，二值化可以通过cv2.threshold()方法，该方法会返回两个参数，第一个为阈值，第二个为转化后的图像&quot;&quot;&quot;cv2.threshold(src,thresh,maxval,type,dst=None)其中thresh为阈值，maxval为高于阈值的处理，dst为二值化处理方法选择的参数。dst指定的不同在于处理逻辑的不同。&quot;&quot;&quot;img_org = cv2.imread(&#x27;R-C.png&#x27;)img_gray = cv2.cvtColor(img_org,cv2.COLOR_BGR2GRAY) # 灰度化# 像素点高于127的设置为255，不高于127的设置为0_,img_bin = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY)#二值化plt.subplot(131),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))plt.subplot(132),plt.imshow(cv2.cvtColor(img_gray,cv2.COLOR_BGR2RGB))plt.subplot(133),plt.imshow(cv2.cvtColor(img_bin,cv2.COLOR_BGR2RGB))plt.show()#4.去除边框,当存在边框的时候去除,def clearBorder(img): h,w = img.shape[:2] for y in range(0,w): for x in range(0,h): if(y&lt;2 or y&gt;w-2): img[x,y] = 255 if x &lt; 2 or x &gt;h-2: img[x,y] = 255 return img#5. 降噪# 噪声是图像中亮度或颜色的随机变化，降低噪声有利于准确率的提升，噪声去除的方法有很多# 这里以线降噪为例def interfaceLine(img): h,w = img.shape[:2] for y in range(1,w-1): for x in range(1,h-1): count = 0 if np.all(img[x,y-1])&gt;245: count = count+1 if np.all(img[x,y+1])&gt;245: count = count+1 if np.all(img[x-1],y)&gt;245: count = count+1 if np.all(img[x+1],y)&gt;245: count = count+1 if count &gt; 2 : img[x,y] = 255 return img 通过tesseract-ocr进行识别，需要下载tesseract-oct,并且下载两个包。12pip install pytesseractpip install pillow示例代码如下所示：12345678def pyteFunction(): image = Image.open(&#x27;test.PNG&#x27;) # 设置tesseract的安装路径,这里设置你下载的tesseract-ocr的安装路径 pytesseract.pytesseract.tesseract_cmd = r&#x27;C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe&#x27; code = pytesseract.pytesseract.image_to_string(image) print(code) 通过百度API调用文字识别服务。可以访问https://ai.baidu.com/，然后搜索文字识别，创建一个文字识别应用，创建成功后，根据提供的APP_KEY和Secret_Key即可调用API，获取到服务。示例代码如下所示： 123456789101112131415161718192021222324252627#通过调用百度API进行验证码的识别def identify_Verification_code(API_Key, Secret_Key, Verification_code): host = &#x27;https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&amp;client_id=&#x27; + API_Key + &#x27;&amp;client_secret=&#x27; + Secret_Key response = requests.get(host) access_token = response.json()[&#x27;access_token&#x27;] request_url = &quot;https://aip.baidubce.com/rest/2.0/ocr/v1/general_basic&quot; # 二进制方式打开图片文件,Verification_code是要识别的验证码的名字 f = open(Verification_code, &#x27;rb&#x27;) img = base64.b64encode(f.read()) params = &#123;&quot;image&quot;: img&#125; access_token = access_token request_url = request_url + &quot;?access_token=&quot; + access_token headers = &#123;&#x27;content-type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;&#125; response = requests.post(request_url, data=params, headers=headers) shibie_result = response.json()[&#x27;words_result&#x27;][0][&#x27;words&#x27;] print(shibie_result)#通过调用百度提供的接口来进行识别验证码def baiduAPI(): API_Key = &#x27;#&#x27; #应用的key Secret_Key = &#x27;#&#x27; #应用的Secret_Key Verification_code = &#x27;R-C.PNG&#x27; #图片地址 identify_Verification_code(API_Key, Secret_Key, Verification_code) 滑动验证码 滑动验证码涉及到图片拼接方面的知识。要想解决可以有以下思路，一是通过selenium模拟行为，解决滑动验证码，二是组建Cookie池绕过验证码。 通过selenium解决滑动验证码的流程： 获取图片（不带缺口的图片，带缺口的图片） 识别缺口位置（设置一个对比阈值，遍历两张图片，找出相同位置像素RGB差距，超过此阈值的像素点，此像素点的位置就是缺口的位置） 计算滑动距离 模拟运动 www&gt;m&gt;wap www是PC浏览器看到的网站，m和wap是移动端，大部分智能手机用的是m站，少部分旧手机用的还是wap。一般wap爬取较为简单，我们可以通过修改User-Agent模拟不同终端发送出请求，请求不同的页面。如何设置不同的User-Agent,我们可以在浏览器扩展选项中下载相关的User-Agent的应用。以Edge为例：下载图中的扩展应用。按照如下操作，选择需要模拟的浏览器，然后通过Verify User-Agent Setting可以查看User-Agent，以Iphone为例.获取到的请求头：1Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1 小结 对于网页爬取登录问题和验证码问题，需要对具体的网页进行分析，是通过API解析还是通过Cookie池登录，以及登录过程中的验证码破解问题。 4. 终端协议分析一个应用不仅仅具有PC端而且还具有移动客户端。所以我们在进行爬虫开发的时候，不仅仅可以从PC端进行爬取，而且还可以从移动客户端进行爬取。我们可以借助User-Agent来伪装自己，进行数据的爬取。 1. PC端数据抓包分析 将爬虫伪装成PC客户端，可以对PC客户端进行抓包分析。PC抓包软件有Wireshark,Http Analyzer等。Wireshark擅长各类网络协议分析，比较重型。而Http Analizer更注重于对Http/Https协议的分析。Http Analizer可以针对某个进程进行抓包。 Wireshark Wireshark的使用方法很简单，只需要设置过滤规则进行监听即可。简单的过滤规则，如指定监听的网址ip.addr == 121.40.103.238(这个地址是虾米音乐网地址，https://www.xiami.com/)。然后点击开始监听。则Wireshark就会监听和121.40.103.238的数据。通过cmd，ping这个地址即可。 http Analyzer http Analyzer的使用方法也很简单，点击开始监听即可。可以通过选择需要监听的进程进行监听。通过type可以选择需要过滤的数据类型。 通过抓包进行分析请求，进行数据爬取一般都是挺复杂的，涉及到逆向PC客户端软件和分析算法的能力。所以如果想要通过抓包进行爬取，需要了解相关技术进行爬取。 2. App抓包分析 移动端的抓包分析，以Android App为例。对Android应用抓包，通过下载安卓模拟器，将应用安装到模拟器中，然后进行抓包分析。 这里采用wireshake进行抓包分析，wireshake的过滤示例如下： 过滤域名包含baidu.com的数据http.host contains “baidu.com”过滤指定ip地址ip.addr == xxx.xxx.xxx.xxx 5. 初窥Scrapy爬虫框架Scrapy爬虫框架操作简单，是比较流行的爬虫解决方案，所以在这里对Scrapy进行学习。Scraoy使用Twisted这个异步网络来处理网络通信，并且包含了各种中间件接口，可以灵活的完成各种需求。 1. Scrapy各大组件 Scrapy的各个组件其实和之前学过的简单爬虫的各个模块相类似。 Scrapy 引擎(Engine) Scrapy引擎负责控制数据流在系统的所有组件中流动，并在相应动作发生时触发事件。这类似于爬虫调度器的作用。 Scrapy 调度器(Scheduler) Scrapy调度器从引擎接收Request并将它们入队，以便之后引擎请求request时提供给引擎。这类似于URL管理器的作用。 Scrapy 下载器(Downloader) Scrapy下载器负责获取页面数据并提供给引擎，而后提供给Spider。这类似于html下载器作用。 Spider Spider是Scrapy用户编写用于分析Response并提取Item(即获取到的Item)或额外跟进的URL的类。每个Spider负责处理一个特定(或一些)网站。类似于html解析器作用。 Item Pipeline Item Pipeline负责处理被Spider提取出来的Item。典型的处理有清理验证及持久化(例如存储到数据库中)。类似于数据存储器的作用。 Downloader middlewares（下载器中间件） 下载器中间件是在引擎及下载器之间的特定钩子(specific hook),处理Downloader传递给引擎的Response.其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。 Scrapy中间件(Spider middlewares) Scrapy中间件是在引擎及Spider之间的特定钩子(specific hook),处理Spider的输入(response)和输出(Items及requests)。其提供了一个渐变的机制，通过插入自定义代码来扩展Scrapy功能。 2. Scrapy的安装 这里介绍一下Windows下如何安装Scrapy(Python 3.7版本)。如果不是Python3.7版本的，需要下载对应的Pywin32。 1. Pywin32下载 下载地址 各个Pywin32的版本如图所示： 找到3.7版本，64位的安装包。 运行安装包后，通过命令行测试，如果没报错，则安装成功。 安装成功后，需要通过pip下载pywin32，或者直接通过pycharm安装pywin321pip install pywin32 2. pyOpenSSL下载 下载地址 , 或者通过pycharm下载 pyOpenSSL,或者通过 pip install pyOpenSSL 3. lxml下载 通过pycharm下载，或者pip install lxml 4. Scrapy下载 通过pycahrm下载，或者通过pip install Scrapy 如果要查看是否安装成功，通过pip list命令可以查看已经安装的库。 3. Scrapy应用 创建一个新的Scrapy项目 命令行切换到指定文件夹，运行命令 scrapy startproject xxxx ,即可创建名为xxxx的项目。 如果出现”ImportError: DLL load failed: 找不到指定的程序”错误提示，或者出现python.exe无法找到入口….pythoncom37.dll。 首先下载：1ImportError: DLL load failed: 找不到指定的程序 然后在电脑搜索pythoncom37.dll。观察无法找到入口的路径，一般都是在C盘的System32下，然后将搜索到的pythoncom37.dll所在的文件夹的内容(选择lib目录下的pywin32下的pythoncom37.dll文件夹里面的两个dll文件放到system32下)，复制到System32下，覆盖即可。 创建成功后的目录结构如图所示： 1. 创建爬虫模块 爬虫模块都放置于spiders文件夹中。爬虫模块用于从单个网站或多个网站爬取数据的类。包含初始页面的URL，网页链接，分析网页内容，提取数据。需要创建一个Spider类，继承scrapy.Spider类。需要定义下面三个属性：name:区别Spider，名字必须唯一。start_urls:Spider启动时进行爬取的入口URL列表。parse():Spider的一个方法，被调用时，负责解析返回数据，提取数据。 启动爬虫模块：通过命令行，切换到项目根目录下，运行scrapy crawl xxxx , 其中xxxx为Spider类的name属性。 2. 选择器 Scrapy选择器构建于lxml库之上。也可也通过Beautifulsoup库解析。 Selector的用法：Selector对象有四个基本方法 xpath(query):传入XPath表达式query，返回该表达式所有节点的selector list列表 css(query):传入CSS表达式query，返回该表达式所对应的所有节点的selector list列表 extract():序列化该节点为Unicode字符串并返回list列表 re(regex):根据传入的正则表达式对数据进行提取，返回Unicode字符串列表。 在spider类中的parse方法中，传入的一个参数为response,通过Selector(response)即可创建一个Selector对象。示例代码如下所示1234567891011121314151617181920212223242526272829class CnblogsSpider(scrapy.Spider): name = &quot;cnblogs&quot; #爬虫名称 allowed_domains = [&quot;cnblogs.com&quot;] #允许的域名 start_urls = [ &quot;http://www.cnblogs.com/qiyeboy/default.html?page=1&quot; ] # Spider启动时进行爬取的入口URL列表。 #解析函数 def parse(self, response): # #Selector使用 # selector = Selector(response) # #调用xpath # selector.xpath() # #调用css # selector.css() # #调用re # selector.re() # #调用extract # selector.extract() papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;) for paper in papers: url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0] title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0] content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] print(url,title,time,content) 3. 命令行工具 这里介绍一下Scrapy命令行功能。 创建一个项目 scrapy createproject xxxx运行一个项目scrapy crawl name（spider的name属性）运行单个spider模块scrapy runspider xxxx.py列出当前项目所有Spiderscrapy list快速创建spider模板scrapy genspider -lscrapy genspider -d basicscrapy genspider -t basic example example.com将项目部署到Scrapy服务scrapy deploy 4. 定义Item Item用于保存爬取到的数据，类似于字典类型，在之前的目录结构中，有个items.py文件用来定义存储数据的Item类。这个类需要继承scrapy.Item。示例代码如下所示：123456789101112131415161718192021222324252627import scrapyclass SpiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() passclass CnblogspiderItem(scrapy.Item): # define the fields for your item here like # 类似于字典类型，可以直接通过[]和get方法获取 url = scrapy.Field() time = scrapy.Field() title = scrapy.Field() content = scrapy.Field()if __name__ == &#x27;__main__&#x27;: item = CnblogspiderItem(title=&#x27;爬虫&#x27;,content=&#x27;爬虫开发&#x27;) print(item[&#x27;title&#x27;]) print(item.get(&#x27;title&#x27;)) print(item.keys()) print(item.items()) item2 = CnblogspiderItem(item) #Item复制 #dict和item转化 dict_item = dict(item) print(dict_item) item = CnblogspiderItem(&#123;&#x27;title&#x27;:&#x27;爬虫&#x27;,&#x27;content&#x27;:&#x27;开发&#x27;&#125;) print(item) 我们可以在原有的Item基础之上，添加更过的字段用于扩展Item。 12345678910class CnblogspiderItem(scrapy.Item): # define the fields for your item here like # 类似于字典类型，可以直接通过[]和get方法获取 url = scrapy.Field() time = scrapy.Field() title = scrapy.Field() content = scrapy.Field()class newCnblogspiderItem(CnblogspiderItem): body = scrapy.Field() title = scrapy.Field(CnblogspiderItem.fields[&#x27;title&#x27;],body=body) 如果要实现翻页功能,将提取出来的URL，构建新的Request对象，并指定解析方法。：1234567891011121314151617181920212223242526#解析函数def parse(self, response): # #Selector使用 # selector = Selector(response) # #调用xpath # selector.xpath() # #调用css # selector.css() # #调用re # selector.re() # #调用extract # selector.extract() papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;) for paper in papers: url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0] title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0] content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) yield item #将数据返回 next_page = Selector(response).re(u&#x27;&lt;a href=&quot;(\\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;) if next_page: #Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由 #谁来解析此项Request请求的响应 yield scrapy.Request(url=next_page[0],callback=self.parse) 5. 定义Item Pipeline 通过Item Pipeline可以对数据进行持久化存储，通过Item Pipeline可以对Item进行处理。Item Pipeline主要有如下几个作用： 清理HTML数据 验证爬取数据的合法性，检查Item是否包含某些字段 查重并丢弃 将数据结果保存到文件或数据库中 Item Pipeline组件是一个独立的Python类，必须实现proocess_item方法 process_item(self,item,spider)其中itme是被爬取的item，Spider对象代表着爬取该Item的Spider 示例代码如下所示:1234567891011121314class CnblogspiderPipeline(object): def __init__(self): self.file = open(&#x27;parpers.json&#x27;,&#x27;wb&#x27;) def process_item(self,item,spider): if item[&#x27;title&#x27;]: #将数据保存到文件中 line = json.dumps(dict(item))+&quot;\\n&quot; self.file.write(line) return item else: #DropItem是异常类型，如果不存在就爆出异常，进行丢弃 raise DropItem(&quot;Missing title in %s&quot;%item) 编写好的Item Pipeline需要在settings.py(在目录结构中可以找到)中，将类添加到ITEM_PIPELINES变量中。示例代码如下所示：其中的key就是Item Pipeline的类路径，value是自定义数字，当执行Item Pipeline时，按数字大小从低到高依次执行Item Pipeline。通常定义在0-1000范围内。123ITEM_PIPELINES = &#123; &#x27;spider.pipelines.SpiderPipeline&#x27;: 300,&#125; 6. 定义Item Pipeline内置数据存储 内置文本格式下载方式Scrapy提供了一些简单的存储方式。生成一个带有爬取数据的输出文件(feed)。Scrapy的输出自带各种序列化格式。 jsonJsonItemExporterJSON linesJsonLinesItemExporterCSVCsvItemExporterXMLXmlItemExporterPicklePickleItemExporterMarshalMarshalItemExporter 使用方法：通过命令行使用，例如scrapy crawl xxx -o xxx.csv 内置图片和文件下载方式Scrapy提供了可重用的Item Pipeline(MediaPipeline分为FilesPipeline和ImagesPipeline)，如果要使用ImagesPipeline需要下载pillow模块(pip install pillow)。这类Pipeline具有如下特性。 避免重新下载最近下载过的数据 指定存储的位置和方式对于ImagesPipeline还具有额外的特性： 将所有下载的图片转换为通用的格式(JPG)和模型(RGB) 缩略图生成 检测图像的宽/高，确保它们满足最小限制对于要下载的Item会在内部保存一个内部对了，避免多次下载几个Item共享的同一个图片。 对于FilesPipeline的工作流程： 在一个爬虫里，抓取一个Item，将文件URL，放入file_urls组内。 Item从爬虫内返回，进入Item Pipeline 当Item进入FilesPipeline,file_urls组内的URL将被Scrapy的调度器和下载器安排下载。 当文件下载完后，另一个字段files将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息。信息包括：下载文件的信息比如下载路径，源抓取地址，图片校验码。如果下载失败，会记录下载错误信息。 对于ImagesPipeline的工作流程： 从一个爬虫中，抓取一个Item，把图片的URL放图images_url组内。 项目从爬虫内返回，进入Item Pieline 当Item进入ImagesPipelin,images_urls组内的URL将被Scrapy的调度器和下载器安排下载。 当文件下载完成之后，另一个字段(images)将别更新到结构中。信息包括：下载图片的信息比如下载路径，源抓取地址，图片校验码。如果下载失败，会记录下载错误信息。 使用FilesPipeline: 在settings.py文件的ITEM_PIPELINES添加一条’scrapy.pipelines.files.FilesPipeline’:1 在item添加两个字段，比如 file_urls = scrapy.Filed()files = scrapy.Filed() 在settings.py中添加下载路径FILES_STORE，文件url所在的itme字段FILE_URLS_FILED，和文件信息所在item字段FILES_RESULT_FIELD。例如：1234FILES_STORE = &#x27;G:\\\\python&#x27;FILES_URLS_FIELD = &#x27;file_urls&#x27;FILES_RESULT_FIELD = &#x27;files&#x27;FILES_EXPIRES = 30 # 设置文件过期时间(天) 使用ImagesPipeline: 在settings.py文件的ITEM_PIPELINES添加一条’scrapy.pipelines.files.ImagesPipeline’:1 在item添加两个字段，比如image_urls = scrapy.Filed()images = scrapy.Filed() 在settings.py中添加下载路径IMAGES_STORE，文件url所在的itme字段IMAGES_URLS_FILED，和文件信息所在item字段IMAGES_RESULT_FIELD。IMAGES_THUMBS制作缩略图，并设置图片大小和尺寸。如果需要过滤特别小的图片可以使用IMAGES_MIN_HEIGHT和IMAGES_MIN_WIDTH来设置图片的最小高和宽。例如：12345678IMAGES_STORE = &#x27;G:\\\\python&#x27;IMAGES_URLS_FIELD = &#x27;file_urls&#x27;IMAGES_RESULT_FIELD = &#x27;files&#x27;IMAGES_THUMBS = &#123; &#x27;small&#x27;:(50,50), &#x27;big&#x27;:(270,270)&#125;IMAGES_EXPIRES = 30 # 设置文件过期时间(天) 在之前的示例基础上，在setting.py设置如下：123456789101112ITEM_PIPELINES = &#123; &#x27;spider.pipelines.CnblogspiderPipeline&#x27;: 300, &#x27;scrapy.pipelines.images.ImagesPipeline&#x27;:1&#125;IMAGES_STORE = &#x27;G:\\\\python\\spider\\spider&#x27;IMAGES_URLS_FIELD = &#x27;cimage_urls&#x27;IMAGES_RESULT_FIELE = &#x27;cimages&#x27;IMAGES_EXPIRES = 30IMAGES_THUMBS = &#123; &#x27;small&#x27;:(50,50), &#x27;big&#x27;:(270,270)&#125;然后在CnblogspiderItem添加两个字段cimage_urls和cimages。修改spider代码，用于下载图片12345678910111213141516171819202122def parse(self,response): papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;) for paper in papers: url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0] title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0] content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) request = scrapy.Request(url=url,callback=self.parse_body) #调用Request方法，并设置解析函数 request.meta[&#x27;item&#x27;] = item #将item暂存 yield request next_page = Selector(response).re(u&#x27;&lt;a href=&quot;(\\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;) if next_page: #Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由 #谁来解析此项Request请求的响应 yield scrapy.Request(url=next_page[0],callback=self.parse) def parse_body(self,response): item = response.meta[&#x27;item&#x27;] body = response.xpath(&quot;.//*[@class=&#x27;postBody&#x27;]&quot;) item[&#x27;cimage_urls&#x27;] = body.xpath(&#x27;.//img//@src&#x27;).extract()# 提取图片链接 yield item 这里容易看出ImagePipeline的执行流程，首先是将图片url放入到image_urls中，然后由Scrapy下载，最后保存到指定的路径。 自定义FilesPipline和ImagesPipeline如果要自定义FilesPipline和ImagesPipeline则需要继承FilesPipeline或者ImagesPipeline，重写get_media_requests和item_completed()方法。 12在工作流程中，管道会得到图片的URL并从项目中下载。需要重写get_media_requests方法.并对各个图片URL返回一个Request。def get_meida_requests(self,item,info): 12345678910111213141516171819202122232425class MyImagesPipeline(ImagesPipeline): # 管道通过get_media_requests接收到图片的URL并从项目中进行下载 # get_media_request会返回图片URL对应的Request # 对于返回的结果results会以二元素的元组列表形式传送到item_completed方法 # 返回格式 # success: 布尔值，成功返回True，是啊比返回False # url:图片下载的url # path:图片存储路径 # checksum:图片内容的MD5 hash def get_media_requests(self, item, info): for image_url in item[&#x27;image_urls&#x27;]: yield scrapy.Request(image_url) # 当所有图片请求完成时，item_completed方法将被调用 # results是get_media_requests下载完成之后的结果。 # item_completed需要返回一个输出。这个输出会被送到随后的ItemPipelines(在Scrapy中，item的执行顺序会根据id大小依次调用) # def item_completed(self, results, item, info): image_paths = [x[&#x27;path&#x27;] for ok,x in results if ok] if not image_paths: raise DropItem(&quot;Item contains no images&quot;) #丢弃项目DropItem item[&#x27;image_paths&#x27;] = image_paths return item 7. 启动爬虫 命令行方式scrapy crawl spider_name 使用CrawlProcess类CrawlProcess类内部会开启Twisted reactor，配置log,设置Twisted reactor自动关闭。 1234567if __name__ == &#x27;__main__&#x27;: #初始化CrawlerProcess参数 procerss = CrawlerProcess(&#123; &#x27;USER-AGENT&#x27;:&#x27;Mozilla/4.0 (compatible; MSIE 7.0 ; Windows NT 5.1)&#x27; &#125;) procerss.crawl(CnblogsSpider) #运行爬虫 procerss.start() 带启动参数的CrawlProcess类。12345procerss = CrawlerProcess(get_project_settings())procerss.crawl(CnblogsSpider) # 运行爬虫procerss.start() 使用CrawlerRunner CrawlerRunner：在spider结束后，必须自行关闭Twisted reactor 需要在CrawlerRunner.crawl所返回的对象中添加回调函数。12345configure_logging(&#123;&#x27;LOG_FORMAT&#x27;:&#x27;%(levelname)s：%(message)s&#x27;&#125;) runner = CrawlerRunner() d = runner.crawl(CnblogsSpider) d.addBoth(lambda _:reactor.stop()) reactor.run() 8. 强化爬虫 这里主要是介绍关于Scrapy的调试方法和异常，控制运行状态等内容。 1. 调试方法 Parse命令检查Spider输出的最基本方法(Parse命令)，可以在函数层上检查spider各个部分的效果。使用方法为scrapy parse —spider==spider_name -c parse -d 2例如 scrapy parse —spider==cblogs -c parse -d”https://www.baidu.com“ Scrapy shell通过Scrapy shell 可以查看spider某个位置中被处理的response，以确定期望的response是否到达特定位置。在spider类中，添加scrapy.shell.inspect_response方法。当程序运行到inspect_response方法时，会暂停，并切换进shell中，方便进行调试。如果要退出终端可以ctrl+d进行退出。代码如下所示：12345678910111213141516171819202122232425262728def parse(self,response): papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;) #添加scrapy shell from scrapy.shell import inspect_response inspect_response(response,self) for paper in papers: url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0] title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0] content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0] item = CnblogspiderItem(url=url,title=title,time=time,content=content) request = scrapy.Request(url=url,callback=self.parse_body) #调用Request方法，并设置解析函数 request.meta[&#x27;item&#x27;] = item #将item暂存 yield request next_page = Selector(response).re(u&#x27;&lt;a href=&quot;(\\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;) if next_page: #Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由 #谁来解析此项Request请求的响应 yield scrapy.Request(url=next_page[0],callback=self.parse) def parse_body(self,response): item = response.meta[&#x27;item&#x27;] body = response.xpath(&quot;.//*[@class=&#x27;postBody&#x27;]&quot;) item[&#x27;cimage_urls&#x27;] = body.xpath(&#x27;.//img//@src&#x27;).extract()# 提取图片链接 yield item loggin通过运行后的记录查看爬虫的运行状态 编译器借助编译器提供的Debug进行断点调试，查看程序运行情况。 2. 异常 之前已经使用过以恶异常也就是抛弃Item,DropItem。下图是关于Scrapy异常的介绍。 3. 控制运行状态 通过telnet访问Scrapy终端。talnet localhost 6023进入终端后，通过est()可以查看scrapy运行情况。通过engine/pause()暂停运行，通过engine.unpause()继续运行。通过engine.stop()停止运行。配置telnet，在setting.py中配置IP和端口。TELNETCONSOLE_PORT:6023 #设置为0或者None动态分配端口。TELNETCONSOLE_HOST:’127.0.0.1’ #y也就是本地地址ip 7. 实战项目：Scrapy爬虫","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python爬虫开发与项目实战-第二回合（实战）","date":"2021-11-18T08:35:25.519Z","path":"wiki/程序技术/Python/爬虫/Python爬虫2.1/","text":"爬虫的原理十分简单，通过URL，获取网页资源，再根据网页资源进一步获取需要的信息数据。 我们很容易借助requests,beautifulSoup库等实现一个简答的爬虫。实际上，到了这基本上爬虫的大概就已经学习完毕了。对于我们现在编写的爬虫与大型爬虫的差距在于以下几点： 实现方式 优化方式 稳健性我们现在仅仅只考虑功能的实现，所以还只是基础爬虫，要实现一个大型的爬虫还因该从效率，稳健性，结构化，维护等方面综合考虑。 一个爬虫的基础框架可以分为：爬虫调度器，URL管理器，HTML下载器，HTML解析器，数据存储器 爬虫调度器 负责其他四个模块的协调工作 URL管理器主要负责URL链接，维护已经爬取的URL集合和未爬取的URL集合，提供互殴去新的URL链接的接口 HTML下载器用于从URL管理器中获取未爬取的URL链接并下载HTML页面 HTML解析器用于从HTML下载器中获取已经下载的HTML页面，并从中解析出新的URL链接交给URL管理器解析出有效数据交给数据存储器。 数据存储器用于将HTML解析器解析出来的数据通过文件或数据库的形式存储起来。 爬虫的动态流程如下所示： 百度百科词条爬取项目 URL管理器 简单分布式爬虫简介这里实现简单的分布式爬虫，采用主从模式。目前大型的爬虫都采用分布式爬取，所以通过此次实践加深对分布式爬虫的理解。分布式需要考虑如何设计结构，保证各个节点稳定高效地运作。 主从模式主从模式是指由一台主机作为控制节点，负责管理所有运行网络爬虫的主机，爬虫只需要从控制节点那里接收任务，并把新生成任务提交给控制节点就可以了，在这个过程中不必与其它爬虫通信。 采用主从模式，实现简单，利于管理。而控制节点则需要与所有爬虫进行通信。 主从模式的缺陷在于，控制节点会成为整个系统的瓶颈，容易导致整个分布式网络爬虫系统性能下降。 主从模式的结构如下所示(以一台主机和两台从机为例)：控制节点（ControlNode）主要分为URL管理器，数据存储器和控制调度器。(1)控制调度器通过三个进程来协调URL管理器和数据存储器的工作：(2)一个是URL管理进程，负责URL的管理和将URL传递给爬虫节点；(3)一个是数据提取进程，负责读取爬虫节点返回的数据，将返回数据中的URL交给URL管理进程，将标题和摘要等数据交给数据存储进程；最后一个是数据存储进程，负责将数据提取进程中提交的数据进行本地存储。 对于爬虫节点(SpdierNode):包含爬虫调度器，HTML下载器，HTML解析器，主要负责对URL进行爬取，下载，然后将新的URL,data返回给控制节点(ControlNode)。爬虫调度器的执行流程： 爬虫调度器从控制节点中的url_q队列读取URL 爬虫调度器调用HTML下载器，HTML解析器获取网页中新的URL和标题摘要 爬虫调度器将新的URL和标题摘要传入result_q队列交给控制节点","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python爬虫开发与项目实战-第二回合","date":"2021-11-17T02:40:35.328Z","path":"wiki/程序技术/Python/爬虫/Python爬虫开发与项目实战-第二回合/","text":"网络爬虫：是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。 网络爬虫可以分为： 通用网络爬虫 通过搜索引擎搜索关键词，然后从搜索引擎返回的数据中，爬取想要的数据。 聚焦网络爬虫 有针对性的爬取指定的网页链接。定向爬取相关页面。 增量式网络爬虫 也就是爬取新的数据，而对于之前爬取过的数据，不会再进行爬取。 深层网络爬虫 也就是爬取一些不能通过静态链接获取的，需要用户操作后才能访问获取的Web页面。 网络爬虫结构爬虫的一般流程： 需要爬取页面的URL 读取URL，获取到网页数据 从网页数据提取有用数据 抽取网页需要进一步爬取URL，进行再次爬取 Request库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&quot;&quot;&quot;Python对HTTP请求的支持：通过urllib2和urllib或者通过request模块。一般通过request实现http请求。 #发送get请求 r = requests.get(&#x27;&#x27;) # 对于带有参数的url，可以通过建立一个map作为参数进行请求 payload = &#123;&#x27;keywords&#x27;:&#x27;blog:qiyeboy&#x27;,&#x27;pageindex&#x27;:1&#125; r = requests.get(&#x27;&#x27;,params=payload) #通过map提供参数 #发送post请求 r = requests.post(&#x27;&#x27;) &quot;&quot;&quot;import requests#chardet是用于检测文本编码import chardetdef requestTemp(): r = requests.get(&#x27;www.baidu.com&#x27;) print(r.content) # 返回字节类型的数据 print(r.text) # 返回文本形式的html print(r.encoding) # 返回网页编码格式 r.encoding = &#x27;utf-8&#x27; # 可以自定义编码格式。然后再读取网页文本数据，这样就不会乱码 print(r.text) # 这是在utf-8编码下的文本数据 #通过chardet进行解码 r.encoding = chardet.detect(r.content)[&#x27;encoding&#x27;] ### 我们可以对请求头headers进行处理,即对请求设置请求头,设置Cookie ### 对于请求头，我们可以通过f12查看请求头格式 user_agent = &#x27;Mozilla/4.0&#x27; headers = &#123;&#x27;User-Agent&#x27;:user_agent&#125; #设置请求头 #自定义Cookie cookies = dict(name=&#x27;qiye&#x27;,age=&#x27;10&#x27;) r = requests.get(&#x27;www.baidu.com&#x27;,headers=headers,cookies=cookies) # 获取响应的Cookie值 for cookie in r.cookies.keys(): print(r.cookies.get(cookie)) ### 获取返回状态编码，判断请求是否成功 if r.status_code == requests.codes.ok : print(r.status_code) print(r.headers.get(&#x27;content-type&#x27;)) else: r.raise_for_status() #通过raise_for_status可以抛出一个异常。 ###自动处理Cookie方法 , 通过session，每次都可以将Cookie值带上 s = requests.Session() r = s.get(&quot;wwww.baidu.com&quot;,allow_redirects=True) datas = &#123;&#x27;name&#x27;:&#x27;qiyi&#x27;,&#x27;passwd&#x27;:&#x27;123&#x27;&#125; # 通过Session机制，可以保证每次都加上了cookie的值，进行请求 r = s.post(&#x27;wwww.baidu.com&#x27;,data=datas,allow_redirects=True) # 通过allow_redirects可以设置是否允许重定向 # 通过r.history可以获取到历史信息，也就是访问成功之前的所有请求跳转信息 print(r.history) #设置超时参数，Timeout r = requests.get(&#x27;www.baidu.com&#x27;,timeout=2) #代理设置，使用代理Proxy,可以为任意请求方法通过设置proxies参数来配置单个请求 proxies = &#123; &quot;http&quot;:&quot;http://0.10.1.10:3128&quot;, &quot;https&quot;:&quot;http://10.10.1.10:1080&quot;, #&quot;http&quot;:&quot;http://user:pass@10.10.1.10:3128&quot; #这是代理中身份认证的用户名和密码，来设置代理 &#125; requests.get(&quot;www.baidu.com&quot;,proxies=proxies) #设置代理ip 网页解析我们通过requets可以下载网页中的文本，那么我们怎么通过下载的文本获取到想要的信息呢？这里就需要使用到文本解析技术了，常用的方法有很多，我们这里采用BeautifulSoup库进行解决。BeautifulSoup具备的解析器如下所示： BeautifulSoup将HTML文档转换成一个复杂的树形结构，每个节点都是一个python对象，所有对象可以归纳为四种。 Tag NavigableString BeautifulSoup Comment BeautifulSoup对网页的解析，主要是搜索指定标签，遍历标签元素，提取标签内容。对于解析文本的方法，可以参考如下代码，本质上就是调用API定位到要爬取到的数据信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153&quot;&quot;&quot;正则表达式：\\b: 匹配单词的开始或结束^: 匹配字符串的开始$: 匹配字符串的结束\\w: 匹配字母，数字，下划线或汉字\\s: 匹配任意空白字符\\d: 匹配数字. : 匹配除换行符以外的任意字符字符串转义: 通过\\进行转义数量匹配：*：零次或多次+： 一次或多次？：零次或一次&#123;n&#125;: n次&#123;n,&#125;: n次或更多次&#123;n,m&#125; 重复n-m次分支条件：正则表达式通过 | 表示或的关系。&quot;&quot;&quot;&quot;&quot;&quot; 这里采用BeautifulSoup(美味汁)进行文本解析&quot;&quot;&quot;from bs4 import BeautifulSoupimport requestsimport rer = requests.get(&quot;http://www.baidu.com&quot;)r.encoding = &#x27;utf-8&#x27;html = r.text# 可以直接打开，不知道解析器，一般采用lxml解析器soup = BeautifulSoup(html,&#x27;lxml&#x27;,from_encoding=&#x27;utf-8&#x27;)def 获取对象属性(): #Tag 对象 #Tag对象就像html标签一样，直接通过soup.标签名进行对象的提取 #要获取tag对象的属性，通过标签.name获取标签的名称 ， 并且可以设置标签的名称 #如果需要获取标签的属性，如href和class之类的属性，则通过标签.get(属性名) print(soup.a) # 根据标签获取 print(soup.a.name) #获取标签名称 print(soup.a.get(&#x27;href&#x27;)) # 获取属性名称 print(soup.a.attrs) # 获取标签中的所有属性 # 如果要修改标签中的属性，怎么获取也同样可以怎么设置 # BeautifulSoup用NavigableString类来包装Tag中的字符串，通过标签.string就可以获取到标签内部的文字 print(soup.a.string) print(type(soup.a.string)) #BeautifulSoup对象表示的是一个文档的全部内容。def 遍历(): ### 遍历操作， beautiful soup 对文档树的遍历 &quot;&quot;&quot; 1. contents 2. children &quot;&quot;&quot; # contents属性，可以将tag的子节点列表输出 print(soup.head.contents) # children属性返回的是一个生成器。可以对Tag的子节点进行循环 # 也就是通过children可以迭代遍历节点的所有子节点. for child in soup.head.children: print(child) # 如果要递归遍历所有标签的孙子结点，则通过desendants属性，对所有tag的子孙节点进行循环 # 标签的内容也属于标签的子节点 for child in soup.head.descendants: print(child) # 获取结点的内容 # stirng , 如果标记唯一就返回标记的内容，而如果标记不唯一，可能返回None print(soup.a.string) # strings属性主要用于tag中包含多个字符串的情况，可以循环遍历 for string in soup.strings: print(string) # stripped_strings 可以去掉输出字符串包含的空格或空行 for string in soup.stripped_strings: print(string) # 获取父节点，通过parent属性 print(soup.a.parent) # 获取节点的所有父辈节点，通过parents属性 for parent in soup.a.parents: if parent is None: print(parent) else: print(parent.name) # 获取节点的兄弟节点,空白或换行也可也被视作一个节点 # next_sibling获取下一个兄弟节点 print(soup.a.next_sibling) # previous_sibling获取上一个兄弟节点 print(soup.a.previous_sibling) # 通过next_siblings或者previous_siblings可以对当前兄弟节点迭代输出 for sibling in soup.a.next_siblings: print(sibling) # 获取节点的前后节点。前后节点是不区分层次结构的前后关系，如&lt;div&gt;&lt;a&gt;&lt;div&gt;,div的后一个节点就是a # next_element,previous_element, print(soup.a) print(soup.a.next_element) # 如果想遍历所有前后节点，通过next_elements和previous_elements进行遍历 for element in soup.a.next_elements: print(element)# 搜索文档树，搜索指定的内容def 搜索(): # find_all()方法，搜索当前tag的所有tag子节点，判断是否满足搜索条件 # find_add(name,attrs,recursive,txext,**kwargs) # name参数可以查找所有名字为name的标记,返回列表.一般用这个来找指定的标签 # name参数可以是单独的字符，也可以是字符列表。 # 可以自定义过滤器，用于匹配指定规则的标签 print(soup.find_all(&#x27;a&#x27;)) print(soup.find_all([&#x27;a&#x27;,&#x27;b&#x27;])) def hasClass_Id(tag): return tag.has_attr(&#x27;class&#x27;) and tag.has_attr(&#x27;id&#x27;) print(soup.find_all(hasClass_Id)) # 寻找满足匹配规则的标签 # 多条件过滤标签 # 可以在find_all()中根据属性搜索指定的标签，并且可以将正则表达式作为搜索条件 print(soup.find_all(&#x27;a&#x27;,href=re.compile(&#x27;elsie&#x27;),id=&#x27;12&#x27;,class_=&#x27;sister&#x27;)) #如果要限制搜索数目，则通过limit参数进行限制 print(soup.find_all(&#x27;a&#x27;,limit=5)) # 限制只搜索直接节点，而不搜索子孙节点，设置recursive = False print(soup.find_all(&#x27;a&#x27;,limit=5),recursive=False) # CSS选择器 # 通过元素的CSS属性定位元素的位置 # 根据name属性通过.class值 , 根据id属性通过#id值 # 返回类型为list #找到所有a标签 soup.select(&#x27;a&#x27;) #找到a标签，id为1 soup.select(&#x27;a#1&#x27;) #根据name查询 soup.select(&#x27;.classs&#x27;) #通过判断是否存在某个属性进行查找 soup.select(&#x27;a[href]&#x27;) #通过属性值查找 ， test可以是待查找的字符串，可以通过正则表达式查询 #href^= &quot;&quot; , href$= , href*= , 进行正则判断。 soup.select(&#x27;a[href=&quot;test&quot;]&#x27;)","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"Python爬虫与开发项目实战——第一回合","date":"2021-11-15T07:44:24.136Z","path":"wiki/程序技术/Python/爬虫/Python爬虫开发与项目实战/","text":"背景介绍&nbsp;&nbsp;&nbsp;&nbsp;这一系列主要是对爬虫进一步深入了解,学习之前已经了解过爬虫，并编写过相应的代码，现在阅读书籍，进一步对爬虫的原理进行理解。大家可以通过该系列的开发过程，掌握爬虫的运用，以及进一步了解爬虫的原理。 序列化 把内存中的变量变成可存储或可传输的过程，就是序列化。将内存中的变量序列化之后，可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上，实现程序状态的保存和共享。反过来，把变量内容从序列化的对象重新读取到内存，称为反序列化。 也就是说，序列化就是保存了变量的一个快照(某个时刻的值)。反序列化就是根据保存的快照，将值赋值给变量，这样就回到了那个时刻(因为各个变量的值都一样)。Python对序列化的支持：cPickle和pickle来实现序列化。一般都是先导入cPickle模块。实例代码如下:1234567891011121314151617181920212223242526272829### 导入序列化模块try: import cPickle as pickleexcept ImportError: import pickle&quot;&quot;&quot; pickle实现序列化主要使用dumps方法或dump方法， dumps方法可以将任意对象序列化成一个str，然后可以将这个str写入文件进行保存. dump方法可以将序列化后的对象直接写入到文件中 pickle实现反序列化，主要通过loads或load方法，把序列化后的文件从磁盘上读取一个str，然后使用loads方法将str转化位对象 或者直接使用load方法将文件直接反序列化位对象&quot;&quot;&quot;#dumpsd = dict(url=&quot;index.html&quot;,title=&quot;首页&quot;,content=&quot;首页&quot;)str = pickle.dumps(d)print(str)#dumpf = open(r&#x27;dump.txt&#x27;,&#x27;wb&#x27;)pickle.dump(d,f)f.close()#通过load方法f = open(r&#x27;dump.txt&#x27;,&#x27;rb&#x27;)d = pickle.load(f)f.close()print(d) 进程和多进程 python对多进程的方法：一种通过os模块的fork方法(适用于unix和linux操作系统)，一种通过multiprocessing模块(跨平台的实现方式)。这里主要采用Multiprocessing来创建多进程.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#multiprocessing模块创建多进程#通过Process类来描述一个进程对象，创建子进程时，只需要传入一个执行函数和函数参数，#即可完成一个Process实例的创建，用strt()方法启动进程，用join方法实现进程间的同步#子进程要执行的代码def run_proc(name): print(&#x27;Child process %s (%s) Running...&#x27;%(name,os.getpid()))##进程池任务def run_task(name): print(&#x27;Task %s (pid=%s) is runing...&#x27;%(name,os.getpid())) time.sleep(random.random()*3) print(&#x27;Task %s end.&#x27;%name)#写数据进程执行的代码def proc_write(q,urls): print(&#x27;Process(%s) is writing...&#x27;%os.getpid()) for url in urls: q.put(url) print(&#x27;Put %s queue...&#x27;%url) time.sleep(random.random())#读数据进程执行的代码def proc_read(q): print(&#x27;Process(%s) is reading&#x27;%os.getpid()) while True: url = q.get(True) print(&#x27;Get %s from queue.&#x27;%url)def 多进程(): # print(&#x27;Parent process %s.&#x27;%os.getpid()) # for i in range(5): # p = Process(target=run_proc,args=(str(i),))#指定子进程要执行的方法，以及传递的参数 # print(&#x27;Process will start.&#x27;) # p.start() # p.join() # print(&#x27;Process end&#x27;) # 通过进程池创建多个进程 print(&#x27;Current process %s.&#x27; % os.getpid()) p = Pool(processes=3) # 创建进程池，指定进程池中进程的个数，默认位CPU核数 for i in range(5): p.apply_async(run_task, args=(i,)) print(&#x27;Waiting for all subprocess done ...&#x27;) p.close() # g关闭进程池，就不能继续向进程池中添加新的任务 p.join() # 使用join表示等待所有子进程结束 # 进程通信 通过Queue或者Pipe(一般用于两个进程通信)实现进程通信 # 通过Queue进行get和put操作，可以设置blocked和timeout两个属性，blocked是设定操作是否阻塞 # timeout是设置操作的等待时间。 q = Queue() # 创建消息队列 proc_write1 = Process(target=proc_write, args=(q, [&#x27;url_1&#x27;, &#x27;url_2&#x27;, &#x27;url_3&#x27;])) proc_write2 = Process(target=proc_write, args=(q, [&#x27;url_4&#x27;, &#x27;url_5&#x27;, &#x27;url_6&#x27;])) proc_reader = Process(target=proc_read, args=(q,)) # 启动子进程proc_writer写入 proc_write1.start() proc_write2.start() # 启动子进程读取 proc_reader.start() # 等待写入结束 proc_write1.join() proc_write2.join() # 由于读取是死循环，所以只能强行终止 proc_reader.terminate() 多线程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#多线程def thread_run(urls): print(&#x27;Current %s is running ...&#x27;%threading.current_thread().name) for url in urls: print(&#x27;%s ------&gt;&gt;&gt; %s&#x27;%(threading.current_thread().name,url)) time.sleep(random.random()) print(&#x27;%s ended.&#x27;% threading.current_thread().name)#继承创建线程类class MyThread(threading.Thread): def __init__(self,name,urls): threading.Thread.__init__(self,name=name) self.urls =urls def run(self): print(&#x27;Current %s is running ...&#x27; % threading.current_thread().name) for url in self.urls: print(&#x27;%s ------&gt;&gt;&gt; %s&#x27; % (threading.current_thread().name, url)) time.sleep(random.random()) print(&#x27;%s ended.&#x27; % threading.current_thread().name)if __name__ == &#x27;__main__&#x27;: print(&#x27;in&#x27;) &quot;&quot;&quot;&quot; 多线程，线程类似于执行多个不同的程序，多线程可以将允许时间长的任务放到后台处理。 Python对多线程的支持，thread和threading，一般我们使用threading模块 &quot;&quot;&quot; print(&#x27;%s is running...&#x27;%threading.current_thread().name) t1 = threading.Thread(target=thread_run,name=&#x27;Thread_1&#x27;,args=([&#x27;url_1&#x27;,&#x27;url_2&#x27;,&#x27;url_3&#x27;],)) t2 = threading.Thread(target=thread_run, name=&#x27;Thread_2&#x27;, args=([&#x27;url_4&#x27;, &#x27;url_5&#x27;, &#x27;url_6&#x27;],)) t1.start() t2.start() t1.join() t2.join() print(&#x27;%s ended.&#x27;%threading.current_thread().name) #自定义线程类 print(&#x27;%s is running...&#x27; % threading.current_thread().name) t1 = MyThread( name=&#x27;Thread_1&#x27;, urls=([&#x27;url_1&#x27;, &#x27;url_2&#x27;, &#x27;url_3&#x27;])) t2 = MyThread( name=&#x27;Thread_2&#x27;, urls=([&#x27;url_4&#x27;, &#x27;url_5&#x27;, &#x27;url_6&#x27;])) t1.start() t2.start() t1.join() t2.join() print(&#x27;%s ended.&#x27; % threading.current_thread().name) ##线程同步 ##一般通过Thread的Lock和RLock实现简单的线程同步，两个对象都有acquire和release方法 mylock = threading.RLock() #创建一个锁 num = 0 class myThread1(threading.Thread): def __init__(self,name): threading.Thread.__init__(self,name=name) def run(self): global num while True: mylock.acquire() print(&#x27;%s locked,Number :%d&#x27;%(threading.current_thread().name,num)) if num &gt;=4 : mylock.release() print(&#x27;%s released, Number: %s&#x27;%(threading.current_thread().name,num)) break num += 1 print(&#x27;%s released Number %s&#x27;%(threading.current_thread().name,num)) mylock.release() thread1 = myThread1(&#x27;Thread_1&#x27;) thread2 = myThread1(&#x27;Thread_2&#x27;) thread1.start() thread2.start() 协程 12345678910111213141516171819202122#各个协程执行的任务def run_task(url): print(&#x27;Visis --&gt; %s&#x27;%url) try: data = urllib3.connection_from_url(url=url).urlopen(url=url,method=&quot;GET&quot;).data print(&#x27;%s bytes received from %s.&#x27;%(len(data),url)) except Exception as e: print(e)if __name__ == &#x27;__main__&#x27;: print(&#x27;in&#x27;) # 协程又称微线程(纤程),用户级的轻量级线程 #协程能够保留上一次调用时的状态 #Python对写出的支持通过gevent库，Python通过yield提供对协程的基本支持但是不完全，所以使用 #gevent更加好 #gevent实际上是greenlet在实现切换工作。如果出现io阻塞的时候，gevent会自动切换到没有阻塞的代码执行 #所以gevent一直保持greenlet在允许 urls = [&#x27;https://github.com/&#x27;,&#x27;https://www.python.org/&#x27;,&#x27;http://www.baidu.com/&#x27;] # 各个协程访问的网址 greenlets = [gevent.spawn(run_task,url) for url in urls] # 这里将各个协程加入到greenlets中 gevent.joinall(greenlets=greenlets) #进行执行各个协程 分布式进程 分布式进程：分布式也就是将计算任务分布到多个计算机上进行运算，然后将结果返回。分布式进程也就是指，将Process进程分不到多台机器，利用多台机器的性能，完成复杂的任务。 对于分布式进程，通过multiprocessing模块的managers子模块，将多线程分布到多台机器上。 一般我们通过分布式处理任务，将某块的任务分配给某个机器执行，然后某个功能模块给其他模块执行。将任务分成多个计算机集群进行处理，提高速度。例如爬取图片，可以一个计算机专门爬取图片链接，然后多个计算机专门根据爬取到的图片链接下载图片。将中间处理的结果，让其他机器进程都能访问的过程称为本地队列的网络化。过程如下所示： 两个实现代码 taskManager.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import queuefrom multiprocessing.managers import BaseManagerfrom multiprocessing import freeze_support#服务进程#任务个数task_number = 10#定义收发队列task_queue = queue.Queue(task_number)result_queue = queue.Queue(task_number)def get_task(): return task_queuedef get_result(): return result_queue#创建类似的QueueManagerclass QueueManager(BaseManager): passdef win_run(): #Windows下绑定调用接口 QueueManager.register(&#x27;get_task_queue&#x27;,callable=get_task) QueueManager.register(&#x27;get_result_queue&#x27;,callable=get_result) #绑定端口并设置验证口令，Windows下需要填写IP地址 manager = QueueManager(address=(&#x27;127.0.0.1&#x27;,8001),authkey=&#x27;qiye&#x27;.encode(&#x27;utf-8&#x27;)) #启动 manager.start() try: #通过网络获取任务队列和结果队列 task = manager.get_task_queue() result = manager.get_result_queue() #添加任务 for url in [&#x27;ImageUrl_&#x27;+str(i) for i in range(10)]: print(&#x27;put task %s...&#x27;%url) task.put(url) print(&#x27;try get result...&#x27;) for i in range(10): print(&#x27;result is %s &#x27;%result.get(timeout=10)) except: print(&#x27;Manager error&#x27;) finally: #一定要关闭，否则会报管道未关闭的错误 manager.shutdown()if __name__ == &#x27;__main__&#x27;: #Windows多线程可能有问题，进行缓解 freeze_support() win_run() taskWorker.py1234567891011121314151617181920212223242526272829import timefrom multiprocessing.managers import BaseManager#处理进程#创建类似的QueueManagerclass QueueManager(BaseManager): pass#第一步使用Queuemanager注册用于获取Queueu的方法名称QueueManager.register(&#x27;get_task_queue&#x27;)QueueManager.register(&#x27;get_result_queue&#x27;)#第二部连接到服务器server_addr = &#x27;127.0.0.1&#x27;print(&#x27;Connect to server %s...&#x27;%server_addr)#端口哦和验证口令m = QueueManager(address=(server_addr,8001),authkey=&#x27;qiye&#x27;.encode(&#x27;utf-8&#x27;))#从网络链接m.connect()#第三步获取Queue的对象task = m.get_task_queue()result = m.get_result_queue()#第四步，从task队列获取任务，把结果写入到result队列while(not task.empty()): image_url = task.get(True,timeout=5) print(&#x27;run task download %s...&#x27;%image_url) time.sleep(1) result.put(&quot;%s---&gt;success&quot;%image_url)#处理结束print(&#x27;worker exit .&#x27;) 网络编程 两台计算机之间的通信，实际上是两台计算机，端口之间的通信。当在浏览器浏览网页的时候，实际上就是本地计算机的一个端口进程和服务器的某个端口进程建立了连接，并进行通信。 一般通过Socket（套接字）,描述通信。Socket由ip+端口组成。Python提供了两个Socket模块。Socket：提供了标准的BSD Sockets APISocketServer: 提供了服务器中心类，可以简化网络服务器的开发。 Socket类型套接字格式为：socket(family,type[,protocal]),使用给定的地址族，套接字类型，协议编号（默认为0）来创建套接字。套接字类型如下所示： Socket常用函数 TCP编程 TCP-Server.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 网络编程包含两个部分：服务端和客户端# TCP是面向连接的通信方式# 主动发起连接的叫做客户端# 被动响应连接的叫服务端# 创建TCP服务端&quot;&quot;&quot;1. 创建Socket,绑定Socket到本地IP与端口2. 开始监听连接3. 进入循环，不断接收客户端的连接请求4.接收传来的数据，并发送给对方数据5.传输完毕后，关闭Socket&quot;&quot;&quot;import socketimport threadingimport timedef dealClient(sock,addr): # 第四步：接收传来的数据，并发送给对方数据 print(&#x27;Accept new connection from %s:%s...&#x27;%(sock,addr)) sock.send(b&#x27;Hello,I am server!&#x27;)#发送数据 while True: data = sock.recv(1024) #接收数据 time.sleep(1) if not data or data.decode(&#x27;utf-8&#x27;) == &#x27;exit&#x27;: break print(&#x27;----&gt;&gt;&gt;%s!&#x27;%data.decode(&#x27;utf-8&#x27;)) sock.send((&#x27;Loop_Msg:%s!&#x27;%data.decode(&#x27;utf-8&#x27;)).encode(&#x27;utf-8&#x27;)) #关闭Socket sock.close() print(&#x27;Connection from %s:%s closed.&#x27;%(sock,addr))if __name__ == &#x27;__main__&#x27;: #第一步：创建基于IPv4和TCP协议的Socket #Socket绑定IP（127.0.0.1为本机IP）与端口 s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.bind((&#x27;127.0.0.1&#x27;,9999)) #第二步：监听连接 s.listen(5)#最大连接数量 print(&#x27;Waiting for connection...&#x27;) while True: #第三步: 接收一个新连接 sock,addr = s.accept() #创建新线程来处理TCP连接 t = threading.Thread(target=dealClient,args=(sock,addr,)) t.start()TCP-Client.py123456789101112131415161718192021#TCP客户端&quot;&quot;&quot;1.创建Socket，连接远端地址2.连接后发送数据和接收数据3.传输完毕后，关闭Socket&quot;&quot;&quot;import socket#初始化Sockets = socket.socket(socket.AF_INET,socket.SOCK_STREAM)#连接目标的ip和端口s.connect((&#x27;127.0.0.1&#x27;,9999))#接收消息print(&#x27;---&gt;&gt;&gt;&#x27;+s.recv(1024).decode(&#x27;utf-8&#x27;))#发送消息s.send(b&#x27;Hello,I am Client&#x27;)print(&#x27;---&gt;&#x27;+s.recv(1024).decode(&#x27;utf-8&#x27;))s.send(b&#x27;exit&#x27;)#关闭Sockets.close() UDP编程 UDP-Server.py123456789101112131415161718192021222324# UDP 服务器端&quot;&quot;&quot;UDP是面向无连接的协议。使用UDP协议时，不需要建立连接，只需要直到对方的IP地址和端口号，就可以直接发数据包。并不关心能够到达目的端。对于不要求可靠到达的数据，就可以使用UDP协议。&quot;&quot;&quot;&quot;&quot;&quot;服务端创建过程： 1. 创建Socket，绑定指定的ip和端口 2. 直接发送数据和接收数据 3. 关闭Socket&quot;&quot;&quot;import socket# 创建Socket,绑定指定IP和端口# SOCK_DGRAM指定了这个Socket的类型是UDP，绑定端口和TCP示例一样s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)s.bind((&#x27;127.0.0.1&#x27;, 9999))print(&#x27;Bind UDP on 9999...&#x27;)while True: # 直接发送数据和接收数据 data, addr = s.recvfrom(1024) print(&#x27;Received from %s:%s. &#x27; %(addr,data.decode(&#x27;utf-8&#x27;))) s.sendto(b&#x27;Hello,!&#x27;, addr) DUP-Client.py 1234567891011121314#UDP的客户端# UDP客户端，创建Socket即可与服务器数据交换。import sockets = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)for data in [b&#x27;Hello&#x27;,b&#x27;World&#x27;]: #发送数据 s.sendto(data,(&#x27;127.0.0.1&#x27;,9999)) #接收数据 try: print(s.recv(1024).decode(&#x27;utf-8&#x27;)) except: passs.close() 小结 两个计算机之间的通信是端口进程之间的通信。端口属于应用层。对于应用层之间的数据，通过运输层提供相应的数据。运输层的协议，有TCP和UDP传输协议。 对于TCP协议，是面向连接的，所以在使用tcp协议传送数据的时候，需要保证两个通信的主机建立了连接，然后再进行数据的传输。&gt; 所以对于TCP而言，服务器和客户端的连接步骤对于TCP服务器： 创建Socket，绑定到对应端口 监听端口，检测是否有连接 如果有客户端连接，接收客户端连接，可以获得客户端的套接字 建立连接后，可以循环监听接收客户端发送的数据，也可发送数据给客户端 会话完毕，就可以关闭连接 对于TCP客户端 创建Socket 连接到服务器，然后建立连接 发送，接收数据 会话完毕，关闭连接 对于UDP协议，是面向无连接的，所以在使用UDP协议的时候，并不需要确定双方是否建立了连接，直接向对应的（地址，端口号），发送数据即可，不关心是否发送成功。同理，接收数据也是直接接收即可。因为发送的时候就是直接发到对应主机，然后接收直接接收即可。 对于UDP服务器 创建Socket,绑定端口 监听端口，查看是否有数据 接收到数据（data,addr），可以发送给数据到指定的addr 对于UDP客户端 创建Socket 向服务器(addr,post)发送数据 可以监听自己的计算机，是否有其他计算机发送的数据 总结 从上面TCP/UDP客户端和服务器的创建过程可以比较出，TCP需要确定连接，而对于UDP而言不需要确定是否连接成功。 WEB前端基础 W3C标准 W3C即万维网联盟，W3C标准是一系列标准的集合。 一个网页由三部分组成：结构(Structure),表现(Presentation),行为(Behavior)。对应 结构化标准语言：XHTML，XML表现标准语言：CSS行为标准：对象模型(如W3C DOM)，ECMAScript等。 HTML（超文本标记语言），也就是是一种标签语言。 CSS(层叠样式表): 用于解决内容和表现的分离。CSS主要由选择器+属性构成。选择器用于指定渲染元素，属性用于指定渲染效果。 JavaScript（轻量级脚本语言） XPath: 一门在XML文档中查找信息的语言。 JSON：JavaScript对象表示法，用于存储和交换文本信息。 HTTP协议(超文本传输协议)，用于从www服务区传输超文本到本地浏览器的传送协议。HTTP协议永远都是客户端发起请求，服务器会送响应。 HTTP协议是一个无状态协议，同一个客户端的这次请求和上次请求没有对应关系。 HTTP状态码： 200—请求成功301—资源网页被永久转移到其他URL404—请求的资源不存在500—内部服务器错误 1开头一般表示服务器收到请求，需要请求者继续执行操作2开头一般表示请求操作成功3开头一般表示重定向4开头一般表示客户端错误5开头一般表示服务器错误 Cookie和Session都用于保存状态信息，Cookie保存在客户端，Session保存在服务器端。 Cookie：服务器给每个Session分配一个唯一的JSESSIONID，并通过Cookie发送给客户端，当客户端发起新的请求的时候，将在Cookie头中携带JSESSIONID。这样服务器就能够找到这个客户端对应的Session。","tags":[{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/tags/%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://example.com/categories/%E7%88%AC%E8%99%AB/"}]},{"title":"4. Java对象与类","date":"2021-11-03T07:05:53.433Z","path":"wiki/程序技术/Java/JavaSE/第四章-对象与类/class/","text":"面向对象程序设计OOP(Object Oriented Programming)。 类用于描述对象所具有的属性和方法，具体的对象称为类的一个实例。 封装也就是将数据和行为组合在一个包中，并对对象的使用者隐藏了数据的实现方式。 对象的三个特性，行为，状态，标识。 对象的行为描述了对象具有哪些操作，对象的状态（状态也就是对象当前的特征信息）决定了在不同情况下相应的操作的响应，对象的标识描述了不同的实例对象。 类之间的关系 依赖关系(uses-a) 如果一个类A的方法使用了另一个类B，那么就形成了依赖关系。 A依赖于B。（过多的依赖会使得类之间的耦合度高） 聚合关系(has-a) 如果类A包含一些类B，那么A和B是聚合关系。 继承关系(is-a) 父类和子类的关系，也就是继承关系。 UML（Unified Modeling Language,统一建模语言）中类关系的表示： Java中的对象通过new进行创建，通过new创建一个新的对象变量。一个对象变量并没有实际包含一个对象，而仅仅引用一个对象。通过new 操作符的返回值也是一个引用。这可以理解为Java中所有的对象变量实际上就是一个指针，指向实际对象所在的地址。 更改器方法(get),访问器方法(set) 注意不要编写返回引用可变对象的访问器方法。如果返回了可变对象，那么在类外部对返回对象的改变，同时也会影响到类内部的变量。如果需要返回一个可变对象的引用，首先因该对它进行克隆(clone)。对象的clone是指存放在另一个位置上的对象副本。如果需要返回一个可变数据域的拷贝，就应该使用 clone。 1234567class Employee&#123; public boolean equals(Employee other) &#123; return name.equals(other.name); &#125;&#125; 对于上段代码，我们可以看出，在equals方法中，调用了other的私有变量。对于Javaa来说，类的方法可以访问该类的任何对象的遍私有变量。类方法可以访问所属类的私有特性。 构造器 •构造器与类同名 •每个类可以有一个以上的构造器 •构造器可以有 0 个、1 个或多个参数 •构造器没有返回值 •构造器总是伴随着 new 操作一起调用 隐式参数与显示参数 以下列代码为例： 123456789101112class B &#123; private int a ; public B(int a)&#123; this.a = a ; &#125; public int getA()&#123;return this.a;&#125; public void setA(int a)&#123; this.a = a ; &#125; &#125; 我们对于方法setA,可以看出对于参数a是显示的参数。而隐式参数是谁呢？隐式参数是调用该方法调用的类对象，通过关键字this表示隐式参数。 第一个参数称为隐式 （ implicit ) 参数， 是出现在方法名前的Employee 类对象。第二个参数位于方法名后面括号中的数值，这是一个显式 （ explicit)参数（ 有些人把隐式参数称为方法调用的目标或接收者。 也就是说，this表明该方法的调用者。 访问修饰符 修饰符 作用 作用域 public 公开的 任意类都能访问 private 私有的 只能被定义它们的类使用 final final 修饰符大都应用于基本 （primitive ) 类型域，或不可变（immutable)类的域（如果类中的每个方法都不会改变其对象， 这种类就是不可变的类)。 static 静态域（类域）和静态方法。对于静态域变量，该变量属于类，而不属于任何独立的对象，通过类就能够直接访问到静态域变量(公开的)，实例对象，也能够访问到该变量，但是该变量属于类，所有对象访问到的静态域对象都是一样的。对于静态域一般可以用于设置静态常量，通过类名就能引用，但是不会被修改。对于静态方法，加在方法名之前，静态方法可以通过类名直接调用，静态方法不属于单独的对象。静态方法不具有this参数，所以静态方法不能访问非静态域的实例域。 在下面两种情下使用静态方法： 一 方法不需要访问对象状态，其所需参数都是通过显式参数提供（例如：Math.pow) 一个方法只需要访问类的静态域（例如：Employee.getNextld）。 初始化数据域 默认域初始化 如果在构造器中没有显式地给域赋予初值，那么就会被自动地赋为默认值： 数值为 0、布尔值为 false、 对象引用为 null。尽量对对象进行赋初始值，而不是采用默认初始值 构造器初始化通过this可以调用同一个类的另一个构造器。如下所示：12345public Employee(String double)&#123; // calls Employee(String, double) this(&quot;Employee #&quot; + nextld, s); nextld++;&#125;通过this，引用到了同一个Employee的另一个的构造函数，对类对象进行初始化。 在声明中赋值 在初始化块赋值 初始块也就是通过{}包含的一段代码块。只要构造类的对象，这些块就会被执行。初始化块的执行顺序，首先运行初始化块，然后才运行构造器的主体部分。 对于静态域的初始化，需要加上关键字staticstatic{ //进行初始化} Java调用构造器的步骤 所有数据域被初始化默认值(0,false,null) 按照类声明中出现的次序，依次执行所有域初始化语句或初始化块。 如果构造器第一行调用了其它构造器，则调用其它构造器 执行这个构造器主体。 构造器是最后执行的，先执行声明和代码块语句。 Java参数调用方法参数共有两种类型： 基本数据类型（数字、布尔值K 对象引用。 Java 程序设计语言总是采用按值调用。也就是说， 方法得到的是所有参数值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容。一个方法，无法改变基本数据类型的参数，但是对象引用作为参数就不同了。对于引用参数就不一样了，方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 总结 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。（因为Java的引用对象，并不是真正的引用(只是值的传递),可以通过反证进行证明） 重载(overloading)和重写(overwrite) 重载 如果多个方法（比如， StringBuilder 构造器方法）有相同的名字、 不同的参数，便产生了重载。对于不同的重载方法， 通过方法给出的参数类型和特定的方法调用所使用的值类型进行匹配来挑选相应的方法（返回值不作为特征）。。如果编译器找不到匹配的参数， 就会产生编译时错误，因为根本不存在匹配， 或者没有一个比其他的更好。（这个过程被称为重载解析（overloading resolution)。） 对于一个方法的签名(signature)，包含方法名，方法参数类型构成。方法签名=方法名(参数类型)。 重写 Java的包Java允许使用包(package)将类组织起来，借助于包可以方便地组织自己的代码，并将自己的代码与别人提供的代码库分开管理。使用包的主要原因是确保类名的唯一性(全限定类名)，一般我们对包名的命名规则是以因特网域名的逆序形式作为包名，并且对于不同的项目使用不同的子包。 一个类可以使用所属包中的所有类，以及其他包中的公有类(public class)。 import语句不仅可以导入类，还增加了导入静态方法和静态域的功能。 import static java.lang.System.*通过上述语句就可以直接使用System类的静态方法和静态域，而不必加类名前缀。这样就可以直接在程序中使用out.println(“hello world”);而不用加前缀。 把一个类放到一个包中， 需要通过package 指定所属的包。如果没有设置，则类被放在默认的包(default package)。 对于包的作用域 对于标记public部分，可以被任意的类使用。标记为private部分，只能被定义它们的类使用。如果没有指定public或private，这个部分(类，方法或变量)可以被同一个包中的所有方法访问。 类路径 为了使类能够被多个程序共享，需要做到下面几点： 1 ) 把类放到一个目录中， 例如 /home/user/classdir。需要注意， 这个目录是包树状结构的基目录。如果希望将 com.horstmann.corejava.Employee 类添加到其中，这个Employee.class类文件就必须位于子目录 /home/user/classdir/com/horstmann/corejava 中。 2 ) 将 JAR 文件放在一个目录中，例如：/home/user/archives。 3 ) 设置类路径（classpath)。类路径是所有包含类文件的路径的集合。 类路径包括：•基目录 /home/user/classdir或 c:\\classes； •当前目录 (.); •JAR 文件 /home/user/archives/archive.jar或c:\\archives\\archive.jar 从 Java SE 6 开始，可以在 JAR 文件目录中指定通配符，如下：/home/user/dassdir:.:/home/aser/archives/或者c:\\classdir;.;c:\\archives\\ 假定虚拟机要搜寻 com.horstmann.corejava.Employee 类文件。它首先要查看存储在 jre/lib 和jre/lib/ext 目录下的归档文件中所存放的系统类文件。显然，在那里找不到相应的类文件，然后再查看类路径。然后查找以下文件： •/home/user/classdir/com/horstmann/corejava/Employee.class•com/horstmann/corejava/Employee.class 从当前目录开始•com/horstmann/corejava/Employee.class inside /home/user/archives/archive.jar 设置类路径 如果需要指定类的路径，可以借助-classpath来指定类路径。 文档注释 通过/* /开始注释。 对于代码的描述性句子，一般在/* /第一行进行编写，对于概要性的句子，可以使用HTML修饰符进行修饰(但是不要使用h1标签),如果需要键入等宽代码，使用{@code}。 方法主要使用的注释 @param 变量描述 @return 返回值描述 @throws 抛出异常描述 通用注释 @author 姓名@version 版本信息@since 从哪个版本开始加入的@deprecated 描述该类或方法或变量不再使用@see 增加一个超级链接 , 可以通过全限定类名作为超链接eg: @see com.itheima.Employee#raiseSalary(double ),链接指向Employee下的raiseSalary(double)方法 总结 所有Java对象都是在堆中构造的，构造器总是伴随着new操作符一起使用。 类设计技巧 数据私有 对数据初始化 不要再类中使用过多的基本类型 不是所有的域都需要独立的get/set方法 将职责过多的类进行分解 类名和方法名要能够体现它们的职责，也就是见名知义 优先使用不可变的类","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"},{"name":"JAVASE","slug":"JAVASE","permalink":"http://example.com/tags/JAVASE/"}],"categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://example.com/categories/JavaSE/"}]},{"title":"Anaconda配置","date":"2021-11-02T03:22:44.814Z","path":"wiki/软件配置/python/Anaconda/","text":"Anaconda下载地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 。","tags":[{"name":"配置","slug":"配置","permalink":"http://example.com/tags/%E9%85%8D%E7%BD%AE/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}],"categories":[{"name":"软件配置","slug":"软件配置","permalink":"http://example.com/categories/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/"},{"name":"python配置","slug":"软件配置/python配置","permalink":"http://example.com/categories/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/python%E9%85%8D%E7%BD%AE/"}]},{"title":"hexo使用问题","date":"2021-11-01T07:32:29.409Z","path":"wiki/软件配置/hexo/问题解决/","text":"如何解决wiki中图片显示不全只需要把图片放在images文件夹下， 然后通过/images/xxx.xx进行引用即可，这是通过绝对路径进行引用。如果要通过相对路径引用则每个md需要创建相应的文件夹存放图片，较为麻烦。 如何解决wiki中mermaid流程图无法显示的问题 修改 /theme/Wikitten下的config.yml文件 讲下列代码粘贴到文件末尾123456mermaid: ## mermaid url https://github.com/knsv/mermaidenable: true # default trueversion: &quot;7.1.2&quot;# default v7.1.2options: 下载mermaid 1npm install --save hexo-filter-mermaid-diagrams 找到Wikitten下layout下 common/footer.ejs ,替换如下代码 1234567891011121314151617181920212223242526&lt;footer id=&quot;footer&quot;&gt; &lt;% if (theme.mermaid.enable) &#123; %&gt; &lt;script src=&#x27;https://unpkg.com/mermaid@&lt;%= theme.mermaid.version %&gt;/dist/mermaid.min.js&#x27;&gt;&lt;/script&gt; &lt;script&gt; if (window.mermaid) &#123; mermaid.initialize(&#123; theme: &#x27;forest&#x27; &#125;); &#125; &lt;/script&gt; &lt;% &#125; %&gt; &lt;div class=&quot;outer&quot;&gt; &lt;div id=&quot;footer-info&quot; class=&quot;inner&quot;&gt; &lt;%= config.author || config.title %&gt; &amp;copy; &lt;%= date(new Date(), &#x27;YYYY&#x27;) %&gt; &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png&quot; /&gt;&lt;/a&gt; &lt;br&gt; Powered by &lt;a href=&quot;http://hexo.io/&quot; target=&quot;_blank&quot;&gt;Hexo&lt;/a&gt;. Theme - &lt;a href=&quot;https://github.com/zthxxx/hexo-theme-Wikitten&quot;&gt;wikitten&lt;/a&gt; &lt;% if (theme.plugins.busuanzi_count) &#123; %&gt; &lt;br&gt; &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;&lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;&lt;/span&gt; &amp;nbsp;|&amp;nbsp; &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; &lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;&lt;/span&gt; &lt;% &#125; %&gt; &lt;/div&gt; &lt;/div&gt;&lt;/footer&gt; 解决Wiki无法渲染数学公式的问题 执行下列指令 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 找到../node_modules/kramed/lib/rules/inline.js 修改如下代码：1234//escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()#$+\\-.!_&gt;])/, 第11行，将其修改为escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,//em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 第20行，将其修改为em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 对需要使用MathJax公式的文章，修改文章标签,增加mathjax: true 1234---mathjax: true---","tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"bug","slug":"bug","permalink":"http://example.com/tags/bug/"}],"categories":[{"name":"软件配置","slug":"软件配置","permalink":"http://example.com/categories/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/"},{"name":"hexo","slug":"软件配置/hexo","permalink":"http://example.com/categories/%E8%BD%AF%E4%BB%B6%E9%85%8D%E7%BD%AE/hexo/"}]},{"title":"TensorFlow2笔记-LeNet(经典卷积网络)","date":"2021-11-01T02:11:58.082Z","path":"wiki/程序技术/Python/机器学习/Tensorflow笔记/经典卷积网络/","text":"介绍这里主要介绍卷积神经网络的经典网络，然后通过tensorflow进行实现（以上章的卷积神经网络实现代码为基础，进行实现）。 统计卷积网络神经网络层数一般只统计卷积计算层和全连接计算层。 ImageNet ImageNet 是一个计算机视觉系统识别项目,是目前世界上图像识别最大的数据库。是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。ImageNet是一个非常有前景的研究项目，未来用在机器人身上，就可以直接辨认物品和人了。 经典卷积网络 graph LR A(LeNet 1998) --> B(AlexNet 2012) B --> C(VGGNet 2014) C --> D(Inception Net 2014) D --> E(ResNet 2015) LeNet由Yann LeCun于1998年提出，卷积网络开篇之作。通过共享卷积核减少了网络的参数。LeNet如下所示(C5画错了是F5) LeNet提出的时候还没提出BN和Dropout层，所以LeNet网络不具有BN和Dropout层。 根据上图实现LeNet代码如下： 12345678910111213141516171819202122232425class MyLeNet(Model): def __init__(self): super(MyLeNet, self).__init__() self.c1 = Conv2D(filters=6,kernel_size=(5,5),activation=&#x27;sigmoid&#x27;) self.p1 = MaxPool2D(pool_size=(2,2),strides=2) self.c2 = Conv2D(filters=16,kernel_size=(5,5),activation=&#x27;sigmoid&#x27;) self.p2 = MaxPool2D(pool_size=(2,2),strides=2) self.flatten = Flatten() self.f1 = Dense(120,activation=&#x27;sigmoid&#x27;) self.f2 = Dense(84,activation=&#x27;sigmoid&#x27;) self.f3 = Dense(10,activation=&#x27;softmax&#x27;) def call(self,x): x = self.c1(x) x = self.p1(x) x = self.c2(x) x = self.p2(x) # 提取的特征作为神经网络的输入特征 x = self.flatten(x) x = self.f1(x) x = self.f2(x) y = self.f3(x) return y AlexNetAlexNet网络诞生于2012年，是Hinton代表作之一。使用relu激活函数，提升训练速度，使用Dropout缓解过拟合。AlexNet实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class AlexNet(Model): def __init__(self): super(AlexNet, self).__init__() # 第一层 self.c1 = Conv2D(filters=96,kernel_size=(3,3)) self.b1 = BatchNormalization() self.a1 = Activation(&#x27;relu&#x27;) self.p1 = MaxPool2D(pool_size=(3,3),strides=2) #第二层 self.c2 = Conv2D(filters=256,kernel_size=(3,3)) self.b2 = BatchNormalization() self.a2 = Activation(&#x27;relu&#x27;) self.p2 = MaxPool2D(pool_size=(3,3),strides=2) #第三层 self.c3 = Conv2D(filters=384,kernel_size=(3,3),padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;) #第四层 self.c4 = Conv2D(filters=384,kernel_size=(3,3),padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;) #第五层 self.c5 = Conv2D(filters=256,kernel_size=(3,3),padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;) self.p3 = MaxPool2D(pool_size=(3,3),strides=2) #神经网络计算层 self.flatten = Flatten() self.f1 = Dense(2048,activation=&#x27;relu&#x27;) self.d1 = Dropout(0.5) self.f2 = Dense(84,activation=&#x27;relu&#x27;) self.d1 = Dropout(0.5) self.f3 = Dense(10,activation=&#x27;softmax&#x27;) def call(self,x): x = self.c1(x) x = self.b1(x) x = self.a1(x) x = self.p1(x) x = self.c2(x) x = self.b2(x) x = self.a2(x) x = self.p2(x) x = self.c3(x) x = self.c4(x) x = self.c5(x) x = self.p3(x) # 提取的特征作为神经网络的输入特征 x = self.flatten(x) x = self.f1(x) x = self.d1(x) x = self.f2(x) x = self.d2(x) y = self.f3(x) return y VGGNetCGGNet诞生于2014年，当年ImageNet竞赛的亚军。使用小尺寸卷积核，在减少的参数的同时，提高了识别准确率。VGGNet网络结构框图如下所示。 VGGNet的网络结构是：两次CBA，CBAPD，三次CBA , CBA,CBAPD。实现代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157class VGGNet(Model): def __init__(self): super(VGGNet, self).__init__() # 首先重复两次CBA CBAPD #1 self.c1 = Conv2D(filters=64,kernel_size=(3,3),padding=&#x27;same&#x27;) self.b1 = BatchNormalization() self.a1 = Activation(&#x27;relu&#x27;) self.c2 = Conv2D(filters=64, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b2 = BatchNormalization() self.a2 = Activation(&#x27;relu&#x27;) self.p1 = MaxPool2D(pool_size=(2,2),strides=2,padding=&#x27;same&#x27;) self.d1 = Dropout(0.2) #2 self.c3 = Conv2D(filters=128, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b3 = BatchNormalization() self.a3 = Activation(&#x27;relu&#x27;) self.c4 = Conv2D(filters=128, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b4 = BatchNormalization() self.a4 = Activation(&#x27;relu&#x27;) self.p2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=&#x27;same&#x27;) self.d2 = Dropout(0.2) # 再重复三次 CBA CBA CBAPD # 1 self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b5 = BatchNormalization() self.a5 = Activation(&#x27;relu&#x27;) self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b6 = BatchNormalization() self.a6 = Activation(&#x27;relu&#x27;) self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b7 = BatchNormalization() self.a7 = Activation(&#x27;relu&#x27;) self.p3 = MaxPool2D(pool_size=(2, 2), strides=2, padding=&#x27;same&#x27;) self.d3 = Dropout(0.2) # 2 self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b8 = BatchNormalization() self.a8 = Activation(&#x27;relu&#x27;) self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b9 = BatchNormalization() self.a9 = Activation(&#x27;relu&#x27;) self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b10 = BatchNormalization() self.a10 = Activation(&#x27;relu&#x27;) self.p4 = MaxPool2D(pool_size=(2, 2), strides=2, padding=&#x27;same&#x27;) self.d4 = Dropout(0.2) # 3 self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b11 = BatchNormalization() self.a11 = Activation(&#x27;relu&#x27;) self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b12 = BatchNormalization() self.a12 = Activation(&#x27;relu&#x27;) self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding=&#x27;same&#x27;) self.b13 = BatchNormalization() self.a13 = Activation(&#x27;relu&#x27;) self.p5 = MaxPool2D(pool_size=(2, 2), strides=2, padding=&#x27;same&#x27;) self.d5 = Dropout(0.2) # 三个全连接层 self.flatten = Flatten() self.f1 = Dense(512,activation=&#x27;relu&#x27;) self.d6 = Dropout(0.2) self.f2 = Dense(512,activation=&#x27;relu&#x27;) self.d6 = Dropout(0.2) self.f3 = Dense(10,activation=&#x27;softmax&#x27;) def call(self,x): # 两次CBA CBAPD #1 x = self.c1(x) x = self.b1(x) x = self.a1(x) x = self.c2(x) x = self.b2(x) x = self.a2(x) x = self.p1(x) x = self.d1(x) #2 x = self.c3(x) x = self.b3(x) x = self.a3(x) x = self.c4(x) x = self.b4(x) x = self.a4(x) x = self.p2(x) x = self.d2(x) #三次 CBA CBA CBAPD # 1 x = self.c5(x) x = self.b5(x) x = self.a5(x) x = self.c6(x) x = self.b6(x) x = self.a6(x) x = self.c7(x) x = self.b7(x) x = self.a7(x) x = self.p3(x) x = self.d3(x) # 2 x = self.c8(x) x = self.b8(x) x = self.a8(x) x = self.c9(x) x = self.b9(x) x = self.a9(x) x = self.c10(x) x = self.b10(x) x = self.a10(x) x = self.p4(x) x = self.d4(x) # 3 x = self.c11(x) x = self.b11(x) x = self.a11(x) x = self.c12(x) x = self.b12(x) x = self.a12(x) x = self.c13(x) x = self.b13(x) x = self.a13(x) x = self.p5(x) x = self.d5(x) # 提取的特征作为神经网络的输入特征 x = self.flatten(x) x = self.f1(x) x = self.d5(x) x = self.f2(x) x = self.d6(x) y = self.f3(x) return y Inception NetInceptionNet诞生于2014年。当年ImageNet冠军。Inception引入了Inception结构快。同一层网络使用不同尺寸的卷积核，提升了模型感知力，使用了批标准化，缓解了梯度消失。 Inception结构快如图所示 从图中可以看出，Inception包含四个卷积过程，分成四个不同的卷积核进行卷积操作。 1×1的卷积核 1×1的卷积核+3×3的卷积核 1×1的卷积核+5×5的卷积核 3×3的最大池+1×1的卷积核 最后将四个部分的输出结果，按照深度方向堆叠在一起，作为一个Inception结构快输出。 ResNetResNet（何凯明）于2015年提出，是当时的ImageNet竞赛冠军。ResNet提出了层间残差跳连，引入了前方信息，缓解梯度消失，使神经网络层数增加称为可能。 单纯堆叠神经网络层数，会使神经网络模型退化，以致于后面的特征丢失了前边特征的原本模样。 ResNet块的结构如下所示： ResNet的输出值包括两部分组成，一部分是由卷积过程提取出的特征输出F(x)，另一部分是直接由输入X得到的恒等映射X组成。将F(x)和x的对应元素相加得到输出特征H(x)。这样可以缓解神经网络堆叠导致的退化。使得神经网络层数增加称为可能。 对于X到跳过卷积层直接到输出特征有两种处理方式。 不做任何处理H(x) = F(x) + x由于不做任何处理，所以维度没有改变。 通过函数W(x)进行处理，其中W是1×1的卷积操作，用于调整X的维度。H(x) = F(x) + W(x)其中通过卷积步长可以改变输出特征图尺寸，通过卷积核的个数可以改变特征图的深度（类似Inception结构，多个卷积核，该变深度）。 ResNet网络结构如下所示： 实现代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class ResentBlock(Model): def __init__(self,filters,strides=1,residual_path=False): super(ResentBlock, self).__init__() self.filters = filters self.strides = strides self.residual_path = residual_path self.c1 = Conv2D(filters,(3,3),strides=strides,padding=&#x27;same&#x27;,use_bias=False) self.b1 = BatchNormalization() self.a1 = Activation(&#x27;relu&#x27;) self.c2 = Conv2D(filters,(3,3),strides=1,padding=&#x27;same&#x27;,use_bias=False) self.b2 = BatchNormalization() #fesiders_path 为True时候，对输入进行采样，都用1×1的卷积核做卷积操作，保证x能和F(x)维度相同，顺利相加 if residual_path: self.down_c1 = Conv2D(filters,(1,1),strides=strides,padding=&#x27;same&#x27;,use_bias=False) self.down_b1 = BatchNormalization() self.a2 = Activation(&#x27;relu&#x27;) def call(self,inputs): residual = inputs # residual等于输入本身 #将输入通过卷积层，BN层，激活层计算F(x) x = self.c1(inputs) x = self.b1(x) x = self.a1(x) x = self.c2(x) y = self.b2(x) if self.residual_path: residual = self.down_c1(inputs) residual = self.down_b1(residual) # 最后输出是两部分的和，即F(x)+x或F(x)+W(x),然后再过激活函数。 out = self.a2(y + residual) return out# 由一层卷积网络+八个ResNet块组成# 神经网络由一个全连接层构成class ResNet(Model): # block_list表示每个block有几个卷积层 def __init__(self,block_list,initial_filters=64): super(ResNet, self).__init__() self.num_blocks = len(block_list) self.block_list = block_list self.out_filters = initial_filters # 对应图中第一个卷几层 self.c1 = Conv2D(self.out_filters,(3,3),strides=1,padding=&#x27;same&#x27;,use_bias=False,kernel_initializer=&#x27;he_normal&#x27;) self.b1 = BatchNormalization() self.a1 = Activation(&#x27;relu&#x27;) self.blocks = tf.keras.models.Sequential() # 对应图中的八个ResNet块 #构建ResNet网络结构 4*2 = 8 for block_id in range(len(block_list)):#第几个resnet block for layer_id in range(block_list[block_id]):# 第几个卷层 if block_id != 0 and layer_id == 0 : #对除第一个block以外的每个Block的输入进行采样 block = ResentBlock(self.out_filters,strides=2,residual_path=True) else: block = ResentBlock(self.out_filters,residual_path=False) self.blocks.add(block) # 将构建好的blcok加入到renset self.out_filters *=2 #下一个block卷积核数是上一个block的两倍 # 平均池 self.p1 = tf.keras.layers.GlobalAveragePooling2D() # 全连接层 self.f1 = tf.keras.layers.Dense(10) def call(self,inputs): x = self.c1(inputs) x = self.b1(x) x = self.a1(x) x = self.blocks(x) x = self.p1(x) y = self.f1(x) return ymodel = ResNet([2,2,2,2]) 总结 LeNet通过共享卷积核，减少网络参数 AlexNet 通过使用relu激活函数，提升训练速度。 使用Dropout缓解过拟合。 VGGNet 小尺寸卷积核减少参数，网络结构规整，适合并行加速。 InceptionNet 一层内使用不同尺寸卷积核，提升感知力。使用批标准化，缓解梯度消失。 ResNet 层间残差跳连，引入前方信息，缓解模型退化，使神经网络层数加深成为可能。 训练优化 &nbsp;&nbsp;&nbsp;&nbsp;一些训练方法和超参数的设定对模型训练结果的影响是相当显著的，如数据增强（对训练集图像进行旋转、偏移、翻转等多种操作，目的是增强训练集的随机性）、学习率策略（一般的策略是在训练过程中逐步减小学习率）、Batch size 的大小设置（每个 batch 包含训练集图片的数量）、模型参数初始化的方式等等。。所以，在神经网络的训练中，除了选择合适的模型以外，如何更好地训练一个模型也是一个非常值得探究的问题。","tags":[{"name":"TensorFlow2","slug":"TensorFlow2","permalink":"http://example.com/tags/TensorFlow2/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow2","slug":"深度学习/TensorFlow2","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow2/"}]},{"title":"TensorFlow2笔记-NN(全连接)","date":"2021-10-29T06:05:38.694Z","path":"wiki/程序技术/Python/机器学习/Tensorflow笔记/NN/","text":"介绍全连接NN：每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果。（可以类比于满射，前后层神经元之间都有联系） 参数个数 参数个数=$\\displaystyle \\sum^{}_{各层}{前层*后层(w)+后层(b)}$","tags":[{"name":"TensorFlow2","slug":"TensorFlow2","permalink":"http://example.com/tags/TensorFlow2/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow2","slug":"深度学习/TensorFlow2","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow2/"}]},{"title":"TensorFlow2笔记-第四讲(网络八股扩展)","date":"2021-10-29T06:00:43.670Z","path":"wiki/程序技术/Python/机器学习/Tensorflow笔记/第四讲-网络八股扩展/","text":"","tags":[{"name":"TensorFlow2","slug":"TensorFlow2","permalink":"http://example.com/tags/TensorFlow2/"}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow2","slug":"深度学习/TensorFlow2","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow2/"}]},{"title":"TensorFlow2笔记-CNN(卷积神经网络)","date":"2021-10-29T04:44:21.509Z","path":"wiki/程序技术/Python/机器学习/Tensorflow笔记/CNN/","text":"问题介绍如果仅仅依靠全连接神经网络来训练模型，则在实际应用中，输入特征会非常多，参数十分复杂，让训练变得非常庞大。所以在实际应用时，会对原始图像进行特征提取再把提取到的特征送给全连接网络。流程如下所示： graph LR 原始图片 --> 若干层特征提取 若干层特征提取 --> 全连接网络 卷积计算是一种提取图片特征的有效方法。 卷积计算过程 单通道卷积计算 从上图可以看出,卷积神经网络计算的过程实际上就是将大的数据，根据区域提取其相关特征，减少了特征数目。从 5x5x1 的参数，经过 3x3x1的卷积核卷积计算后，转化为3x3x1的参数网络。&gt; 多通道卷积计算从图中可以看出，对于输入特征是多通道的，每个通道都通过卷积层计算相应的调整值。从图片看从5×5×3经过3×3×3的卷积核计算，对每个通道的重合区域，经过对应通道 的卷积核计算的结果作为输出调整图中的一个像素点。 总结对于卷积神经网络的计算过程，就是从左到右，从上到下，根据卷积核重叠的区域，依次计算结果，作为输出特征图的一个像素点。 用CNN实现离散数据的分类(以图像分类为例)1. 感受野(Receptive Field)感受野：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。 这里可以参考一下卷积神经网络的计算过程中，5×5×1 经过 3×3×1 卷积后得到一张3×3的特征图，那么最终3×3特征图中的一个像素点所对应在5×5×1输入特征图的区域大小，（大小只取正方形区域边的大小），称之为感受野，从图上可以看出对应的感受野为3。 如果输出特征图的感受野都是一样的,但是选取的卷积核不一样（如刚刚的5×5×1的输入特征可以经过两层3×3×1的卷积核得到1的输出特征图，同样可以经过一层5×5×1的卷积核得到1的输出特征图，二者的感受野都是5）。 这里就需要考虑选取不同的卷积核所带来的计算代价，计算越少越好。 对计算量的计算步骤，以两层3×3×1为例，对于输入特征图经过第一层卷积核的计算量-&gt;首先每次计算共有9次乘法运算，卷积核总共扫描的区域数很容易得出为(x-3+1)(x-3+1),总的计算数为9×(x-2)×(x-2)。同理经过第一层卷积核的输出特征图经过第二个卷积核的计算量为9×(x-2-3+1)×(x-2-3+1)。将两个卷积核计算量相加得到总的计算量：18$x^2$-108x+180 2. 全零填充(Padding)为了保证输入特征图的尺寸不变，通过0进行填充，在输入特征图周围填充0，如原来的5×5×1经过3×3×1后仍然还是5×5×1。 填充公式(卷积输出特征图维度计算公式) padding=\\left\\{ \\begin{aligned}SAME(全0填充)\\frac{入长}{步长} & ,(向上取整) \\\\ VALID(不全0填充) \\frac{入长-核长+1}{步长}&,(向上取整) \\end{aligned} \\right.TF描述全0填充，用参数padding=’SAME’或padding=’VALID’表示。 3. Tensorflow2描述卷积层12345678910111213141516171819### TF描述卷积层的代码# tf.keras.layers.Conv2D(# filters=卷积核个数,# kernel_size=卷积核尺寸,#正方形写核长整数,或（核高h,核宽w）# strides = 滑动步长,#横纵向相同写步长整数，或（纵向步长h，横向步长w）,默认1# padding = &quot;same&quot; or &quot;valid&quot;,#使用全0填充是same，不使用是valid(默认)# activation=&quot;relu&quot;or&quot;sigmoid&quot;or&quot;tanh&quot;or&quot;softmax&quot;等,#如有BN此处不写# input_shape=(高，宽，通道数) #输入特征图维度，可省略# )model = tf.keras.models.Sequential([ Conv2D(6,5,padding=&#x27;valid&#x27;,activation=&#x27;sigmoid&#x27;), MaxPool2D(2,2), Conv2D(6,(5,5),padding=&#x27;valid&#x27;,activation=&#x27;sigmoid&#x27;), MaxPool2D(2,(2,2)), Conv2D(filters=6,kernel_size=(5,5),padding=&#x27;valid&#x27;,activation=&#x27;sigmoid&#x27;), MaxPool2D(pool_size=(2,2),strides=2), Flatten(), Dense(10,activation=&#x27;softmax&#x27;)]) 4. 批标准化(Batch Normalization , BN)神经网络对0附件的数据更敏感。 标准化：使数据符合0均值，1为标准差的分布批标准化：对一小批数据（batch），做标准化处理。批标准化，第K个卷积核的输出特征图(feature map)中第i个像素点。 $H_{i}^{‘k}$ = $\\frac{H_{i}^{k}-u_{batch}^{k}} {\\sigma_{batch}^{k}}$$H_{i}^{k}$：批标准化前，第K个卷积核，输出特征图中第i个像素点。$u_{batch}^{k}$：批量化前，第k个卷积核，batch张输出特征图中所有像素点平均值。 $u_{batch}^{k}$ = $\\frac{1} {m} \\displaystyle \\sum^{m}_{i=1} H_{i}^{k}$ $\\sigma_{batch}^{k}$：批标准化前，第k个卷积核，batch张输出特征图中所有像素点标准差。 $\\sigma_{batch}^{k}$ = $\\sqrt{\\delta+\\frac{1}{m}\\displaystyle \\sum^m_{i = 1}(H_i^k-u_{batch}^k)^2}$ 通过BN操作，将数据标准化到0均值。如下图所示： 通过标准化使得输入特征的微小变化也能使得激活函数有明显的变化，提升激活函数对输入数据的区分力。 从图中可以看出，经过标准化的Sigmoid函数，特征数据$H_i^{‘k}$集中在0附近，但是从图中可以看出，Sigmoid函数在0区域附近的曲线接近线性函数，所以这样会导致激活函数的非线性特性丧失。为了解决这个问题，需要为每个卷积核引入可训练参数$\\gamma$（缩放因子）和$\\beta$（偏移因子）,用于调整归一化的力度,优化特征数据分布的宽窄和偏移量，保证了网络的非线性表达力。调整后的数据$x_i^k$ = $\\gamma_{k}H_{i}^{‘k}+\\beta_k$。 从上面可以看出，BN层用于对数据的标准化处理，可以缓解梯度消失，所以BN层位于卷积层之后，激活层之前。 TensorFlow描述批标准化的代码如下 12345678910&quot;&quot;&quot; TF通过tf.keras.layers.BatchhNormalization()描述BN层&quot;&quot;&quot;model = tf.keras.models.Sequential([ Conv2D(padding=&#x27;same&#x27;,kernel_size=(5,5),filters=6), BatchNormalization(),#BN层 Activation(&#x27;relu&#x27;),#激活层 MaxPool2D(pool_size=(2,2),strides=2,padding=&#x27;same&#x27;), Dropout(0.2),#droupt层]) 5. 池化(Pooling)池化用于减少特征数据量。池化包含最大值池化和均值池化。最大值池化可以提取图片纹理，均值池化可以保留背景特征。 两种池化的计算如下图所示。用池大小为2×2，步长为2的池进行处理。从图中可以看出二者的计算过程。 最大值池化 选择被池包含区域内最大的特征值作为输出结果。 均值池化 选择被池包含区域内特征值的平均值作为输出结果。 TensorFlow描述池化的代码如下 12345678910111213141516171819 &quot;&quot;&quot; tf.keras.MaxPool2D( pool_size=池化核尺寸,#正方形写核长整数，或用元组给出核的高和宽(核高h,核宽w) strides=池化步长,#步长整数，或(纵向步长h，横向步长w),默认为pool_size padding=&#x27;valid&#x27;or&#x27;same&#x27;#q全0填充是same,否则是&#x27;valid ) tf.keras.AveragePooling2D( pool_size=池化核尺寸,#正方形写核长整数，或用元组给出核的高和宽(核高h,核宽w) strides=池化步长,#步长整数，或(纵向步长h，横向步长w),默认为pool_size padding=&#x27;valid&#x27;or&#x27;same&#x27;#q全0填充是same,否则是&#x27;valid ) &quot;&quot;&quot; model = tf.keras.models.Sequential([ Conv2D(padding=&#x27;same&#x27;,kernel_size=(5,5),filters=6),#卷积层 BatchNormalization(),#BN层 Activation(&#x27;relu&#x27;),#激活层 MaxPool2D(pool_size=(2,2),strides=2,padding=&#x27;same&#x27;),#池化层 Dropout(0.2),#droupt层]) 6. 舍弃（Dropout）舍弃是为了缓解神经网络过拟合。舍弃也就是在神经网络训练的过程中，将一部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复链接。（也就是说在神经网络训练的过程中，一些神经元会被暂时踢出训练，等最后在加入到神经网络中。）。舍弃过程如图所示： 在训练的过程中，将一部分神经元暂时舍弃(类似于神经元死亡，也就是神经元的参数不再更新)。 TensorFlow描述舍弃（Dropout） 12345678910&quot;&quot;&quot; tf描述舍弃：tf.keras.layers.Dropout(舍弃的概率)&quot;&quot;&quot; model = tf.keras.models.Sequential([ Conv2D(padding=&#x27;same&#x27;,kernel_size=(5,5),filters=6),#卷积层 BatchNormalization(),#BN层 Activation(&#x27;relu&#x27;),#激活层 MaxPool2D(pool_size=(2,2),strides=2,padding=&#x27;same&#x27;),#池化层 Dropout(0.2),#droupt层，随机舍弃掉20%的神经元 ]) 7. 总结从上面介绍可以看出，卷积的过程就是对特征值的特征进行提取，来减少特征值的数量（卷积，池化）。通过用一个输出像素点来反映一块区域内像素点的特征。通过这样的方式有效的减少了特征值的数量。 卷积神经网络的主要组成模块,卷积层，BN层，激活层，池化层,舍弃层(dropout层)，全连接层（Fully Connected）。卷积层，BN层，激活层，池化层这四层用于对输入特征进行特征提取。 graph LR A(\"卷积&#40;Convolutional&#41;\") --> B(\"批标准化&#40;BN&#41;\") B --> C(\"激活&#40;Activation&#41;\") C --> D(\"池化&#40;Pooling&#41;\") D --> F(\"舍弃层&#40;Dropout&#41;\") F --> E(\"全连接&#40;FC&#41;\") 卷积是什么？ 卷积就是特征提取器,就是CBAPD(D表示的是舍弃，Dropout)","tags":[{"name":"TensorFlow2","slug":"TensorFlow2","permalink":"http://example.com/tags/TensorFlow2/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"TensorFlow2","slug":"深度学习/TensorFlow2","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow2/"}]},{"title":"KMP算法","date":"2021-10-28T09:15:08.580Z","path":"wiki/算法/Leetcode/字符串/KMP/","text":"1. 问题描述 2. 解题思路 1. 问题描述今天的问题很简单，就是给定两个字符串text和pattern，让你找出在text中是否存在一个序列等于pattern,如果存在就返回true,如果不存在就返回false 。 2. 解题思路从题目描述上来看，很容易看出这题目是关于字符串匹配的问题，也就是从一段文本中，找出某个模式的全部出现位置的问题。 朴素的字符串匹配算法 用一个循环找出所有有效位移，对text每个字符进行检测，然后匹配pattern，如果匹配成功就返回下标。如果失败，就移动text下一个位置，重复操作。123456789101112131415161718192021public int strStr(String haystack, String needle) &#123; int len_h , len_s , i , j = 0 ; len_s = needle.length() ; if(len_s == 0 ) return 0 ; len_h = haystack.length() ; for(i = 0 ; i &lt;= len_h - len_s ; i++)&#123; if(haystack.charAt(i)==needle.charAt(j))&#123; for(j=1;j&lt;len_s;j++)&#123; if(haystack.charAt(i+j)!=needle.charAt(j)) break ; &#125; if(j&gt;=len_s) return i ; else j = 0 ; &#125; &#125; return -1 ; &#125; KMP算法 KMP算法的巧妙之处就是利用已经匹配字符串来确定下一次匹配开始的位置。这就是和朴素字符串匹配算法的区别，朴素字符串匹配算法每次匹配完毕之后，只是从下一个字符开始进行匹配，那么就浪费了已经匹配过字符串的有效信息，所以造成时间上的浪费。如何确定下一字符开始匹配的位置呢？假设已经匹配模式串pattern的位置为j,也就是从text的位置i开始 text[i…..,i+j] = pattern[0……,0+j] KMP算法的前缀函数(next) 前缀函数 $\\pi$(i)(0 $\\leq$ i &lt; m)表示s 的子串 s[0:i] 的最长的相等的真前缀与真后缀的长度。特别地，如果不存在符合条件的前后缀，那么 $\\pi$(i) = 0。其中真前缀与真后缀的定义为不等于自身的的前缀与后缀。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 求解字符前缀函数 * / public int[] getNext(String s)&#123; int len , k ,q ; len = s.length() ; int[] next = new int[len]; next[0] = 0 ; for(q = 1,k=0 ; q&lt; len ; q++)&#123; //长字符串的前缀建立在短字符前缀的匹配上。 while (k&gt;0&amp;&amp;s.charAt(k)!=s.charAt(q))&#123; k = next[k-1] ; &#125; if(s.charAt(q) == s.charAt(k))&#123; k = k+1 ; &#125; next[q] = k ; &#125; return next ; &#125;/** * 通过前缀函数进行字符串的匹配 * / public int Matcher(String t , String p )&#123; int q , i; int n = t.length(), m = p.length(); if (m == 0) &#123; return 0; &#125; int[] next = getNext(p) ; q = 0 ; for(i = 0 ,q=0; i &lt; n ;i++)&#123; while(q&gt;0&amp;&amp;p.charAt(q)!=t.charAt(i))&#123; //如果不匹配则移动到下一个位置 q = next[q-1] ; &#125; if(p.charAt(q) == t.charAt(i))&#123; q = q+1 ; &#125; if(q == m)&#123; return i-m+1 ; &#125; &#125; return -1 ; &#125; 个人总结 KMP算法主要在于计算前缀函数next，前缀函数的计算依据最长相同前后缀长度为依据来计算。通过next来获取到移动的位置。 对于next前缀函数的计算过程，patten(q) 和 patten(k) 不等说明了，在前一个（q-1）的最长前后缀匹配的位置，增加一个text(q)后，patten[0-&gt;next(q-1),next(q-1)+1] 和patten[(q-1)-next(q-1)-&gt;q]并不相等(因为新增的最后一个字符和前一个patten(q-1)字符串的最长前后缀并不相等)，所以需要进一步比较next[next(q-1)] 是否满足，patten[0-&gt;k+1] == patten[k-q,q] , 如果相等，则另k+1 , next[q] = k 。","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"字符串","slug":"字符串","permalink":"http://example.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}],"categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"经典算法","slug":"算法/经典算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/"}]},{"title":"48.旋转数组","date":"2021-10-28T08:36:32.058Z","path":"wiki/算法/Leetcode/链表/19.删除链表的倒数第N个结点/","text":"题目介绍给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。(进阶：你能尝试使用一趟扫描实现吗？) 示例 1： 输入：head = [1,2,3,4,5], n = 2 输出：[1,2,3,5] 示例 2： 输入：head = [1], n = 1 输出：[] 示例 3： 输入：head = [1,2], n = 1 输出：[1] 解法思路从题目分析，我们需要删除链表的倒数第N个结点，那么我们就必须知道哪个结点是倒数第N个结点。确定一个结点在列表中的位置大概有如下几种方法。 方法一 先确定整个列表的长度，然后就能够确定结点所在的位置(可以通过堆栈或者数组存放数据元素，通过数组有利于操作)。示例代码如下： 1234567891011121314151617181920 class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ArrayList&lt;ListNode&gt; listNodes = new ArrayList&lt;&gt;() ; int len = 0 ; if(head.next==null) return null ; while(head!=null)&#123; len++ ; listNodes.add(head) ; head = head.next ; &#125; listNodes.add(head) ; if(len-n-1&lt;0 )&#123; return listNodes.get(1) ; &#125;else &#123; listNodes.get(len-n-1).next = listNodes.get(len-n).next ; return listNodes.get(0) ; &#125; &#125;&#125; 方法二 通过快慢指针法，让一个指针永远比当前指针快n个元素，那么当快指针到达末尾的时候，就能够确定倒数第N个元素。示例代码如下所示： 12345678910111213141516171819202122class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode p , q , pre ; int i = 0; p = head ; // 快指针 q = head ;// 慢指针 pre = q ; //用于记录q的前一个元素，有利于元素的删除 for(i = 0; i&lt; n ; i++) //让p比q快n个元素 p = p.next ; while(p!=null)&#123; p = p.next ; pre = q ; q = q.next ; &#125; if(pre == q)&#123; // 判断删除元素是否为头 return q.next; &#125;else&#123; pre.next = q.next ; return head ; &#125; &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"链表","slug":"链表","permalink":"http://example.com/tags/%E9%93%BE%E8%A1%A8/"}],"categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Leetcode","slug":"算法/Leetcode","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/"},{"name":"链表","slug":"算法/Leetcode/链表","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/%E9%93%BE%E8%A1%A8/"}]},{"title":"3. Java控制流程","date":"2021-10-27T03:07:05.097Z","path":"wiki/程序技术/Java/JavaSE/第三章-Java基本程序结构/java controll followe/","text":"程序的控制流程一般包含，选择结构，循环结构，顺序结构。 顺序结构 顺序结构很简单，程序都具有顺序结构，依次按顺序执行。Java的语法，不允许在嵌套的两个块(块一般是指，通过{}包含的代码区域，称为块)中，声明同名变量。示例代码如下：12345678public static void main(String[] args)&#123; int n; &#123; int k; int n; // Error can&#x27;t redefine n in inner block &#125;&#125;选择结构选择结构也就是 if-else结构，通过条件语句来选择需要执行的语句。else与最近的if构成匹配。示例代码如下：12345678910public static void ifElseTemplemate()&#123; int a = 1 ,b = 2 ; if(a &gt; b) System.out.println(&quot;a大于b&quot;); else if(a == b) System.out.println(&quot;a 等于 b&quot;); else System.out.println(&quot;a 小于 b&quot;);&#125;如果有多个选择情况，可以通过Switch-case语句进行实现。代码如下:12345678910111213141516171819public static void ifElseTemplemate()&#123; int a = 1 ,b = 2 ; if(a &gt; b) System.out.println(&quot;a大于b&quot;); else if(a == b) System.out.println(&quot;a 等于 b&quot;); else System.out.println(&quot;a 小于 b&quot;); switch (a)&#123; case 1: System.out.println(a);break ; case 2: System.out.println(a); break ; default: System.out.println(a);break ; &#125; &#125;switch语句将从与选项值相匹配的case标签处开始执行直到遇到 break 语句，或者执行到switch i吾句的结束处为止。如果没有相匹配的case标签,而有default子句,就执行这个子句。case 标签可以是： •类型为 char、byte、 short 或 int 的常量表达式。 •枚举常量。 •从 Java SE 7开始， case 标签还可以是字符串字面量。 循环结构循环执行某个代码块。一般包含for循环和while循环，do-while循环。示例代码如下：12345678910111213141516171819202122public static void forWhileStatement()&#123; // 计算1-100的和 int sum = 0 , i = 0 ; for(i = 0 ; i &lt;= 100 ; i++)&#123; sum +=i; &#125; System.out.println(sum); i = 0 ; sum = 0 ; while(i&lt;=100)&#123; sum+=i; i++; &#125; System.out.println(sum); i = 0 ; sum = 0 ; do&#123; sum+=i; i++; &#125;while(i&lt;=100) ; System.out.println(sum); &#125;还有一种循环方式为for-each循环，参考数组的循环。 如果想跳出循环可以使用break;，如果想要跳出某个执行过程可以用continue。Java也支持类似于goto的语法，通过break 标签来结束循环。示例代码如下：12345678910public static void forWhileStatement()&#123; int i = 0, j ; lables: for(i = 0 ; i &lt; 10 ; i++)&#123; for(j=0;j&lt;10;j++)&#123; System.out.println(&quot;in&quot;); break lables;//跳出指定的循环 &#125; &#125; &#125;","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"},{"name":"JAVASE","slug":"JAVASE","permalink":"http://example.com/tags/JAVASE/"}],"categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://example.com/categories/JavaSE/"}]},{"title":"2. Java基本语法","date":"2021-10-26T09:21:11.459Z","path":"wiki/程序技术/Java/JavaSE/第三章-Java基本程序结构/java basic statment/","text":"1. Java注释2. Java数据类型3. Java变量4. Java运算符5. Java字符串6. Java输入流7. Java数组8. 大数值一般我们在学习一门语言的时候，总是从hello world入门，所以我们也是从hello world开始入门。1234567public class FirstSample&#123; public static void main(String[] args) &#123; System.out.println(&quot;Hello World!&quot;); &#125;&#125;如果你想运行一个java程序，那么需要在类里面加上main方法，作为程序的入口让程序运行起来。格式如下。1234567public class ClassName&#123; public static void main(String口 args) &#123; program statements &#125;&#125; 1. 注释注释一般用于书写一些关于程序或者变量，算法的说明。用于辅助他人阅读你的程序代码，增加代码的可读性。 Java的注释可以分成三种注释。 以双反斜杠标记的注释 // &nbsp;&nbsp;&nbsp;&nbsp;如下代码所示，通过//注释对变量进行解释说明。1234567public class ClassName&#123; public static void main(String口 args) &#123; int i , j ; // i和j为临时变量 &#125;&#125; 2, 以/ TO-DO /,将一段内容进行注释。 &nbsp;&nbsp;&nbsp;&nbsp;这个注释和//注释的区别在于。//是单行注释，而/…./可以表示多行注释。示例如下。 1234567891011public class ClassName&#123; public static void main(String口 args) &#123; /* 这是多行注释。 可以注释多行。 */ int i , j ; &#125;&#125; 第三种是/* /注释，称为文档注释，用于生成程序说明文档。一般在类上或者方法上进行注释。 &nbsp;&nbsp;&nbsp;&nbsp;示例如下：通过文档注释，标明了类创建的时间。关于文档注释的其它声明，可以查阅相关资料。1234567891011/** * @date: 2021-10-26 19:44 **/public class Java基本数据类型 &#123; public static void main(String args[]) &#123; System.out.println(&quot;hello world&quot;); // 如果想要程序在执行中间退出，则通过System.exit()退出程序 System.exit(1); System.out.println(&quot;程序中止返回&quot;); &#125;&#125; 2. 数据类型Java是一种强类型语言，Java的变量在使用前，都需要进行变量的声明。Java有8种基本数据类型。分别为4种整型，2种浮点型，1种字符型，1种布尔型(boolean). 1, 整型 整型也就是常说的整数。用于表示数据为整数。 类型 字长 取值 byte 1 -128-127 short 2 -2^15-(2^15-1) int 4 -2^31-(2^31-1) long 8 -2^63-(2^63-1) 示例代码如下：1234567891011public static void basicType()&#123; byte a ; short b ; int c; long d ; d = 10000000L ; //通过L表示为长整型数据 c = 0x123 ; //通过0x表示为十六进制 c = 0b1001 ; //通过0b表示二进制数据 c = 010 ; //通过0开头，表示为八进制数据。 &#125; 浮点类型 浮点类型的数据也就是我们常说的小数。 类型 字长 取值 float 4 大约 ± 3.402 823 47E+38F (有效位数为 6 ~ 7 位） double 8 大约 ± 1.797 693 134 862 315 70E+308 (有效位数为 15 位) double 表示这种类型的数值精度是 float 类型的两倍（有人称之为双精度数值)。绝大部分应用程序都采用 double 类型。float 类型的数值有一个后缀 F 或 f (例如，3.14F。) 没有后缀 F 的浮点数值（如 3.14 ) 默认为double 类型。当然，也可以在浮点数值后面添加后缀 D 或 d (例如，3.14D)。 字符类型字符类型char采用unicode编码格式。char 类型的字面量值要用单引号括起来。, char 类型的值可以表示为十六进制值，其范围从 \\u0000 到 \\Uffff。例如：W2122 表示注册符号 ( ), \\u03C0 表示希腊字母 it。在Java中，char类型占用两个字节（采用16位的Unicode字符集）。示例代码如下，用于输出显示两个特殊的字符。1234567public static void basicType()&#123; char g = &#x27;\\u2122&#x27;; //注册符号 Tm System.out.println(g); g = &#x27;\\u03C0&#x27; ; System.out.println(g); &#125; Boolean类型boolean(布尔)类型一般有两个值:true和false,用来判定逻辑条件。在Java中整型不能替代布尔类型(这在C/c++却是可以的。) 对于数据类型，我们可以在不同的数据类型之间进行转化(强制类型转换)。 3. 变量变量的声明在前面数据类型也可以看到。一般格式为：数据类型 变量名; 变量名的命名格式 变量名必须是一个以字母开头并由字母或数字构成的序列。需要注意，与大多数程序设计语言相比，Java 中“ 字母” 和“ 数字” 的范围更大。字母包括“A’~Z”、“a~z”、或在某种语言中表示字母的任何 Unicode 字符。 变量的初始化和赋值操作 初始化一般是在变量的声明时候，给变量一个初始值(这是一个好的编程习惯)。变量的赋值一般通过=对变量进行赋值操作。示例代码如下所示。1234public static void bytes()&#123; int a = 30 , b ; //对变量进行初始化 b = 20 ; //对变量进行赋值 &#125; 常量 常量也就是一直不变的量，只能被赋值一次。一般我们通过final来指示常量(一般用纯大写作为变量名，下划线进行分割)。示例代码如下:12345public static void bytes()&#123; int a = 30 , b ; //对变量进行初始化 b = 20 ; //对变量进行赋值 final double PI = 3.1415962 ; &#125;对于类常量，一般是作为类的成员变量，并且访问修饰符为static。可以理解为，加了sttic外部就能直接引用该变量。123public class test&#123; public static final double PI = 3.1415926 ; &#125; 4. 运算符运算符一般就是加减乘除这些运算符，以及一些特殊操作。+,-,*./,%(取余)。 对于取余有一个技巧，保证不取负数。通过（(position +adjustment) % 12 + 12) % 12。公式进行取余操作，就保证了余数在0-11之间，不出现负数的情况。 还有一些运算符属于双目运算符，如 +=,-=,*=…。这写运算符和前面的运算符类似只不过，多了赋值操作。a += b; 等价于 a = a+b 。可以编程进行验证。 自增运算符：++和—, x++也就等价于 x += 1; 关系运算符：主要是用于比较数值的大小，如&gt;=,&lt;=,&gt;,&lt;，!= 等关系运算。简单的例子就是：a&gt;b ,比较a和b的大小。 逻辑运算符：也就是与，或，非三个运算。&amp;&amp; 表示与， || 表示或，！表示非。一般用于条件表达式表示多条件的情况。如表示 a&gt; b 且 a&gt; c的情况,可以写成 a&gt;b &amp;&amp; a&gt;c 三元操作符?: 用法 a&gt;b?a:b 。如果a&gt;b则执行a,反之执行b 位运算符：&amp;,|,^（异或）,~(非)。一般用于对整数各个位(二进制)进行位运算。&gt;&gt;和&lt;&lt;运算符，用于将数据左移和右移。需要建立位模式来完成位掩码时， 这两个运算符会很方便。&gt;&gt;&gt;会用0填充高位，而&gt;&gt;则是用符号位填充高位。移位运算符的右操作数要完成模 32 的运算（除非左操作数是 long 类型， 在这种情况下需要对右操作數模 64 )。例如， 1 «35 的值等同于 1 «3 或 8。也就是最多不会超过int类型，如果超过那么就等同于模32后再右移动。 运算符优先级运算符优先级也就是运算符的优先执行顺序。如果不确定优先级，我们可以用括号包含起来。运算符优先级 单目运算(!,~,++,new,强制类型转换) &gt; (*，/,%) &gt; (+,-) &gt; (&lt;&lt;, &gt;&gt; , &gt;&gt;&gt;) &gt; (&lt; , &gt; , &gt;= , instanceof) &gt; (==,!=) &gt; (&amp;) &gt; (^) &gt;(|) &gt; (&amp;&amp;) &gt; (||) &gt; (?) &gt; (=,+=,-=……)。 枚举类型通过enum 进行枚举类型的声明，包括有限个命名的值。示例代码如下:1234567891011121314151617181920public enum 枚举类型 &#123; SMALL(1,&quot;小号&quot;), MEDIUM(2,&quot;中号&quot;),; private int code ; private String message ; 枚举类型(int code , String message)&#123; this.code = code ; this.message = message ; &#125; public static int getCode(String define)&#123; // 返回枚举类型的code,根据Message return 枚举类型.valueOf(define).code ; &#125; public static String getMessage(String define)&#123; return 枚举类型.valueOf(define).message ; &#125;&#125; 5. 字符串字符串也就是一串字符，通常用””包含。一般我们使用String表示字符类型。示例: String str = “hello world” ; , 字串则是字符串的一串连续的字符，如”worl”就是str的一个字串。如果需要拼接字符，可以通过+号进行拼接。如果需要拼接多个，可以通过定界符(，)分割，然后用静态join方法拼接。如:String str = String.join(“h”,”ello”,”w”,”rold”) ; Java如果要修改某个字符，一般需要创建新的字符常量对象。Java的String类似于指针，指向字串常量在存储池（堆）的相应位置。如果指向相同的字符常量， 则这两个变量是相同的。可以运行如下代码:123456789public static void byteString()&#123; String a = &quot;123&quot;; String b = &quot;123&quot; ; String c = new String(&quot;123&quot;) ; System.out.println(a == b); System.out.println(a.equals(b)); System.out.println(a == c); System.out.println(a.equals(c));&#125;上面的代码中两个变量 a == b 比较的是两个变量是否相同(类比于比较地址是否相同)，a.equeals(b)检测的是两个字符串是否相等。检查一个字符串既不是 null 也不为空串，这种情况下就需要使用以下条件：if (str != null &amp;&amp; str.length()!= 0),先检测变量是否为空，在判断长度是否为空。 StringBuffer和StringBuilder的区别 StringBuilder支持单线程，StringBuffer支持多线程。就效率而言,StringBuilder快于StringBuffer，所以在单线程程序中采用StringBuilder较好，多线程程序采用StringBuffer。 6. 输入输出流java输入输出流，通过Scanner对象和标准输入流（System.in）相关联进行获取。1234567891011 public static void ioStream()&#123; // Java构建输入流，通过Scanner对象和标准输入流Sytem.in对象关联。 Scanner in = new Scanner(System.in) ; System.out.print(&quot;输入字符：&quot;); String text = in.nextLine() ; System.out.println(text);&#125;因为输入是可见的， 所以 Scanner 类不适用于从控制台读取密码。Java SE 6 特别引入了 Console 类实现这个目的。要想读取一个密码， 可以采用下列代码： Console cons = System.console(); String username = cons.readLine(&quot;User name: &quot;)； char [] passwd = cons.readPassword(&quot;Password:&quot;); 为了安全起见， 返回的密码存放在一维字符数组中， 而不是字符串中。在对密码进行处理之后，应该马上用一个填充值覆盖数组元素（数组处理将在 3.10 节介绍）。采用 Console 对象处理输入不如采用 Scanner 方便。每次只能读取一行输入， 而没有能够读取一个单词或一个数值的方法。 格式化输出最简单的方法是采用C语言的方法进行输出System.printf就可以通过C语言的printf输出方式进行输出。 文件输入与输出通过File对象构建Scanner对象进行文件的读取，通过PrintWriter进行文件的输出。示例代码如下所：12345678910111213141516171819public static void fileIO() throws IOException &#123; //文件输入流 , get路径为文件路径 Scanner in = new Scanner(Paths.get(&quot;javase/src/main/java/Java核心技术/第三章Java的基本程序设计/myfile.txt&quot;),&quot;UTF-8&quot;) ; while(in.hasNext())&#123; System.out.println(in.next()); &#125; in.close(); System.out.println(); in = new Scanner(new File(&quot;javase/src/main/java/Java核心技术/第三章Java的基本程序设计/myfile.txt&quot;)) ; while(in.hasNextLine())&#123; System.out.println(in.nextLine()); &#125; in.close(); //文件输出流，路径填写文件路径 PrintWriter out = new PrintWriter(new File(&quot;javase/src/main/java/Java核心技术/第三章Java的基本程序设计/myfile.txt&quot;) ) ; out.write(&quot;this is a test two&quot;); out.close(); &#125; 7.数组数组存储了一串连续地址的元素，通过元素下标可以访问到数组元素。数组有如下几种声明方式。 int[] a ; int[] a = new int[100] ; int a[] ; int[] a = &#123;1,2,3,4,5&#125; ; new int[] &#123;17,19,23,29&#125; ; //匿名数组 数组的迭代方法 for，while循环通过遍历数组下标来遍历数组的元素 for-each循环通过for-each遍历数组的每一个元素。for-each的一般形式为：for (variable : collection) statement。如果要遍历集合，那么对应的集合必须是数组或者实现了Iterable接口的类对象。 123456789101112public static void array()&#123; int i ; int[] a = &#123;1,2,3,4,5&#125; ; //for循环遍历 for(i = 0 ; i &lt; a.length ; i++)&#123; System.out.println(a[i]); &#125; //for-each循环 for(int temp : a)&#123; System.out.println(temp); &#125;&#125; 打印数组通过Arrays类的toString方法，可以打印一个数组。1234567891011121314public static void array()&#123; int i ; int[] a = &#123;1,2,3,4,5&#125; ; //for循环遍历 for(i = 0 ; i &lt; a.length ; i++)&#123; System.out.println(a[i]); &#125; //for-each循环 for(int temp : a)&#123; System.out.println(temp); &#125; //打印数组 System.out.println(Arrays.toString(a) );; &#125;数组拷贝简单的数组赋值，只是浅拷贝，二者指向的还是一个堆栈空间。如果需要进行数值拷贝，则通过Arrays.copyOf进行数据的拷贝。123456int[] a = &#123;1,2,3,4&#125; ; int[] b ; b = a ; //这是简单的赋值，浅拷贝b[1] = 10 ;// 则a[1]也变为10 b = Arrays.copyOf(a,a.length) ; //b[1] = 2 ; //则a[1] 还是为10并不影响。Arrays包含许多关于数组的操作,包括数组的打印，数组的拷贝，排序等功能。 数组排序通过Arrays.sort（a），可以对数组a进行排序（无返回值）。该排序方法使用了优化的快速排序算法(可以阅读源码，理解排序方法)。 数组填充通过Arrays.fill（a,value），用数据填充数组。 多维数组多维数组的初始化方法： double[][] balances ; balances = new double[NYEARS] [NRATES]; int[][] magicSquare = { {16, 3, 2, 13}， {5, 10, 11, 8}, (9, 6, 7, 12}, {4, 15, 14, 1} };多维数组的迭代for循环迭代，只需要通过下标进行迭代即可。for-each迭代格式如下： for (doubleG row : a) for (double value : row) do something with value 多维数组的打印通过Arrays.deepToString进行多为数组的打印。 不规则的数组不规则数组一般是多维的维数可能不一致。如果需要创建多维数组，必须先分配具有所含行数的数组，然后再分配行数组的长度。 8. 大数值如果基本的整数和浮点数精度不能够满足需求， 那么可以使用jaVa.math 包中的两个很有用的类：Biglnteger 和 BigDecimaL 这两个类可以处理包含任意长度数字序列的数值。Biglnteger 类实现了任意精度的整数运算， BigDecimal 实现了任意精度的浮点数运算。123456public static void bigData()&#123; //通过BigInter和BigDecimal可以实现大数的运算,需要将数据转化为大数对象，再进行运算。 BigInteger a = BigInteger.valueOf(1) ; a = a.add(BigInteger.valueOf(Long.MAX_VALUE)); System.out.println(a.toString());&#125;","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"},{"name":"JAVASE","slug":"JAVASE","permalink":"http://example.com/tags/JAVASE/"}],"categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://example.com/categories/JavaSE/"}]},{"title":"1. Java介绍","date":"2021-10-26T06:59:14.834Z","path":"wiki/程序技术/Java/JavaSE/第三章-Java基本程序结构/java introduce/","text":"1. Java语言的特性2. JAVA8新特性1. Java语言的特性1 ) 简单性 Java语法相对于C++而言，少了很多复杂与语法，如头文件，指针运算，结构，联合，操作符重载，虚基类等。 2 ) 面向对象 面向对象是一种程序设计技术，与之相对应的技术为面向过程。面向对象的程序设计在设计方法上着重于事物的属性和行为的设计，而不是具体的过程上进行设计。 3 ) 分布式 分布式也就是将一个任务分到多个服务器进行解决。Java提供了丰富的例程库，用于处理TCP/IP协议。 4 ) 健壮性 Java在对程序错误的检测包括，编译时错误和运行时错误，消除了容易出错的情况。并且对于Java采用的指针模型，可以消除重写内存和损坏数据的可能性。 5 ) 安全性 从一开始，Java 就设计成能够防范各种攻击，其中包括： •运行时堆栈溢出。 如蠕虫和病毒常用的攻击手段。 •破坏自己的进程空间之外的内存。 •未经授权读写文件 Java 浏览器插件不再信任远程代码，除非代码有数字签名而且用户同意执行这个代码 6 ) 体系结构中立 编译器生成字节码文件(类似于用一个新语言进行解释),只要有java运行环境，就能够在计算机上运行程序。虚拟机可以将执行最频繁的字节码序列翻译成机器码(即时编译)，加快运行速度。(字节码的解释程序由C语言进行实现。) Java源程序-编译器&gt;生成字节码文件(出现频繁的字节码序列会被翻译成机器码)-java虚拟机&gt;运行字节码文件，运行Java程序。 7 ) 可移植性 Java中的int永远为32位的整数，与平台无关。字符数据通过Unicode格式存储。 8 ) 解释型 Java 解释器可以在任何移植了解释器的机器上执行 Java 字节码。Java是解释型语言。 9 ) 高性能 即时编译器可以监控经常执行哪些代码并优化这些代码以提高速度。更为复杂的优化是消除函数调用（即“ 内联”）。即时编译器知道哪些类已经加载。基于当前加载的类集， 如果特定的函数不会被覆盖，就可以使用内联。必要时，还可以撤销优化。 10 ) 多线程 多线程可以带来更好的交互响应和实时行为。 11 ) 动态性 从各种角度看， Java 与 C 或 C++ 相比更加具有动态性。它能够适应不断发展的环境库中可以自由地添加新方法和实例变量， 而对客户端却没有任何影响。在Java 中找出运行时类型信息十分简单。当需要将某些代码添加到正在运行的程序中时， 动态性将是一个非常重要的特性。 总结Java特性：Java采用字节码文件作为编译后的文件，通过JVM生成字节码文件，并且对于常用出现的字节码序列会转化为机器码(即时编译器))，加快程序编译运行速度。并且Java与平台无关，拥有丰富的库，能够支持分布式，动态式，多线程。并且很安全。 2. JAVA8新特性Lambda表达式，包含默认方法的接口，流和时间/日期库 JAVA开发环境 一些Java环境术语 JDK（Java Development Kit）: 编写Java程序的程序员使用的软件。在Java 1.2-1.4版本被称为Java SDK (软件开发包,Software Development Kit)。我们现在经常看到关于JDK1.8,以及Java8，那么二者之间是什么关系呢？Java早期发布版本是直接增加小数点后面的数值。例如1.3，1.4，1.5等。而在2006年后版本号进行简化，开始次啊用Java SE6,Java SE 7 ,Java SE 8 作为版本号，这就是我们常熟悉的版本号。但是对于其内部的版本号，分别是1.6.0，1.7.0，1.8.0。对于版本号为1.8.0_31（Java SE 8u31）表示的是对Java SE 8 的第31次修改后的版本。 JRE（Java Runtime Enviroment）:运行Java程序的用户使用的软件。 SE(Standard Edition): 用于桌面或简单服务器应用的Java平台。 EE(Enterprise Edition):用于复杂服务器应用的Java平台。 ME(Micro Edition):用于手机和其它小型设备的Java平台。 命令行操作 通过命令行命令查看Java版本号 1javac -version 通过命令行运行Java程序 通过javac编译java源程序，通过java程序启动java虚拟机，虚拟机执行编译器放在class文件的字节码。 12javac xxxx.javajava xxxx 在编译运行java程序的时候，会将程序中包含main方法的类名提供给字节码解释器，以便启动程序。","tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://example.com/tags/JAVA/"},{"name":"JAVASE","slug":"JAVASE","permalink":"http://example.com/tags/JAVASE/"}],"categories":[{"name":"JavaSE","slug":"JavaSE","permalink":"http://example.com/categories/JavaSE/"}]},{"title":"48.旋转数组","date":"2021-10-26T04:21:45.745Z","path":"wiki/算法/Leetcode/数组/48.旋转数组/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445// 通过辅助数组进行实现 public void rotate(int[][] matrix) &#123; int[][] totataMatrix = new int[matrix.length][matrix.length] ; int i = 0 , j = 0 , len = matrix.length; for(i = 0 ; i &lt; len ; i++)&#123; for(j = 0 ; j &lt; len ; j++)&#123; totataMatrix[j][len-i-1] = matrix[i][j]; &#125; &#125; for(i = 0 ; i &lt; len ; i++)&#123; for(j = 0 ; j &lt; len ; j++)&#123; matrix[i][j] = totataMatrix[i][j]; &#125; &#125; &#125; // 如果需要原地旋转，那就必须需要考虑数组下标之间的关系，然后进行旋转操作 public void rotateTwo(int[][] matrix) &#123; int i = 0 , j = 0 , len = matrix.length; for(i = 0 ; i &lt; len / 2; i++)&#123; for(j = 0 ; j &lt; (len+1) /2 ; j++)&#123; int temp = matrix[i][j] ; matrix[i][j] = matrix[len-j-1][i] ; matrix[len-j-1][i] = matrix[len-i-1][len-j-1] ; matrix[len-i-1][len-j-1] = matrix[j][len-i-1] ; matrix[j][len-i-1] = temp ; &#125; &#125; &#125; // 数组选择90度又可以转换为数组翻转问题，首先进行上下翻转，然后再根据主对角线进行翻转。 public void rotateThree(int[][] matrix) &#123; int i = 0 , j = 0 , len = matrix.length; for(i = 0 ; i &lt; len / 2; i++)&#123; for(j = 0 ; j &lt; (len+1) /2 ; j++)&#123; int temp = matrix[i][j] ; matrix[i][j] = matrix[len-j-1][i] ; matrix[len-j-1][i] = matrix[len-i-1][len-j-1] ; matrix[len-i-1][len-j-1] = matrix[j][len-i-1] ; matrix[j][len-i-1] = temp ; &#125; &#125; &#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"}],"categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Leetcode","slug":"算法/Leetcode","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/"},{"name":"数组","slug":"算法/Leetcode/数组","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/%E6%95%B0%E7%BB%84/"}]},{"title":"36.有效数独","date":"2021-10-26T02:09:42.061Z","path":"wiki/算法/Leetcode/数组/36.有效数独/","text":"1234567891011121314151617181920212223242526272829303132333435public boolean isValidSudokuTwo(char[][] board) &#123; int i = 0 , j = 0 , k = 0 , l = 0 , idx ; HashMap&lt;Integer, HashSet&lt;Character&gt;&gt; area = new HashMap&lt;Integer, HashSet&lt;Character&gt;&gt;() ; //判断行列是否符合数独条件 for(i = 0 ; i &lt; board.length ; i++)&#123; HashMap&lt;Character, Integer&gt; row = new HashMap&lt;Character, Integer&gt;() ; HashMap&lt;Character, Integer&gt; col = new HashMap&lt;Character, Integer&gt;() ; for(j = 0 ; j &lt; board.length ; j++)&#123; if(board[i][j] != &#x27;.&#x27;)&#123; if(row.containsKey(board[i][j]))&#123; return false; &#125;else&#123; row.put(board[i][j],1) ; &#125; &#125; if(board[j][i] != &#x27;.&#x27;)&#123; if(col.containsKey(board[j][i]))&#123; return false; &#125;else&#123; col.put(board[j][i],1) ; &#125; &#125; idx = (i/3)*3 + j/3 ; if(!area.containsKey(idx))&#123; area.put(idx,new HashSet&lt;Character&gt;()) ; &#125; if(area.get(idx).contains(board[i][j]))&#123; return false; &#125;else&#123; area.get(idx).add(board[i][j]) ; &#125; &#125; &#125; return true ; &#125;","tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"数组","slug":"数组","permalink":"http://example.com/tags/%E6%95%B0%E7%BB%84/"}],"categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Leetcode","slug":"算法/Leetcode","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/"},{"name":"数组","slug":"算法/Leetcode/数组","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Leetcode/%E6%95%B0%E7%BB%84/"}]},{"title":"Git常见的问题及解决方案","date":"2021-10-25T11:43:34.168Z","path":"wiki/程序技术/Git/问题/","text":"Git使用教程Git常见问题1. 当能够正常运行的项目突然无法运行，出现找不到资源文件错误的时候。首先，能运行就标明项目本身并没有问题。对于资源文件找不到，可能就和系统的缓存加载有关。 所以这个时候，可以对项目的依赖文件进行重新的审查，如果没有问题，再查看依赖是否有问题。对于Maven项目的依赖问题，可以重新clean被依赖的工程，再重新install被依赖的工程。重新加载资源文件。 2. git提交代码到Github时候，出现fatal: unable to access ‘https://github.com/zzcpage/zzcpage.github.io.git/‘: OpenSSL SSL_read: Connection was reset, errno 10054产生的原因：一般是这是因为服务器的SSL证书没有经过第三方机构的签署，所以才报错。 解决方法：解除ssl验证后，再次git即可。 通过git config —global http.sslVerify “false”命令进行解除SSL验证，解除完之后，再次Git即可提交成功。","tags":[{"name":"Git","slug":"Git","permalink":"http://example.com/tags/Git/"},{"name":"Bug解决方案","slug":"Bug解决方案","permalink":"http://example.com/tags/Bug%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"}],"categories":[{"name":"程序技术","slug":"程序技术","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"},{"name":"Git","slug":"程序技术/Git","permalink":"http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Git/"}]},{"title":"Hello World","date":"2021-10-25T08:23:20.423Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 如何科学有效上网借助Edge的有效的扩展工具，进行安全上网","tags":[],"categories":[]}]}