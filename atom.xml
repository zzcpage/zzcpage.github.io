<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-12-14T08:55:49.108Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>ZZC</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>http://example.com/wiki/程序技术/Python/数据处理/python数据处理/python数据处理-阅读笔记(一)/</id>
    <published>2021-12-14T08:55:49.108Z</published>
    <updated>2021-12-14T08:55:49.108Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/"/>
    
      <category term="数据处理" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
      <category term="python数据处理" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>CV2问题集锦</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/cv2/"/>
    <id>http://example.com/wiki/程序技术/BUG集锦/python/cv2/</id>
    <published>2021-12-14T02:26:49.895Z</published>
    <updated>2021-12-14T02:29:39.514Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-关于cv2-imshow不显示，一闪而过的问题"><a href="#1-关于cv2-imshow不显示，一闪而过的问题" class="headerlink" title="1.  关于cv2.imshow不显示，一闪而过的问题"></a>1.  关于cv2.imshow不显示，一闪而过的问题</h4><p>我们在使用cv2显示图片的时候，一定要加上cv2.waitKey进行等待图片，否则会不显示。示例代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&quot;1.jpg&quot;</span>)  <span class="comment"># 图像读取</span></span><br><span class="line">x,y = img.shape[:<span class="number">2</span>] <span class="comment">#获取图片的高和宽</span></span><br><span class="line"><span class="comment"># 创建一个窗口</span></span><br><span class="line">cv.namedWindow(<span class="string">&#x27;test&#x27;</span>,cv.WINDOW_NORMAL) <span class="comment"># 通过cv.WINDOW_NORMAL可以完整显示图片</span></span><br><span class="line">cv.imshow(<span class="string">&#x27;test&#x27;</span>,img) <span class="comment"># 显示图片</span></span><br><span class="line">cv.waitKey(<span class="number">0</span>) <span class="comment"># 等待键盘输入，0表示无限等待，如果不调用waitKey窗口就会一闪而过，看不到任何图片</span></span><br><span class="line">cv.destroyAllWindows() <span class="comment"># 销毁所有串口</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-关于cv2-imshow不显示，一闪而过的问题&quot;&gt;&lt;a href=&quot;#1-关于cv2-imshow不显示，一闪而过的问题&quot; class=&quot;headerlink&quot; title=&quot;1.  关于cv2.imshow不显示，一闪而过的问题&quot;&gt;&lt;/a&gt;1.  关于cv2
      
    
    </summary>
    
      <category term="Bug" scheme="http://example.com/categories/Bug/"/>
    
    
      <category term="Bug" scheme="http://example.com/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/vsnode/"/>
    <id>http://example.com/wiki/程序技术/BUG集锦/vsnode/</id>
    <published>2021-12-14T00:54:21.517Z</published>
    <updated>2021-12-14T00:54:21.517Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="BUG集锦" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"/>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/idea/"/>
    <id>http://example.com/wiki/程序技术/BUG集锦/idea/</id>
    <published>2021-12-14T00:54:13.967Z</published>
    <updated>2021-12-14T00:54:13.967Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="BUG集锦" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>Pycharm问题集锦</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/Pycharm/"/>
    <id>http://example.com/wiki/程序技术/BUG集锦/python/Pycharm/</id>
    <published>2021-12-14T00:54:06.688Z</published>
    <updated>2021-12-14T02:26:43.462Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Pycharm误删代码如何恢复"><a href="#Pycharm误删代码如何恢复" class="headerlink" title="Pycharm误删代码如何恢复"></a>Pycharm误删代码如何恢复</h5><p>在误删代码的父文件夹上，点击鼠标右键，找到Local History，点击后即可找到之前的代码，根据情况进行恢复(Revert)。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;Pycharm误删代码如何恢复&quot;&gt;&lt;a href=&quot;#Pycharm误删代码如何恢复&quot; class=&quot;headerlink&quot; title=&quot;Pycharm误删代码如何恢复&quot;&gt;&lt;/a&gt;Pycharm误删代码如何恢复&lt;/h5&gt;&lt;p&gt;在误删代码的父文件夹上，点击鼠标右键
      
    
    </summary>
    
      <category term="Bug" scheme="http://example.com/categories/Bug/"/>
    
    
      <category term="Bug" scheme="http://example.com/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/%E7%88%AC%E8%99%AB/"/>
    <id>http://example.com/wiki/程序技术/BUG集锦/python/爬虫/</id>
    <published>2021-12-14T00:53:54.425Z</published>
    <updated>2021-12-14T02:26:43.332Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="BUG集锦" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/"/>
    
      <category term="python" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/BUG%E9%9B%86%E9%94%A6/python/"/>
    
    
  </entry>
  
  <entry>
    <title>CV2的常用方法</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/CV2/"/>
    <id>http://example.com/wiki/程序技术/Python/图像处理/CV2/</id>
    <published>2021-12-13T07:33:05.449Z</published>
    <updated>2021-12-14T02:26:23.639Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-图片读取显示"><a href="#1-图片读取显示" class="headerlink" title="1. 图片读取显示"></a>1. 图片读取显示</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&quot;1.jpg&quot;</span>)  <span class="comment"># 图像读取</span></span><br><span class="line">x,y = img.shape[:<span class="number">2</span>] <span class="comment">#获取图片的高和宽</span></span><br><span class="line"><span class="comment"># 创建一个窗口</span></span><br><span class="line">cv.namedWindow(<span class="string">&#x27;test&#x27;</span>,cv.WINDOW_NORMAL) <span class="comment"># 通过cv.WINDOW_NORMAL可以完整显示图片</span></span><br><span class="line">cv.imshow(<span class="string">&#x27;test&#x27;</span>,img) <span class="comment"># 显示图片</span></span><br><span class="line">cv.waitKey(<span class="number">0</span>) <span class="comment"># 等待键盘输入，0表示无限等待，如果不调用waitKey窗口就会一闪而过，看不到任何图片</span></span><br><span class="line">cv.destroyAllWindows() <span class="comment"># 销毁所有串口</span></span><br></pre></td></tr></table></figure><h4 id="2-图片的操作-修改图片大小"><a href="#2-图片的操作-修改图片大小" class="headerlink" title="2. 图片的操作-修改图片大小"></a>2. 图片的操作-修改图片大小</h4><p>通过cv2.resize可以修改图片的大小，指定尺寸size（宽，高）,并且可以指定插入方式，不同插入方式得到的缩放图片的效果不一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line">x, y = img.shape[:<span class="number">2</span>]  <span class="comment"># 获取图片的高和宽</span></span><br><span class="line"><span class="comment"># 输出尺寸为(宽，高)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">cv2.resize(InputArray src, OutputArray dst, Size, fx, fy, interpolation)</span></span><br><span class="line"><span class="string">InputArray src： 输入图片</span></span><br><span class="line"><span class="string">InputArray src:  输出图片</span></span><br><span class="line"><span class="string">Size: 输出图片尺寸(宽，高)</span></span><br><span class="line"><span class="string">fx,fy:沿x轴，y轴的缩放系数</span></span><br><span class="line"><span class="string">interpolation:插入方式</span></span><br><span class="line"><span class="string">插入方式有如下几种：</span></span><br><span class="line"><span class="string">INTER_NEAREST：最近邻插值</span></span><br><span class="line"><span class="string">INTER_LINEAR：双线性插值（默认设置）</span></span><br><span class="line"><span class="string">INTER_AREA:使用像素区域关系进行重采样。</span></span><br><span class="line"><span class="string">INTER_CUBIC:4x4像素邻域的双三次插值</span></span><br><span class="line"><span class="string">INTER_LANCZOS4:8x8像素邻域的Lanczos插值</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">img1 = cv.resize(img,(<span class="built_in">int</span>(y/<span class="number">4</span>),<span class="built_in">int</span>(x/<span class="number">4</span>))) <span class="comment"># 修改图片大小，缩放为原来的1/4</span></span><br><span class="line"><span class="comment">#cv.namedWindow(&#x27;test&#x27;,cv.WINDOW_NORMAL)</span></span><br><span class="line">cv.imshow(<span class="string">&#x27;test&#x27;</span>,img1)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line">img2 = cv.resize(img,(<span class="number">0</span>,<span class="number">0</span>),fx=<span class="number">0.25</span>,fy=<span class="number">0.25</span>,interpolation=cv.INTER_AREA)</span><br><span class="line">cv.imshow(<span class="string">&#x27;test&#x27;</span>, img2)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-图片的操作-灰度化"><a href="#3-图片的操作-灰度化" class="headerlink" title="3. 图片的操作-灰度化"></a>3. 图片的操作-灰度化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&quot;1.jpg&quot;</span>)</span><br><span class="line">img_gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) <span class="comment"># 灰度化</span></span><br><span class="line">cv.namedWindow(<span class="string">&#x27;test&#x27;</span>, cv.WINDOW_NORMAL)</span><br><span class="line">cv.imshow(<span class="string">&#x27;test&#x27;</span>, img_gray)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-图片读取显示&quot;&gt;&lt;a href=&quot;#1-图片读取显示&quot; class=&quot;headerlink&quot; title=&quot;1. 图片读取显示&quot;&gt;&lt;/a&gt;1. 图片读取显示&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
      <category term="Python" scheme="http://example.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="图像处理" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>滑块验证码破解</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/%E6%BB%91%E5%9D%97%E7%A0%B4%E8%A7%A3/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/滑块破解/</id>
    <published>2021-12-13T07:01:04.559Z</published>
    <updated>2021-12-14T02:05:21.794Z</updated>
    
    <content type="html"><![CDATA[<h4 id="滑块验证码破解"><a href="#滑块验证码破解" class="headerlink" title="滑块验证码破解"></a>滑块验证码破解</h4><p>最近在爬虫开发的过程中，遇到了关于滑块验证码，需要进行滑块验证码破解。这里涉及到图像方面的技术，可以借助OpenCV进行解决。</p><h4 id="通过CV2解决滑块验证"><a href="#通过CV2解决滑块验证" class="headerlink" title="通过CV2解决滑块验证"></a>通过CV2解决滑块验证</h4><p>这里通过CV2库进行滑块验证的解决。简单介绍一下滑块验证的几个步骤。</p><h5 id="1-获取图片（不带缺口的图片，带缺口的图片）"><a href="#1-获取图片（不带缺口的图片，带缺口的图片）" class="headerlink" title="1. 获取图片（不带缺口的图片，带缺口的图片）"></a>1. 获取图片（不带缺口的图片，带缺口的图片）</h5><p>根据网页，获取到滑块图片，一般来说分为两个图片，一个是缺口图，也就是缺少缺口的图片。一个是滑块图，也就是缺口图缺少的图片。假设缺口图为img1,滑块图为img2。<br>img1:<br><img src="/images/spider/s1.PNG" alt="img1"></p><p>img2:<br><img src="/images/spider/s2.PNG" alt="img2"></p><h5 id="2-识别缺口位置，计算滑动距离"><a href="#2-识别缺口位置，计算滑动距离" class="headerlink" title="2. 识别缺口位置，计算滑动距离"></a>2. 识别缺口位置，计算滑动距离</h5><p>一般而言，我们通过网页获取到的图片和实际在网页上显示的图片大小是不一致的。读者可以根据自己需要破解的滑块验证码进行校验。查看在网页中的图片大小和下载的图片大小是否一致。如果不一致，那么需要跳转下载的图片的大小。<br>读取图片，然后跳转图片的大小为网页中显示的大小。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">img1 = cv2.imread(缺口图的路径)</span><br><span class="line">img1 = cv2.resize(img1, (网页缺口图实际宽度, 网页缺口图实际高度))</span><br><span class="line">img2 = cv2.imread(缺块图的路径)</span><br><span class="line">img2 = cv2.resize(img2, (网页缺块图实际宽度, 网页缺块图实际高度))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>如果要识别缺口位置，可以借助cv2库的matchTemplate方法来获取缺块距离缺口的位置。实现代码如下所示，一般来说这种方法识别会带有误差，也就是存在失败的情况，这个时候可以在程序中进行设置，进行多次尝试解决滑块验证(可以采用循环处理的办法，重复多次滑块验证解决流程)，以达到解决滑块问题的效果。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) # 对图片灰度化处理</span><br><span class="line">img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)# 对图片灰度化处理</span><br><span class="line">res = cv2.matchTemplate(img1_gray, img2_gray, cv2.TM_CCOEFF_NORMED) </span><br><span class="line">value = cv2.minMaxLoc(res)  #获取缺口和缺块的距离</span><br><span class="line">value = value[3][0] #value值就是缺块距离缺口的距离</span><br></pre></td></tr></table></figure></p><h5 id="3-模拟运动"><a href="#3-模拟运动" class="headerlink" title="3. 模拟运动"></a>3. 模拟运动</h5><p>我们可以借助selenium进行模拟滑块拖动的过程，但是在模拟滑块的拖动过程中，需要模拟人的速度。人在拖动滑块的时候，正常是先快后慢的过程，这个时候我们可以设置一个先快后慢的过程（可以设置两个运动方程，一段加速度为a1,一段加速度为a2），用于模拟人拖动滑块。示例代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ditance为移动的距离，也就是上面获得的value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTrack</span>(<span class="params">self,distance</span>):</span></span><br><span class="line">    <span class="comment">#用于存储每个过程中，拉动滑块的距离</span></span><br><span class="line">    track = []</span><br><span class="line">    <span class="comment"># 当前位移</span></span><br><span class="line">    current = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 当距离超过什么时候，进行减速</span></span><br><span class="line">    mid = distance * <span class="number">4</span> / <span class="number">5</span></span><br><span class="line">    <span class="comment"># 计算间隔</span></span><br><span class="line">    t = <span class="number">0.2</span></span><br><span class="line">    <span class="comment"># 初速度</span></span><br><span class="line">    v = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> current &lt; distance:</span><br><span class="line">        <span class="keyword">if</span> current &lt; mid:</span><br><span class="line">            <span class="comment"># 加速度为正2</span></span><br><span class="line">            a = <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 加速度为负3</span></span><br><span class="line">            a = -<span class="number">3</span></span><br><span class="line">        <span class="comment"># 初速度v0</span></span><br><span class="line">        v0 = v</span><br><span class="line">        <span class="comment"># 当前速度v = v0 + at</span></span><br><span class="line">        v = v0 + a * t</span><br><span class="line">        <span class="comment"># 移动距离x = v0t + 1/2 * a * t^2</span></span><br><span class="line">        move = v0 * t + <span class="number">1</span> / <span class="number">2</span> * a * t * t</span><br><span class="line">        <span class="comment"># 当前位移</span></span><br><span class="line">        current += move</span><br><span class="line">        <span class="comment"># 加入轨迹</span></span><br><span class="line">        track.append(<span class="built_in">round</span>(move))</span><br><span class="line">    <span class="keyword">return</span> track</span><br></pre></td></tr></table></figure><p>通过selenium模拟运动，可以借助ActionChains来拖拽滑块。首先我们需要获取到滑块元素。<br>这个我们可以通过F12来查看拖拽滑块元素的属性，然后通过driver获取到元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ActionChains(driver).click_and_hold(拖拽滑块的元素).perform() <span class="comment"># 点击并且不释放鼠标</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> track:     <span class="comment"># 根据轨迹，移动元素</span></span><br><span class="line">    ActionChains(driver).move_by_offset(xoffset=x, yoffset=<span class="number">0</span>).perform() <span class="comment">#移动元素</span></span><br><span class="line">time.sleep(<span class="number">0.5</span>) </span><br><span class="line">ActionChains(driver).release().perform() <span class="comment">#释放元素</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;滑块验证码破解&quot;&gt;&lt;a href=&quot;#滑块验证码破解&quot; class=&quot;headerlink&quot; title=&quot;滑块验证码破解&quot;&gt;&lt;/a&gt;滑块验证码破解&lt;/h4&gt;&lt;p&gt;最近在爬虫开发的过程中，遇到了关于滑块验证码，需要进行滑块验证码破解。这里涉及到图像方面的技术，可以
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Web项目部署</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Java/JavaWeb/%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2/"/>
    <id>http://example.com/wiki/程序技术/Java/JavaWeb/项目部署/</id>
    <published>2021-12-10T09:31:23.397Z</published>
    <updated>2021-12-13T01:22:04.658Z</updated>
    
    <content type="html"><![CDATA[<ol><li>安装Tomcat</li></ol><blockquote><ol><li>通过 tar -zxvf 解压tomcat。</li><li>修改catalina.sh，在脚本开头增加export JAVA_HOME指定jdk路径。</li></ol></blockquote><p>修改Tomcat的端口：<br>将8080，8005，8009修改其它端口即可。<br> <Connector port="8080"               maxThreads="150" minSpareThreads="25" maxSpareThreads="75"               enableLookups="false" redirectPort="8443" acceptCount="100"               connectionTimeout="20000" disableUploadTimeout="true" /></p><Server port="8005" shutdown="SHUTDOWN"><Connector port="8009" protocol="AJP/1.3" redirectPort="8443"/><ol><li>下载jdk</li></ol><p>下载jdk，然后通过tar -zxvf 进行解压jdk</p><ol><li>打包项目</li></ol><p>将项目打包war包，然后放入到tomcat的webapp目录下即可。</p><blockquote><ol><li>访问路径</li></ol></blockquote><p>将War包包放到webapp下之后，访问路径的名称就是war包的名称，假设war包为AAA.war,部署端口为8080.<br>ip地址为xxx.xxx.xxx则访问路径为：<a href="https://xxx.xxx.xxx/AAA。">https://xxx.xxx.xxx/AAA。</a></p><blockquote><ol><li>报错解决</li></ol></blockquote><h3 id="Cannot-find-usr-tomcat-tomcat9-bin-setclasspath-sh"><a href="#Cannot-find-usr-tomcat-tomcat9-bin-setclasspath-sh" class="headerlink" title="Cannot find /usr/tomcat/tomcat9/bin/setclasspath.sh"></a>Cannot find /usr/tomcat/tomcat9/bin/setclasspath.sh</h3><p>执行：unset CATALINA_HOME</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;安装Tomcat&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;通过 tar -zxvf 解压tomcat。&lt;/li&gt;
&lt;li&gt;修改catalina.sh，在脚本开头增加export JAVA_HOME指定jdk路径。&lt;/li&gt;
&lt;/ol&gt;

      
    
    </summary>
    
      <category term="JavaWeb" scheme="http://example.com/categories/JavaWeb/"/>
    
    
      <category term="JavaWeb" scheme="http://example.com/tags/JavaWeb/"/>
    
  </entry>
  
  <entry>
    <title>反爬技巧</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/%E5%8F%8D%E7%88%AC%E6%8A%80%E5%B7%A7/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/反爬技巧/</id>
    <published>2021-12-10T08:19:06.344Z</published>
    <updated>2021-12-10T08:21:56.064Z</updated>
    
    <content type="html"><![CDATA[<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>这里主要总结了一些爬虫开发过程中的反爬技巧。</p><blockquote><ol><li>正确设置headers</li></ol></blockquote><p>通常我们简单设置一下User-Agent就能够获取到网页内容。但是对于一些网站，通过request获取到的网页内容，通常又和正常访问网页获取到的内容不一致。这里就需要根据网页的Headers来设置request内的headers属性，用于避免被检测。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;这里主要总结了一些爬虫开发过程中的反爬技巧。&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;正确设置headers&lt;/li&gt;
&lt;/ol
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>debug反爬</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/debug%E5%8F%8D%E7%88%AC/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/debug反爬/</id>
    <published>2021-12-10T07:01:24.965Z</published>
    <updated>2021-12-10T07:08:39.363Z</updated>
    
    <content type="html"><![CDATA[<h4 id="关于解决-function-anonymous-debugger-的问题"><a href="#关于解决-function-anonymous-debugger-的问题" class="headerlink" title="关于解决(function anonymous() {debugger})的问题"></a>关于解决(function anonymous() {debugger})的问题</h4><h5 id="1-实现原理"><a href="#1-实现原理" class="headerlink" title="1. 实现原理"></a>1. 实现原理</h5><p>如何实现无限debugger呢？实现无限debugger就是不断的打断你，页面跳转到source页面，阻止你看内容。<br>写一个不断调用debugger即可。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;<span class="keyword">var</span> a = <span class="keyword">new</span> <span class="built_in">Date</span>(); <span class="keyword">debugger</span>; <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Date</span>() - a &gt; <span class="number">100</span>;&#125;())</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h5 id="2-问题解决"><a href="#2-问题解决" class="headerlink" title="2. 问题解决"></a>2. 问题解决</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;关于解决-function-anonymous-debugger-的问题&quot;&gt;&lt;a href=&quot;#关于解决-function-anonymous-debugger-的问题&quot; class=&quot;headerlink&quot; title=&quot;关于解决(function anonym
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫-Scrapy进阶</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB-Scrapy%E8%BF%9B%E9%98%B6/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/Python爬虫-Scrapy进阶/</id>
    <published>2021-12-09T09:00:55.657Z</published>
    <updated>2021-12-14T08:49:51.577Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Spider模块"><a href="#1-Spider模块" class="headerlink" title="1.Spider模块"></a><a href="#one">1.Spider模块</a></h3><h3 id="2-Item-Loader"><a href="#2-Item-Loader" class="headerlink" title="2.Item Loader"></a><a href="#two">2.Item Loader</a></h3><h3 id="3-请求与响应"><a href="#3-请求与响应" class="headerlink" title="3.请求与响应"></a><a href="#three">3.请求与响应</a></h3><h2 id="深入Scrapy爬虫框架"><a href="#深入Scrapy爬虫框架" class="headerlink" title="深入Scrapy爬虫框架"></a>深入Scrapy爬虫框架</h2><h3 id="1-Spider模块-1"><a href="#1-Spider模块-1" class="headerlink" title="1. Spider模块"></a><a id="one"></a>1. Spider模块</h3><p>Spider模块是定义爬虫的动作及分析网页结构的地方，我们容易看出，在这里给出了解析网页获取元素，并进行是否继续爬取下一个网页的操作(也就是爬虫的动作)。Spider的执行流程</p><blockquote><ol><li><p>从入口URL初始化Request并设置回调函数。这个Reuquest下载完毕返回Response，并作为参数传送给回调函数，Spider初始的Request是通过调用start_requests()方法获取。start_requests()读取start_urls中的URL，并以parse为回调函数生成Request。也就是说初始的URL，只需要在start_urls加入，系统会自动的获取response，并以parse()为解析函数。</p></li><li><p>在回调函数分析Response，返回Item对象，dict,ruquest或者一个包括三者的可迭代容器。其中返回的Request对象会经过Scrapy处理，下载相应内容，并调用设置相应的解析函数。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">request = scrapy.Request(url=url,callback=self.parse_body) <span class="comment">#调用Request方法，并设置解析函数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>在解析函数内，可以使用页面解析技术，对页面元素进行解析，可以用BeautifuleSoup等等技术。通过response可以获取到响应的内容。将分析的数据生成item</p></li><li><p>由spider返回item,可以经过Item Pipeline被存到数据库或使用Feed exports存入到文件中。</p></li></ol></blockquote><h5 id="Spider类的成员变量"><a href="#Spider类的成员变量" class="headerlink" title="Spider类的成员变量"></a>Spider类的成员变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> @:param name 定义spider名字的字符串，名字必须唯一。可以生成多个相同的spider实例</span></span><br><span class="line"><span class="string"> 通常可以用网站域名命名spider</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> @:param allowed_domains: 包含了spder允许爬取的域名列表。</span></span><br><span class="line"><span class="string"> 当OffsiteMiddleware组件启用时，域名不在列表中的URL不会被跟进。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> @:param statr_urls:URL列表，当没有配置statr_requests9）f方法的时候，spider会从该列表开始进行爬取。也就是说爬虫开始爬取的</span></span><br><span class="line"><span class="string"> URL就是从start_urls中获取。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> @:param custom_setting：该设置是一个dict,当启动spider时，该设置将会覆盖项目级的设置。也就是</span></span><br><span class="line"><span class="string"> 说可以在这里对spider单独定义。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> @:param crawler 该属性在初始化class后，由类方法from_crawler()设置。并且链接了</span></span><br><span class="line"><span class="string"> 本spider实例羽Crawler对象。</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line"> name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line"> allowed_domains = [<span class="string">&quot;www.baidu.com&quot;</span>]</span><br><span class="line"> start_urls = [</span><br><span class="line">     <span class="string">&quot;https://www.baidu.com&quot;</span></span><br><span class="line"> ]</span><br><span class="line"> custom_settings = &#123;&#125;</span><br><span class="line"> crawler = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="Spider类的方法"><a href="#Spider类的方法" class="headerlink" title="Spider类的方法"></a>Spider类的方法</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常用的Spider方法</span></span><br><span class="line"><span class="comment"># 该方法必须返回一个可迭代对象，对象包含spider用于爬虫的第一个request。</span></span><br><span class="line"><span class="comment"># 也就是说 start_requests是项目启动的开始，是根据start_url作为项目启动URL</span></span><br><span class="line"><span class="comment"># 如果没有设置start_requests方法，就会默认从start_urls的url生成Request。</span></span><br><span class="line"><span class="comment"># 如果需要定制最初爬取的Request对象，可以重写方法。</span></span><br><span class="line"><span class="comment"># 例如通过POST登录</span></span><br><span class="line"><span class="comment"># 总结来说：strt_request就是整个程序的入口，如果不指定就是直接从start_ruls中获取url，以parse()为回调函数进行解析。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">&quot;http://www.example.com/login&quot;</span>,formdata=&#123;</span><br><span class="line">        <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;john&#x27;</span>,<span class="string">&#x27;pass&#x27;</span>:<span class="string">&#x27;secret&#x27;</span></span><br><span class="line">    &#125;,callback=self.login)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># start_requests对url请求后的响应，会通过login进行处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span>(<span class="params">self,response</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_requests_from_url</span>(<span class="params">self, url</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    接受一个URL并返回用于爬取的Request对象</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response, **kwargs</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    用于解析网页内容，一般作为初始URL解析的回调函数</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :param kwargs:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">spider, reason</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    当Spider关闭时，该函数被调用。可以用来在spider关闭时，释放占用的资源。</span></span><br><span class="line"><span class="string">    :param reason:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>Scrapy除了Spider类作为基类进行扩展，还提供了CrawlSpider，XMLFeedSpider,CSVFeedSpider和SitemapSpider等类来实现不同的爬虫任务。</p><h6 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h6><p>CrawlSpider类常用于爬取一般的网站。其中定义了一些规则(rule)来提供跟进链接功能。<br>CrawlSpider提供了新的属性rules。rules包含一个或多个Rule对象的集合。每个Rule对爬取网站的动作定义了特定的规则。如果多个Rule匹配相同的链接，则先定义的被调用。<br>CrawlSpider提供的初始URL解析方法，parse_start_url(response)。该方法返回一个Item对象或者一个Request对象或者包含二者的对象。使用示例如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCrawlSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;crawlSpider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;cnblogs.com&quot;</span>]<span class="comment">#域名</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&quot;http://www.cnblogs.com/qiyeboy/default.html?page=1&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># Rule原型</span></span><br><span class="line">    <span class="comment"># scrapy.contrib.spiders.Rule(link_exactor,callback=None,cb_kwargs=None,</span></span><br><span class="line">    <span class="comment"># follow=None,process_links=None,process_request=None)</span></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">&quot;/qiyeboy/default.html\?page=\d&#123;1,&#125;&quot;</span>,)),</span><br><span class="line">                    follow=<span class="literal">True</span>,</span><br><span class="line">                    callback=<span class="string">&#x27;parse_item&#x27;</span></span><br><span class="line">                           ),</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># LinkExtractor对象的构造</span></span><br><span class="line">    <span class="comment"># allow: 用于匹配满足正则表达式的链接</span></span><br><span class="line">    <span class="comment"># deny: 排除正则表达式匹配的链接，优先级高于allow</span></span><br><span class="line">    <span class="comment"># allow_domains：允许的域名，可以是list或str</span></span><br><span class="line">    <span class="comment"># deny_domains:排除的域名</span></span><br><span class="line">    <span class="comment"># restrict_xpaths:提取满足Xpath选择条件的链接。</span></span><br><span class="line">    <span class="comment"># restrict_css:xxxCSSxxx的链接</span></span><br><span class="line">    <span class="comment"># tags: 提取指定标签下的链接。</span></span><br><span class="line">    <span class="comment"># unique:链接是否去重</span></span><br><span class="line">    <span class="comment"># process_value:值处理函数。</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response, **kwargs</span>):</span></span><br><span class="line">        papers = response.xpath(<span class="string">&quot;.//*[@class=&#x27;day&#x27;]&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">            url = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            title = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            time = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            content = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            item = MyCrawlSpider(url=url, title=title, time=time, content=content)</span><br><span class="line">            request = scrapy.Request(url=url, callback=self.parse_body)  <span class="comment"># 调用Request方法，并设置解析函数</span></span><br><span class="line">            request.meta[<span class="string">&#x27;item&#x27;</span>] = item  <span class="comment"># 将item暂存</span></span><br><span class="line">        <span class="keyword">yield</span> request</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_body</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        body = response.xpath(<span class="string">&quot;.//*[@class=&#x27;postBody&#x27;]&quot;</span>)</span><br><span class="line">        item[<span class="string">&#x27;cimage_urls&#x27;</span>] = body.xpath(<span class="string">&#x27;.//img//@src&#x27;</span>).extract()<span class="comment"># 提取图片链接</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h6 id="XMLFeedSpider"><a href="#XMLFeedSpider" class="headerlink" title="XMLFeedSpider"></a>XMLFeedSpider</h6><p>XMLFeedSpider被设计用于通过迭代各个节点来分析XML源。迭代器可以从Iternodes,XML,HTML中选择。在XMLFeedSpider中，需要定义下列类属性来设置迭代器及标记名称。</p><ol><li>iterator</li></ol><p>用于确定使用哪个迭代器string,默认为iternodes，可选项有(1. iternodes, 2. html , 3. html)</p><ol><li><p>itertag</p><p>itertag为一个包含开始迭代的节点名string</p></li><li><p>namespaces</p><p> 称为命名空间，由(prefix,url),元组(tuple)所组成的list。这里定义了在文档中会被spider处理可用的namespace,prefix和url会被自动调用。由register_namespace()方法生成namespace。</p></li></ol><p>示例代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyXMLFeedSpider</span>(<span class="params">XMLFeedSpider</span>):</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">&quot;myxmlfeed&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;cnblogs.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;https://feed.cnblogs.com/blog/u/269038/rss&quot;</span>]</span><br><span class="line">    namespaces = [&#123;<span class="string">&#x27;n&#x27;</span>,<span class="string">&#x27;http://www.sitemaps.org/schemas/sitemap/0.9&#x27;</span>&#125;]</span><br><span class="line">    iterator = <span class="string">&#x27;html&#x27;</span> <span class="comment"># 用于定义解析方式</span></span><br><span class="line">    itertag = <span class="string">&#x27;entry&#x27;</span></span><br><span class="line">    <span class="comment">#XMLFeedSpider方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">adapt_response</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        这个方法在页面解析前和页面下载后之间被调用。可以用于修改Response内容，并再返回。</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span>  response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_node</span>(<span class="params">self, response, selector</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            当节点符合itertag时，该方法被调用。接收到的response以及相对应的Selector作为参数传递给该方法。</span></span><br><span class="line"><span class="string">            需要返回一个Item对象或Request对象，或包含二者的可迭代对象</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param selector:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(selector.xpath(<span class="string">&#x27;id/text()&#x27;</span>).extract()[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(selector.xpath(<span class="string">&#x27;title/text()&#x27;</span>).extract()[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(selector.xpath(<span class="string">&#x27;summary/text()&#x27;</span>).extract()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span>(<span class="params">self, response, results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        在页面解析后，数据返回前进行处理。主要是对返回数据的最后处理。修改Item的内容</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param results:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [response,results]</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h3 id="2-Item-Loader模块"><a href="#2-Item-Loader模块" class="headerlink" title="2. Item Loader模块"></a><a id="two"></a>2. Item Loader模块</h3><blockquote><h5 id="Item-Loader是什么？"><a href="#Item-Loader是什么？" class="headerlink" title="Item Loader是什么？"></a>Item Loader是什么？</h5></blockquote><p>Item Loader提供了一种边界的方式填充抓到的Items。Item Loader可以直接对Item分析，并提取出想要的数据保存到容器中，而Item则是机械的根据键值对对应，返回数据。所以Item Loader更加灵活，高效。</p><p>Item Loader负责数据的收集，处理和填充。Item Loader包含两个重要的组件：输入处理器(input processors)和输出处理器(output processors)。</p><ol><li>Item Loader的每个字段都包含了一个输入处理器和输出处理器。</li><li>输入处理器接收到response后，通过add_xpath,add_css,add_value等方法提取数据，并将数据保存到Item Loader中。</li><li>收集完成数据之后，通过ItemLoader.load_item()方法来填充并返回Item对象。load_item()方法内部先调用输出处理器来处理收集到的数据，结果保存到最终的Item中。</li></ol><blockquote><h5 id="Item-Loader使用方法"><a href="#Item-Loader使用方法" class="headerlink" title="Item Loader使用方法"></a>Item Loader使用方法</h5></blockquote><p>在Item中声明输入输出处理器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Item中声明输入和输出处理器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_price</span>(<span class="params">value</span>):</span></span><br><span class="line">    <span class="keyword">if</span> value.isdigit():</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line"></span><br><span class="line">    name = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags),</span><br><span class="line">        output_processor=Join(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    price = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags,filter_price),</span><br><span class="line">        output_processor=TakeFirst(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure></p><p>在Item Loader类中声明类似field_in和field_out的属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ItemLoader</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductLoadr</span>(<span class="params">ItemLoader</span>):</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line">    <span class="comment"># 声明输入输出处理器</span></span><br><span class="line">    <span class="comment">#输入处理器</span></span><br><span class="line">    name_in = MapCompose(unicode.title)</span><br><span class="line">    <span class="comment">#输出处理器</span></span><br><span class="line">    name_out = Join()</span><br><span class="line">    price_in =  MapCompose(unicode.price)</span><br><span class="line">    price_out = Join()</span><br><span class="line">    stock_in = MapCompose(unicode.stock)</span><br><span class="line">    stock_out = Join()</span><br><span class="line">    last_updated_in = MapCompose(unicode.last_updated)</span><br><span class="line">    last_updated_out = Join()</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><h5 id="Item-Loader-Context"><a href="#Item-Loader-Context" class="headerlink" title="Item Loader Context"></a>Item Loader Context</h5></blockquote><p>Item Loader Context是一个任意的键值对字典。能够被Item Loader中的输入输出处理器所共享。<br>可以用于调整输入输出处理器的行为。<br>使用Item Loader Context就是为了能够提高代码复用，便于扩展。如下代码所示，可以在原有的Item Loader基础上，对属性进行解析。如果对于不同的解析，只需要设置解析方法即可，增加了可复用性。就不需要再单独设置一个Loader ，可以复用Loader。</p><p>对于输入处理器，我们可以借助方法进行扩展。对于输出处理器，通常是再Item字段元数据进行声明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Item Loader Context</span></span><br><span class="line"><span class="comment"># 通过接收loader_context,能告诉Item loader自己能够接收Item Loader context ,</span></span><br><span class="line"><span class="comment"># 所以方法被调用的时候能将当前的active Context传递给该方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_length</span>(<span class="params">text, loader_context</span>):</span></span><br><span class="line">    unit = loader_context.get(<span class="string">&#x27;unit&#x27;</span>, <span class="string">&#x27;m&#x27;</span>)</span><br><span class="line">    <span class="comment"># 获取长度</span></span><br><span class="line">    parsed_length = <span class="built_in">len</span>(unit)</span><br><span class="line">    <span class="keyword">return</span> parsed_length</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductLoadr</span>(<span class="params">ProductLoader</span>):</span></span><br><span class="line">    <span class="comment"># 定义修改context</span></span><br><span class="line">    length_out = MapCompose(parse_length, unit=<span class="string">&#x27;cm&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><h5 id="Item-Loader-内置的处理器"><a href="#Item-Loader-内置的处理器" class="headerlink" title="Item Loader 内置的处理器"></a>Item Loader 内置的处理器</h5></blockquote><h5 id="1-MapCompose"><a href="#1-MapCompose" class="headerlink" title="1. MapCompose"></a>1. MapCompose</h5><p>输入处理器，将多个方法的执行结果按顺序组合产出最终的输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#MapCompose</span></span><br><span class="line"><span class="comment">#和Compose类似，但是这个方法对输入是：每个元素单独传入第一个函数处理，然后将结果又作为</span></span><br><span class="line"><span class="comment">#整个迭代对象传入到后面的函数进行处理。</span></span><br><span class="line"><span class="comment">#如果输入是None会被自动忽略</span></span><br><span class="line"><span class="comment">#相当于这个方法会对每个元素单独处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_world</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span> <span class="keyword">if</span> x ==<span class="string">&#x27;world&#x27;</span> <span class="keyword">else</span> x</span><br><span class="line">proc = MapCompose(filter_world,unicode.upper)</span><br><span class="line"><span class="built_in">print</span>(proc([<span class="string">u&#x27;hello&#x27;</span>,<span class="string">u&#x27;world&#x27;</span>,<span class="string">u&#x27;this&#x27;</span>,<span class="string">u&#x27;is&#x27;</span>,<span class="string">u&#x27;scrapy&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h5 id="2-Identity"><a href="#2-Identity" class="headerlink" title="2. Identity"></a>2. Identity</h5><p>最简单的处理器，不做任何处理，直接返回原来的数据，无参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Identity ：简单的处理器，不做任何处理，直接返回原来的数据</span></span><br><span class="line">identity = Identity()</span><br><span class="line"><span class="built_in">print</span>(identity([<span class="string">&quot;1231&quot;</span>,<span class="string">&quot;321&quot;</span>]))</span><br></pre></td></tr></table></figure></p><h5 id="3-TakeFirst"><a href="#3-TakeFirst" class="headerlink" title="3. TakeFirst"></a>3. TakeFirst</h5><p>返回第一个非空值，常用于单值字段的输出处理器，无参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TakeFirst:输出处理器</span></span><br><span class="line"><span class="comment"># TakeFirst用于返回第一个非空值，常用于单值字段的输出处理。</span></span><br><span class="line">proc = TakeFirst()</span><br><span class="line"><span class="built_in">print</span>(proc([<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]))</span><br></pre></td></tr></table></figure></p><h5 id="4-Compose"><a href="#4-Compose" class="headerlink" title="4. Compose"></a>4. Compose</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Compose</span></span><br><span class="line"><span class="comment">#用于将给定的多个方法的组合构造处理器</span></span><br><span class="line"><span class="comment">#每个输入值传递到第一个方法，然后将结果传递到第二个方法以此类推，最后一个方法的返回值作为输出</span></span><br><span class="line"><span class="comment">#如果需要当遇到None值时候停止处理，可以通过传递stop_on_one=False设定。</span></span><br><span class="line">proc = Compose(<span class="keyword">lambda</span> v:v[<span class="number">0</span>],<span class="built_in">str</span>.upper)</span><br><span class="line"><span class="built_in">print</span>(proc([<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;world&#x27;</span>]))</span><br></pre></td></tr></table></figure><h5 id="5-Join"><a href="#5-Join" class="headerlink" title="5. Join"></a>5. Join</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Join</span></span><br><span class="line"><span class="comment">#返回用分隔符separator连接后的值，separator默认为空格，不接受loader contexts/</span></span><br><span class="line">proc = Join()</span><br><span class="line"><span class="built_in">print</span>(proc([<span class="string">&#x27;one&#x27;</span>,<span class="string">&#x27;two&#x27;</span>,<span class="string">&#x27;three&#x27;</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="6-SelectJmes"><a href="#6-SelectJmes" class="headerlink" title="6.SelectJmes"></a>6.SelectJmes</h5><p>指定json_path查询并返回值，需要jmespath的支持，每次只接收一个输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#SelectJmes</span></span><br><span class="line"><span class="comment">#指定json_path查询并返回值，需要jmespath的支持</span></span><br><span class="line">proc = SelectJmes(<span class="string">&quot;foo&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(proc(&#123;<span class="string">&#x27;foo&#x27;</span>:<span class="string">&#x27;bar&#x27;</span>&#125;))</span><br></pre></td></tr></table></figure></p><blockquote><h4 id="3-Item-Pipeline模块"><a href="#3-Item-Pipeline模块" class="headerlink" title="3.Item Pipeline模块"></a><a id="two"></a>3.Item Pipeline模块</h4><p>完整的Item Pipeline的demo:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 完整的Item Pipeline</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    coolection_name = <span class="string">&#x27;scrapy_items&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,mongo_uri,mongo_db</span>):</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    <span class="comment">#crawler是一个Crawler对象</span></span><br><span class="line">    <span class="comment">#从Crawl属性中，创建一个pipeline示例，Crawler对象能够接触所有Scrapy的核心组件如seetings，singnals。</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls,crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">&#x27;MONGO_DATABASE&#x27;</span>,<span class="string">&#x27;items&#x27;</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 参数spider表示一个Spider对象，表示被开启的Spider</span></span><br><span class="line">    <span class="comment"># 当spider被开启的时候，这个方法会被调用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#当spider被关闭的时候，这个方法会被调用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">        self.db[self.coolection_name].insert(<span class="built_in">dict</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></blockquote><h3 id="3-请求与响应-1"><a href="#3-请求与响应-1" class="headerlink" title="3. 请求与响应"></a><a id="three"></a>3. 请求与响应</h3><blockquote><h4 id="1-Request对象"><a href="#1-Request对象" class="headerlink" title="1. Request对象"></a>1. Request对象</h4></blockquote><ol><li>Request对象</li></ol><p>Request对象相当于一个HTTP请求。通常在Spider产生，传递给下载器，最后返回一个响应。<br>Request(url[,callback,method=’GET’,headers,body,cookies,meta,encoding=’utf-8’,priority=0,dont_filter=False,errback])。<br>url:请求链接。callback:指定用于解析请求响应的方法，如果没有指定，默认使用spider的parse方法。<br>method:HTTP请求方式。body:请求的body。headers:请求头。cookies（dict or list）：请求的cookie信息。<br>encoding:请求的编码。priority:请求的优先级，默认为0。dont_filter:标明该请求不应由调度器过滤，适用于多次执行相同请求。<br>errback:如果请求出现异常，该方法将被调用。<br>meta的参数如下所示：<br><img src="/images/spider/request-meta.PNG" alt="31"><br>示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">request_with_cookies = scrapy.Request(url=<span class="string">&#x27;http://www.example.com&#x27;</span>,</span><br><span class="line">                                              cookies=&#123;<span class="string">&#x27;currency&#x27;</span>:<span class="string">&#x27;USD&#x27;</span>,<span class="string">&#x27;country&#x27;</span>:<span class="string">&#x27;UY&#x27;</span>&#125;)</span><br><span class="line"><span class="comment">#使用字典列表发送</span></span><br><span class="line">request_with_cookies = scrapy.Request(url=<span class="string">&#x27;http://www.example.com&#x27;</span>,</span><br><span class="line">                                     cookies=[&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;currency&#x27;</span>,<span class="string">&#x27;value&#x27;</span>:<span class="string">&#x27;USD&#x27;</span>,</span><br><span class="line">                                               <span class="string">&#x27;domain&#x27;</span>:<span class="string">&#x27;example.com&#x27;</span>,<span class="string">&#x27;path&#x27;</span>:<span class="string">&#x27;/currency&#x27;</span>&#125;])</span><br><span class="line"><span class="comment">#meta(dont_merge_cookies属性)可以用于当请求发送后，不将返回的cookie信息和现有cookie合并</span></span><br><span class="line"></span><br><span class="line">request_with_cookies = scrapy.Request(url=<span class="string">&#x27;http://www.example.com&#x27;</span>,</span><br><span class="line">                                     cookies=[&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;currency&#x27;</span>, <span class="string">&#x27;value&#x27;</span>: <span class="string">&#x27;USD&#x27;</span>,</span><br><span class="line">                                               <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;example.com&#x27;</span>, <span class="string">&#x27;path&#x27;</span>: <span class="string">&#x27;/currency&#x27;</span>&#125;],</span><br><span class="line">                                     meta=&#123;<span class="string">&#x27;dont_merge_cookies&#x27;</span>:<span class="literal">True</span>&#125;)</span><br></pre></td></tr></table></figure></p><ol><li>FormRequest</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#FromRequest提供了一个类方法from_response</span></span><br><span class="line"><span class="comment">#from_response(response[,formname=None,formnumber=0,formdata=None,formxpath=None,</span></span><br><span class="line"><span class="comment"># clickdata=None,dont_click=False])</span></span><br><span class="line"><span class="comment"># response: 一个包含HTML表单的响应页面，formname:表单的name属性，formnumber:用于指定第几个表单</span></span><br><span class="line"><span class="comment"># formdata(dict):用于填充表单中属性的值</span></span><br><span class="line"><span class="comment"># formxpath:通过xpath定位表单，第一个匹配的将会被操作</span></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">scrapy.FormRequest.from_response(</span><br><span class="line">   response,</span><br><span class="line">   formdata=&#123;<span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;john&#x27;</span>, <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;secret&#x27;</span>&#125;,</span><br><span class="line">   callback=self.after_login</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>使用实例如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    start_url = [<span class="string">&#x27;http://www.example.com/users/login.php&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> scrapy.FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata=&#123;<span class="string">&#x27;username&#x27;</span>:<span class="string">&#x27;john&#x27;</span>,<span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;secret&#x27;</span>&#125;,</span><br><span class="line">            callback=self.after_login</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_login</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;authentication failed&quot;</span> <span class="keyword">in</span> response.body:</span><br><span class="line">            self.logger.error(<span class="string">&quot;Login failed&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><ol><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-Spider模块&quot;&gt;&lt;a href=&quot;#1-Spider模块&quot; class=&quot;headerlink&quot; title=&quot;1.Spider模块&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#one&quot;&gt;1.Spider模块&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;2-Item-Loader&quot;
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python常见问题</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/wiki/程序技术/Python/常见问题/</id>
    <published>2021-12-08T06:36:41.000Z</published>
    <updated>2021-12-08T06:41:02.235Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Python：常遇见的字符编码问题-TypeError-a-bytes-like-object-is-required-not-‘str’"><a href="#Python：常遇见的字符编码问题-TypeError-a-bytes-like-object-is-required-not-‘str’" class="headerlink" title="Python：常遇见的字符编码问题 TypeError: a bytes-like object is required, not ‘str’"></a>Python：常遇见的字符编码问题 TypeError: a bytes-like object is required, not ‘str’</h3><p>需要将写入的数据进行编码转换(通过encode转化)。例如:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file.write(line.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Python：常遇见的字符编码问题-TypeError-a-bytes-like-object-is-required-not-‘str’&quot;&gt;&lt;a href=&quot;#Python：常遇见的字符编码问题-TypeError-a-bytes-like-object-is-
      
    
    </summary>
    
      <category term="Python" scheme="http://example.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>JavaWeb后端数据导出</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Java/JavaWeb/%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/"/>
    <id>http://example.com/wiki/程序技术/Java/JavaWeb/数据导出/</id>
    <published>2021-12-06T07:07:34.071Z</published>
    <updated>2021-12-06T09:11:19.936Z</updated>
    
    <content type="html"><![CDATA[<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>我们在进行页面开发的过程中，经常需要获取数据，以及下载数据。通常从页面下载数据有多种方式，可以生成PDF下载，可以生成Excel表格下载。<br>这里介绍这几种数据下载方式。</p><h6 id="PDF下载"><a href="#PDF下载" class="headerlink" title="PDF下载"></a>PDF下载</h6><h6 id="Excel下载"><a href="#Excel下载" class="headerlink" title="Excel下载"></a>Excel下载</h6><p>Java比较常用的Excel导入和导出技术有两种，Jakarta POI和Java Excel。Jakarta POI 是一套用于访问微软格式文档的Java API。Jakarta POI有不少组件组成，其中有用于操做Excel格式文件的HSSF和用于操做Word的HWPF，在各类组件中目前只有用于操做Excel的HSSF相对成熟。官方主页<a href="http://poi.apache.org/index.html，API文档http://poi.apache.org/apidocs/index.htmlapi。这里主要介绍Jakarta">http://poi.apache.org/index.html，API文档http://poi.apache.org/apidocs/index.htmlapi。这里主要介绍Jakarta</a> POI的用法。</p><p>首先是maven依赖<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;poi&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">3.17</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">3.17</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;p&gt;我们在进行页面开发的过程中，经常需要获取数据，以及下载数据。通常从页面下载数据有多种方式，可以生成PDF下载，可以生成Excel表格下载。&lt;
      
    
    </summary>
    
      <category term="JavaEE" scheme="http://example.com/categories/JavaEE/"/>
    
    
      <category term="Java" scheme="http://example.com/tags/Java/"/>
    
      <category term="JavaWeb" scheme="http://example.com/tags/JavaWeb/"/>
    
  </entry>
  
  <entry>
    <title>Java中对JSON的操作</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Java/JAVA-Json/"/>
    <id>http://example.com/wiki/程序技术/Java/JAVA-Json/</id>
    <published>2021-12-01T06:56:29.125Z</published>
    <updated>2021-12-01T07:00:13.269Z</updated>
    
    <content type="html"><![CDATA[<p>JSON格式的数据在Web开发中经常作为数据请求格式或数据响应格式，所以对JSON格式的数据的操作十分重要。</p><p>常见的JSON操作主要有以下几种方式，JSON对象转化JSON字符串，JSON字符串转化JSON对象，普通数据对象转换JSON字符串，普通数据对象转换JSON对象。</p><p>常见的JSON第三方库有fastjson,gson等。这里主要以fastjson作为介绍。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;JSON格式的数据在Web开发中经常作为数据请求格式或数据响应格式，所以对JSON格式的数据的操作十分重要。&lt;/p&gt;
&lt;p&gt;常见的JSON操作主要有以下几种方式，JSON对象转化JSON字符串，JSON字符串转化JSON对象，普通数据对象转换JSON字符串，普通数据对象转换
      
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Java" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>抖店开发--订单导出之信息获取</title>
    <link href="http://example.com/wiki/%E6%8A%96%E5%BA%97API%E5%BC%80%E5%8F%91/%E8%AE%A2%E5%8D%95%E5%AF%BC%E5%87%BA/"/>
    <id>http://example.com/wiki/抖店API开发/订单导出/</id>
    <published>2021-12-01T04:38:34.647Z</published>
    <updated>2021-12-01T05:05:30.709Z</updated>
    
    <content type="html"><![CDATA[<p>我们可以通过订单列表查询API获取到店铺的订单列表。根据需要获取的信息和抖店API查询到的结果，做如下表格，用于获取对应参数。响应格式如下所示：</p><p><img src="/images/响应.PNG" alt="a1"></p><div class="table-container"><table><thead><tr><th style="text-align:center">名称</th><th style="text-align:center">获取方式</th><th style="text-align:center">获取结果</th></tr></thead><tbody><tr><td style="text-align:center">主订单编号</td><td style="text-align:center">order_id</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">选购商品</td><td style="text-align:center">product_name</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">商品规格</td><td style="text-align:center">order_type_desc</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">订单应付金额</td><td style="text-align:center">order_amount</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">收件人</td><td style="text-align:center">encrypt_post_receiver</td><td style="text-align:center">加密</td></tr><tr><td style="text-align:center">收件人手机号</td><td style="text-align:center">encrypt_post_tel</td><td style="text-align:center">加密</td></tr><tr><td style="text-align:center">详细地址</td><td style="text-align:center">post_addr省，市，区县，街道可获取，详细地址（encrypt_detail）</td><td style="text-align:center">部分加密</td></tr><tr><td style="text-align:center">买家留言</td><td style="text-align:center">buyer_words</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">订单提交时间</td><td style="text-align:center">create_time</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">订单状态</td><td style="text-align:center">order_status</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">承若发货时间</td><td style="text-align:center">appointment_ship_time</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">商家备注</td><td style="text-align:center">seller_words</td><td style="text-align:center">可获取</td></tr><tr><td style="text-align:center">身份证姓名</td><td style="text-align:center">user_id_info.encrypt_id_card_name</td><td style="text-align:center">加密</td></tr><tr><td style="text-align:center">身份证号</td><td style="text-align:center">user_id_info.encrypt_id_card_no</td><td style="text-align:center">加密</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们可以通过订单列表查询API获取到店铺的订单列表。根据需要获取的信息和抖店API查询到的结果，做如下表格，用于获取对应参数。响应格式如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/响应.PNG&quot; alt=&quot;a1&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;table
      
    
    </summary>
    
      <category term="抖店" scheme="http://example.com/categories/%E6%8A%96%E5%BA%97/"/>
    
    
      <category term="JAVA" scheme="http://example.com/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/Untitled-1/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/Untitled-1/</id>
    <published>2021-11-26T01:21:47.277Z</published>
    <updated>2021-12-10T08:50:26.005Z</updated>
    
    <content type="html"><![CDATA[< div class = "v-image__image v-image__image--cover"style = "background-image: url(&quot;https://doge.zzzmh.cn/wallpaper/origin/a58cbbffd2aa49c1b1e99990be912f30.jpg/thumbs?auth_key=1642176000-d1e0a42b4ad44111e5c25911da43d23e-0-3cd3282f8ac059d4a3573b0104c631c3&quot;); background-position: center center;" > < /div><a href = "https://doge.zzzmh.cn/wallpaper/origin/a58cbbffd2aa49c1b1e99990be912f30.jpg?response-content-disposition=attachment&amp;auth_key=1642176000-d1e0a42b4ad44111e5c25911da43d23e-0-0c38988dbe7d8b1203a85814583d2315" > < div title = "下载"class = "tool" > < i aria - hidden = "true"class = "v-icon notranslate mdi mdi-download theme--light"style = "color: rgb(47, 146, 150); caret-color: rgb(47, 146, 150);" > < /i></div > < /a>]]></content>
    
    <summary type="html">
    
      
      
        &lt; div class = &quot;v-image__image v-image__image--cover&quot;
style = &quot;background-image: url(&amp;quot;https://doge.zzzmh.cn/wallpaper/origin/a58cbbffd2a
      
    
    </summary>
    
      <category term="程序技术" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/"/>
    
      <category term="Python" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>selenium+Chrome(79版本以上)反爬</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/selenium%E5%8F%8D%E7%88%AC/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/selenium反爬/</id>
    <published>2021-11-25T08:16:51.544Z</published>
    <updated>2021-11-25T09:06:37.133Z</updated>
    
    <content type="html"><![CDATA[<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><font size=2>&nbsp;&nbsp;&nbsp;最近在学习爬虫的过程中碰到一个奇怪的现象，当我在正常的浏览器页面访问网站的时候，能够正常访问到网页的数据。然而，当我通过selenium进行爬取网页数据的时候，出现服务器异常的提醒。正常访问能够访问，而通过selenium访问的时候却报错，这是为什么呢？通过查阅相关资料，可以得出，碰上反爬虫了。</font><blockquote><p>分析</p></blockquote><font size=2>为什么可以得出碰上反爬虫了呢？通常我们通过selenium进行爬取网页的时候，可以通过 window.navigator.webdriver检测是否使用了webdriver。我们可以试一下。在正常访问的网页中输入 window.navigator.webdriver，通常返回的是false或undifine，而当我们通过selenium访问浏览器的时候，在网页控制台输入 window.navigator.webdriver，会返回true。假如我是网页的设计者，我就会先在网页加载的时候写下这么一行代码 :</font><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="built_in">window</span>.navigator.webdriver)&#123;</span><br><span class="line">    alert(<span class="string">&quot;爬虫爬的好，牢饭吃到饱&quot;</span>)</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    alert(<span class="string">&quot;正常页面&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><font size=2>所以，当我们爬取网页出现上述情况的时候，很大可能浏览器存在反selenium。下面介绍一下如何解决反爬，常见的反反爬方案包含：设置参数 excludeSwitches、mitmproxy 拦截过滤、cdp 命令。</font><h3 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;要想解决上述的问题，我们可以让window.navigator.webdriver返回false即可。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_experimental_option(<span class="string">&#x27;excludeSwitches&#x27;</span>, [<span class="string">&#x27;enable-automation&#x27;</span>])</span><br><span class="line">driver = webdriver.Chrome(executable_path=<span class="string">&#x27;E:\codeEverment\python\chromedriver.exe&#x27;</span>, options=options) <span class="comment"># chrome驱动 </span></span><br><span class="line">                                 </span><br><span class="line"></span><br><span class="line">script = <span class="string">&quot;Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123; get: () =&gt; undefined&#125;)&quot;</span></span><br><span class="line">    </span><br><span class="line">driver.execute_cdp_cmd(<span class="string">&quot;Page.addScriptToEvaluateOnNewDocument&quot;</span>, &#123;<span class="string">&quot;source&quot;</span>: script&#125;)</span><br></pre></td></tr></table></figure><br>要见检查是否避免浏览器对webdriver的检测，可以通过selenium访问<a href="https://intoli.com/blog/not-possible-to-block-chrome-headless/chrome-headless-test.html">https://intoli.com/blog/not-possible-to-block-chrome-headless/chrome-headless-test.html</a>,如果页面显示全绿，那么就表明避免成功，反之失败。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h4&gt;&lt;font size=2&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;最近在学习爬虫的过程中碰到一个奇怪的现象，当我在正常的浏览器页面访问网站的时候，能够
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫-中级</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB-%E4%B8%AD%E7%BA%A7/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/Python爬虫-中级/</id>
    <published>2021-11-22T06:44:03.511Z</published>
    <updated>2021-12-09T09:02:28.525Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-数据存储"><a href="#1-数据存储" class="headerlink" title="1.数据存储"></a><a href="#one">1.数据存储</a></h3><h3 id="2-动态文件抓取"><a href="#2-动态文件抓取" class="headerlink" title="2.动态文件抓取"></a><a href="#two">2.动态文件抓取</a></h3><h3 id="3-Web端协议分析"><a href="#3-Web端协议分析" class="headerlink" title="3.Web端协议分析"></a><a href="#three">3.Web端协议分析</a></h3><h3 id="4-数据存储"><a href="#4-数据存储" class="headerlink" title="4.数据存储"></a><a href="#one">4.数据存储</a></h3><h3 id="1-数据存储-1"><a href="#1-数据存储-1" class="headerlink" title="1.数据存储"></a><a href="#one">1.数据存储</a></h3><h6 id="1-数据存储-2"><a href="#1-数据存储-2" class="headerlink" title="1. 数据存储"></a>1.<a id="one"/> 数据存储</h6><blockquote><p>前面介绍了关于数据存储的csv,txt,json方式，这里介绍如何采用数据库保存数据，主要是了解两个数据库，关系数据库和分布式数据库。<br>即MySQL和MongoDB。</p></blockquote><h6 id="1-1-MySQL"><a href="#1-1-MySQL" class="headerlink" title="1.1 MySQL"></a>1.1 MySQL</h6><blockquote><p>Python对MySQL的操作通过pymsql模块支持。<br>Python操作MySQL的代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Python对MySQL的操作主要是由pymysql模块进行支持。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 主机ip(host),用户名(user),密码(passwd),数据库名称(db),端口(port),编码(charset)</span></span><br><span class="line"><span class="comment"># 打开数据库， pymysql.connect(host=,user=,passwd=,db=,port=,charset=)</span></span><br><span class="line">db = pymysql.connect(</span><br><span class="line">    host=<span class="string">&#x27;localhost&#x27;</span>,</span><br><span class="line">    user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">    passwd=<span class="string">&#x27;123456&#x27;</span>,</span><br><span class="line">    db=<span class="string">&#x27;votemsy&#x27;</span>,</span><br><span class="line">    charset=<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库成功后就可以操作数据库</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个游标对象</span></span><br><span class="line"><span class="comment"># 游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果</span></span><br><span class="line"><span class="comment"># 游标对象支持的数据库的操作</span></span><br><span class="line">cursor = db.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment">#需要执行的sql语句</span></span><br><span class="line">sql = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment">#执行一条SQL语句</span></span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    <span class="comment">#执行多条SQL语句</span></span><br><span class="line">    cursor.executemany(sql)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#用来从结果中取一条记录，并将游标指向下一条记录</span></span><br><span class="line">    result = cursor.fetchone()</span><br><span class="line">    <span class="comment">#用来从结果中取多条记录</span></span><br><span class="line">    result = cursor.fetchmany(<span class="number">5</span>)</span><br><span class="line">    <span class="comment">#获取所有记录列表</span></span><br><span class="line">    results = cursor.fetchall()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 插入数据</span></span><br><span class="line">    data = <span class="string">&quot;&#x27;qiye&#x27;,20&quot;</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;INSERT INTO person (name,age) VALUES (%s)&#x27;</span>%data)</span><br><span class="line">    <span class="comment"># 插入数据，占位符法</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;INSERT INTO person(name,age) VALUES (%s,%s)&#x27;</span>,(<span class="string">&#x27;qiye&#x27;</span>,<span class="number">20</span>))</span><br><span class="line">    <span class="comment">#执行多条插入语句</span></span><br><span class="line">    cursor.executemany(<span class="string">&#x27;INSERT INTO person(name,age) values &#x27;</span>,[(<span class="string">&#x27;qiye&#x27;</span>,<span class="number">20</span>),(<span class="string">&#x27;jack&#x27;</span>,<span class="number">20</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#查询数据</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;SELECT * FROM person&#x27;</span>)</span><br><span class="line">    res = cursor.fetchall() <span class="comment">#获取所有结果</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> res:</span><br><span class="line">        <span class="built_in">print</span>(line)</span><br><span class="line">    cursor.execute(<span class="string">&#x27;SELECT * FROM person&#x27;</span>)</span><br><span class="line">    res = cursor.fetchone() <span class="comment">#只获取一个结果</span></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#修改和删除数据</span></span><br><span class="line">    cursor.execute(<span class="string">&#x27;UPDATE person SET name=%s WHERE id=%s&#x27;</span>,(<span class="string">&#x27;rose&#x27;</span>,<span class="number">1</span>))</span><br><span class="line">    cursor.execute(<span class="string">&#x27;DELETE FROM person where id=%s&#x27;</span>,(<span class="number">0</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用来事务提交，只有commit之后，才会提交到数据库进行一系列的操作</span></span><br><span class="line">    db.commit()</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e :</span><br><span class="line">    <span class="comment"># 由于在执行事务的过程中，出现错误，所以回滚，恢复原来的状态，不执行操作</span></span><br><span class="line">    db.rollback()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    cursor.close() <span class="comment">#关闭游标</span></span><br><span class="line">    db.close() <span class="comment">#关闭一个数据库连接</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></blockquote><h6 id="1-2-MongoDB"><a href="#1-2-MongoDB" class="headerlink" title="1.2 MongoDB"></a>1.2 MongoDB</h6><p>MongoDB是基于分布式文件存储的数据库，用于为Web应用提供可扩展的高性能数据存储解决方案。MongoDB属于非关系数据库。</p><p>MongoDB的基本概念是文档，集合，数据库。</p><p>MongoDB中的表通过collection替代，MongoDB中的行通过document替换。MongoDB中的列通过field替换。<br>MongoDB自动将_id字段设置为主键。</p><blockquote><p>文档：文档是MongoDB中数据的基本单元（即BSON）,类似于关系数据库中的行。文档具有唯一标识_id.数据库可以自动生成。文档以key/value形式。例如: {“name”:”qiye”,”age”:24}</p></blockquote><p>文档具有如下三个特性：</p><ol><li>文档的键值对是有序的，顺序不同文档亦不同。</li><li>文档的值可以是字符串，整数，数组以及文档等类型。</li><li>文档的键是用双引号标识的字符串。以——开头的键是保留的，建议不要使用。</li><li>文档区分大小写以及值类型</li></ol><blockquote><p>集合：集合也就是一组文档，类似于数据表。集合没有固定的结构，可以在集合中插入不同格式和类型的数据（和关系表的差异）。</p></blockquote><p>集合命名不能是空字符串，不能有’\0’字符，不能以system.开头，不要包含$。</p><blockquote><p>数据库：一个MongoDB可以创建多个数据库，默认数据库是db，数据库存储在data目录。MongoDB的单个实例可以容纳多个独立的数据库。</p></blockquote><p>MongoDB的数据类型如下所示：<br><img src="/images/数据类型.PNG" alt="a11"></p><blockquote><p>插入语法：db.集合.insert(JSON格式数据)</p><p>查询语法：db.集合.find()<br><img src="/images/条件查询.PNG" alt="a12"></p></blockquote><p>对于多条件查询(and和or).<br>and通过逗号隔开。例如：db.集合.find({“key1”:{条件1},”key2”:{条件2}})</p><p>or通$or来实现。例如：db.集合.find({<br>    $or:[<br>        {key1:value1},{key2:value2}<br>    ]<br>})</p><blockquote><p>更新文档：MongoDB通过update(),save()方法来更新集合中的文档。update: db.集合.update(<br>    query,<br>    update，{<br>        upsert:boolean,<br>        multi: boolean,<br>        writeConcern:document<br>    }<br>)<br>其中：query为update的查询条件，update:update的对象和一些更新的操作符等（类似于set后面的内容），upsert（可选，如果不存在update记录，是否插入新文档）, multi（可选，是否更新全部查找出来的记录），writeConcern(可选，异常抛出级别)。<br>eg:<br>db.python.update(<br>    {‘title’,’python’},{$set:{‘title’,’python爬’}}<br>)<br>db.python.update(<br>    {‘title’,’python’},{$set:{‘title’:’python爬’}},{multi:true}<br>)</p></blockquote><p>对于save()方法，通过传入的文档替换已有的文档。db.集合.save({<br>    document{<br>        writeConcern:document<br>    }<br>})</p><blockquote><p>删除文档：MongoDB提供了remove()方法来删除文档。</p></blockquote><p>db.集合.remove(<br>    query, #删除的文档的条件<br>    {<br>        justOne:boolean, # 如果设置为true，则只删除一个文档。<br>        writeConcern:document<br>    }<br>)</p><p>删除所有title等于mongodb的文档<br>db.python.remove({‘title’:’Mongodb’})<br>如果没有查询条件，就相当于删除所有的文档。<br>python操作mongodb代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. 建立连接</span></span><br><span class="line"><span class="comment">#pymogo模块使用MongoClient对象描述一个数据库客户端，创建对象的主要参数是host和port</span></span><br><span class="line"><span class="comment">#如下三种方式创建</span></span><br><span class="line">client = pymongo.MongoClient() <span class="comment"># 连接默认的主机IP和端口</span></span><br><span class="line"><span class="comment"># client = pymongo.MongoClient(&#x27;localhost&#x27;,27017) #显示指定IP和端口</span></span><br><span class="line"><span class="comment"># client = pymongo.MongoClient(&#x27;mongodb://localhost:27017/&#x27;) #采用URL格式进行连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 获取数据库,通过MongoClient的属性方式来访问数据库</span></span><br><span class="line">db = client.test <span class="comment">#方式一</span></span><br><span class="line"><span class="comment"># db = client[&#x27;pa-pers&#x27;] #方式二</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3. 获取一个集合</span></span><br><span class="line"><span class="comment"># collection = db.books #方式一</span></span><br><span class="line">collection = db[<span class="string">&#x27;test_one&#x27;</span>] <span class="comment">#方式二</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#插入文档操作</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;author&#x27;</span>:<span class="string">&#x27;mike&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;text&#x27;</span>:<span class="string">&#x27;My first book&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tags&#x27;</span>:[<span class="string">&quot;爬虫&quot;</span>,<span class="string">&quot;python&quot;</span>,<span class="string">&quot;网络&quot;</span>],</span><br><span class="line">    <span class="string">&#x27;date&#x27;</span>: datetime.datetime.utcnow()</span><br><span class="line">&#125;</span><br><span class="line">data_id = collection.insert_one(data) <span class="comment"># 插入一条语句,返回数据的_id值,如果文件内没有_id值，则会自动添加到一文件里</span></span><br><span class="line"><span class="comment"># data_id = collection.insert_many(data) # 插入多条,数据以列表形式[&#123;&#125;,&#123;&#125;]</span></span><br><span class="line"><span class="built_in">print</span>(data_id)</span><br><span class="line"><span class="comment"># 查询语句，find_one</span></span><br><span class="line"><span class="built_in">print</span>(collection.find_one(&#123;<span class="string">&#x27;author&#x27;</span>:<span class="string">&#x27;mike&#x27;</span>&#125;))</span><br><span class="line"><span class="comment"># 通过_id查询</span></span><br><span class="line"><span class="built_in">print</span>(collection.find_one(&#123;<span class="string">&#x27;_id&#x27;</span>: ObjectId(<span class="string">&#x27;619c4dc19c281df292e7e0dd&#x27;</span>)&#125;))</span><br><span class="line"><span class="comment"># 通过find可以查询多个符合条件的文档,并且可以在括号中加入限制条件，查询多个符合条件的文档</span></span><br><span class="line"><span class="keyword">for</span> book <span class="keyword">in</span> collection.find():</span><br><span class="line">    <span class="built_in">print</span>(book)</span><br><span class="line"><span class="comment"># 统计符合条件的数目</span></span><br><span class="line"><span class="built_in">print</span>(collection.count_documents(&#123;<span class="string">&#x27;author&#x27;</span>:<span class="string">&#x27;mike&#x27;</span>&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文档</span></span><br><span class="line">collection.update_one(&#123;<span class="string">&#x27;author&#x27;</span>:<span class="string">&#x27;mike&#x27;</span>&#125;,&#123;<span class="string">&quot;$set&quot;</span>:&#123;<span class="string">&quot;text&quot;</span>:<span class="string">&quot;python book&quot;</span>&#125;&#125;)</span><br><span class="line"><span class="comment">#删除文档</span></span><br><span class="line">collection.delete_one(&#123;<span class="string">&#x27;author&#x27;</span>:<span class="string">&#x27;mike&#x27;</span>&#125;) <span class="comment"># 如果要删除多个，delete_many</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h6 id="2-动态网站抓取"><a href="#2-动态网站抓取" class="headerlink" title="2. 动态网站抓取"></a>2. <a id="two"/>动态网站抓取</h6><p>动态网页主要涉及到的技术是Ajax（Asynchoronous JavaScript and XML）和动态Html。</p><blockquote><p>Ajax技术用于网页的局部刷新，不必刷新整个页面，只需要调整局部内容，达到想要的效果，用户体验得到提升。<br>AJAX使用SOAP,XML或者支持JSON的WebService接口，在客户端利用JavaScript处理来自服务器的响应。</p></blockquote><p>SOAP:简单对象访问协议是交换数据的一种协议规范，是一种轻量的、简单的、基于XML（标准通用标记语言下的一个子集）的协议，它被设计成在WEB上交换结构化的和固化的信息。</p><blockquote><p>动态html（DHTML，Dynamic Html）,由HTML+CSS+JavaScript。</p></blockquote><p>如何从动态html页面爬取数据？有如下两种方法：</p><blockquote><ol><li>直接从JavaScript中采集加载的数据</li></ol></blockquote><h6 id="爬取影评信息"><a href="#爬取影评信息" class="headerlink" title="爬取影评信息"></a>爬取影评信息</h6><p>网页地址(www.mitime.com)</p><blockquote><ol><li>直接采集浏览器中已经加载的数据</li></ol></blockquote><p>对于直接加载渲染后的页面，可以通过PhantomJS,Selenium进行爬取。PhantomJS是基于WebKit的服务端JavaScript API，全面支持Web而无需浏览器支持，运行快，支持各种Web标准，DOM处理，CSS选择器，JSON，Cancas和SVG。PhantomJS可以用于网页自动化，网络检测，网页截屏，无界面测试等。可以把PhantomJS看成一个无界面的浏览器。</p><p>Selenium: Selenium是一个自动化测试工具，支持各种浏览器，Selenium支持浏览器驱动，可以对浏览器进行控制。</p><p>Selenium可以说是网页爬取的大杀器，可以直接模拟操作浏览器页面。下面介绍关于Selenium的使用方法。</p><blockquote><ol><li>安装配置</li></ol></blockquote><p>对于Selenium的安装配置教程可以自行百度。这里我采用的是Firefox,所以只需要两步，1. 下载selenium,通过pip指令就行。2. 下载驱动器geckodriver。通过如下代码即可使用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># executable_path就是下载的geckodriver所在的文件路径</span></span><br><span class="line">driver  = webdriver.Firefox(executable_path=<span class="string">&#x27;E:\codeEverment\python\geckodriver\geckodriver-v0.14.0-win64\geckodriver.exe&#x27;</span>)</span><br><span class="line">driver.get(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure></p><blockquote><ol><li>元素查找</li></ol></blockquote><p>selenium的元素定位方法如下图所示：<br><img src="/images/定位方法.PNG" alt="a22"></p><blockquote><ol><li>页面操作</li></ol></blockquote><p>如何给表单填写内容？我们可以定位到表单元素，然后通过元素.send_keys填入内容。找到按钮或链接通过元素.click()模拟点击事件。如果要清除填入的内容，通过元素.clear()可以清除内容。</p><p>对于下拉选项，可以通过WebDriver提供的一个叫Select方法进行选择。</p><p>对于元素拖拽，首先要找到源元素和目的元素，然后用ActionChains类可以实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">driver  = webdriver.Firefox(executable_path=<span class="string">&#x27;E:\codeEverment\python\geckodriver\geckodriver-v0.14.0-win64\geckodriver.exe&#x27;</span>)</span><br><span class="line">driver.get(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(driver.title)</span><br><span class="line"><span class="keyword">assert</span> <span class="string">u&#x27;百度&#x27;</span> <span class="keyword">in</span> driver.title</span><br><span class="line">elem = driver.find_element_by_name(<span class="string">&#x27;wd&#x27;</span>)</span><br><span class="line">elem.clear()</span><br><span class="line">elem.send_keys(<span class="string">u&#x27;网络爬虫&#x27;</span>) <span class="comment"># 给控件填写内容</span></span><br><span class="line">elem.send_keys(Keys.RETURN) <span class="comment">#这里是回车按钮</span></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">driver.close()</span><br><span class="line"><span class="comment"># 执行js代码</span></span><br><span class="line"><span class="comment"># 将页面拉到最低端</span></span><br><span class="line">driver.execute_script(<span class="string">&quot;window.scrollTo(0,document.body.scrollHeight);&quot;</span>)</span><br></pre></td></tr></table></figure><p>显示等待的API：<br><img src="/images/显示等待.PNG" alt="a24"></p><h6 id="3-Web端协议分析-1"><a href="#3-Web端协议分析-1" class="headerlink" title="3. Web端协议分析"></a>3.<a id="three"></a> Web端协议分析</h6><p>这里主要是关于网页登录POST分析，和验证码的解决方法。一般通过form表单填写账号密码，然后进行获取更多有效数据。</p><blockquote><p>通过POST请求登录</p></blockquote><p>一般我们都会通过构造表单数据，进行post请求。但是通常我们不仅仅提交的是账号，密码，还需要分析页面的具体提交数据，’<br>然后分析对应的数据，进行构建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这里主要演示了，如何通过session构建表单，然后进行表单提交。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_xsrf</span>(<span class="params">session</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    _xsrf是一个动态参数从网页中获取</span></span><br><span class="line"><span class="string">    :param Session:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    index_url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">    index_page = session.get(index_url)</span><br><span class="line">    html = index_page.text</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">session = requests.session()</span><br><span class="line">_xsrf = get_xsrf(session)</span><br><span class="line">post_url = <span class="string">&#x27;htttp://www.zhihu.com/login/phone_num&#x27;</span></span><br><span class="line">postdata = &#123; <span class="comment"># 构造post参数，这需要分析登录过程传递的参数，然后进行构建，通常不仅仅包含</span></span><br><span class="line">    <span class="comment">#账号密码选项，还有许多附加项</span></span><br><span class="line">    <span class="string">&#x27;_xsrf&#x27;</span>:_xsrf,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;xxxxxxx&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;phone_no&#x27;</span>:<span class="string">&#x27;xxxxxxx&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;remember_me&#x27;</span>:<span class="string">&#x27;true&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">login_page = session.post(post_url,data=postdata)</span><br><span class="line">login_code = login_page.text</span><br><span class="line"><span class="built_in">print</span>(login_page.status_code)</span><br><span class="line"><span class="built_in">print</span>(login_code)</span><br></pre></td></tr></table></figure><blockquote><p>加密数据分析</p></blockquote><p>通常在网页传输的数据都会进行数据加密，然后添加一系列附加的参数到POST请求中，而且还有验证码。<br>所以这时候需要对网页进行分析。</p><blockquote><ol><li>监听网络数据，分析传送参数</li></ol></blockquote><p>通常这一过程，我们会反复登录，然后记录传送的参数以及cookie中值的变化。</p><blockquote><ol><li>分析参数的获取方式</li></ol></blockquote><p>当我们完成传送参数的分析过程的时候，就需要进一步解析参数的生成方法，然后在程序中进行生成参数，然后构建data,进行post请求。通过我们获取参数的方法有两种。一是根据网络请求分析，查看是否有参数通过API进行获取，例如有的网页的Token，Public_key等是通过API进行获取，通常我们可以在网络请求中查看。二是根据JS文件获取生成方式，通常通过API获取参数还是需要通过JS获取API需要传递的参数，通常我们从JS中获取参数是通过网页文件中搜索参数名，然后从众多包含参数名的JS文件中，分析出参数在哪个JS文件中生成，并提取出相应的生成方法。</p><p>实例代码如下(以百度网盘为例)：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">分析百度网盘的登录：</span></span><br><span class="line"><span class="string">https://passport.baidu.com/v2/api/?login  #账号验证链接</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">staticpage: https://pan.baidu.com/res/static/thirdparty/pass_v3_jump.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">charset: UTF-8</span></span><br><span class="line"><span class="string">tpl: netdisk</span></span><br><span class="line"><span class="string">subpro: netdisk_web</span></span><br><span class="line"><span class="string">apiver: v3</span></span><br><span class="line"><span class="string">codestring:</span></span><br><span class="line"><span class="string">safeflg: 0</span></span><br><span class="line"><span class="string">u: https://pan.baidu.com/disk/home</span></span><br><span class="line"><span class="string">isPhone:</span></span><br><span class="line"><span class="string">quick_user: 0</span></span><br><span class="line"><span class="string">logintype: basicLogin</span></span><br><span class="line"><span class="string">logLoginType: pc_loginBasic</span></span><br><span class="line"><span class="string">idc:</span></span><br><span class="line"><span class="string">loginmerge: true</span></span><br><span class="line"><span class="string">crypttype: 12</span></span><br><span class="line"><span class="string">mkey:</span></span><br><span class="line"><span class="string">countrycode:</span></span><br><span class="line"><span class="string">fp_uid:</span></span><br><span class="line"><span class="string">fp_info:</span></span><br><span class="line"><span class="string">loginversion: v4</span></span><br><span class="line"><span class="string">supportdv: 1</span></span><br><span class="line"><span class="string">mem_pass: on</span></span><br><span class="line"><span class="string">detect: 1</span></span><br><span class="line"><span class="string">alg: v3</span></span><br><span class="line"><span class="string">tt: 时间戳</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">POST1：</span></span><br><span class="line"><span class="string">gid: A7A60F7-2233-48D5-9CD2-B51EB721ADDF</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">username: 2891112980@qq.com</span></span><br><span class="line"><span class="string">password: OcYb7XL/QONk5nADRh+a8cPYNbCvxlMxmncxXomf94YxHenzYWxVJUnM60bNhLt6mYmL+oXC4QvAVw9ujoREPP/YrFDvJNI87jgBwE6doPvn8/9+H8gATohKY68SaIoF+G9tMh/9VIOk5OuwrmpwzvZNQAY/V7gPAjCGmTKWPu4=</span></span><br><span class="line"><span class="string">rsakey: F6qaltAhSGSTrr3LYZoF0fDJkX11XTI6</span></span><br><span class="line"><span class="string">ppui_logintime: 33785</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># ds通过viewlog可以获取</span></span><br><span class="line"><span class="string">#https://passport.baidu.com/viewlog?callback=jQuery11020116633250707733_1637907449065&amp;ak=1e3f2dd1c81f2075171a547893391274&amp;as=b052f9ad&amp;fs=aJ2zH%2BxmPIQryyPrpIPqtXmH72tk2XARV%2F09o05kx%2FNTGdkoP%2Bzms8NSTm0jdEzP%2BhyPK1KdjXobjyUVxQ9OAuiZZmmEtQpubsfgTL8NYZWy5Sj0%2Bp5a20bTVZFZ70YbQcqZadFjrJ3ompkIg8E%2BYKbzQiEl0HRdRM6o429%2BhEeW3SwAMvEl04XHGbic4703ZlUREQzEeQ5v4b840qnfHziBVVGS9jihhHliV1iPFo6%2Brdl0G1YGREEm%2FvCvr3H15pDBP346ucrwUvqF23DN2WxkjdG68I%2FlW8rMEoTmx2sRpgiVDW4FwR7GYgOinomA7XKjKIV44rACzwS5Cd53wrP%2FCxFhAH91X%2BlFr%2F5Nye6OxX7u7B5FeVe4OnMt%2BrE0cYUqWLzqAls1loOLNoKaGNKpxIjOup%2BA%2B4sulrz1K89itYMKw7MQeNERfCI32hPmJ%2FSAJz36rVBtWO1aMUq66Vu7D8QrNIQ6Ko2L1vq3UQGqO0PRKSObWKZ4leHIf7wzJAMMD2K5cIVq1iLD2k7u5RmP8TI8EYsbxA0V4X8rwdAjAnFOgE4ch%2ByNyJfCeB0FPWs6VPPb9Opd37f6ofMcxWxkjdG68I%2FlW8rMEoTmx2smMDXKV2I%2F%2FSoxAYPYNoZT7XKjKIV44rACzwS5Cd53wuq17oB3hRDHI%2F5FD8ApIUyOxX7u7B5FeVe4OnMt%2BrE0pO2CUHf3wzY2O1nRsJt429KpxIjOup%2BA%2B4sulrz1K8%2Fgsgko9%2FybfRXZevgkJ2FTJ%2FSAJz36rVBtWO1aMUq66TWu3mrKMIRru7O6yI6%2B2TDqQXhdGcWeeWbzEILv75p3aopf9L%2Fmg3TG32fE46ACdLe9EtBCfLdqwpsieMHokZYjAnFOgE4ch%2ByNyJfCeB0FTmTqlzbiE0dSKyXoaNVBrXPmvUO8LF0TrlU912rIKlNqHIWEzcVbD5iGbnqKODdJh2NjhpbaJhRLDM6hXKbFY1SOq8SbKq03JqsbsJTIds6txmXY3cvIjmhaLph06iufE2oUGZsm4FwM9L7uTYsCXEhwGeYx1aILjYgChhgGUkTgVEgyn9h0l5p8hDZ7MKqOqYtXA1fmKQ0NEKMmzZd8eYFcgQPNQxncOm63JKnGItlgXNqFT31JBhv5lC7ybrYm5BE4sZtiEjgFwbXnaseNyqG1B2V5SJLYgUj69M46RJ09vxsCO54v7u3N3p1RjZlHwgU0OC16voeLsV3dm54Q2686dcWl%2BMBcvN6nLD3oN4wZ0sE582gll4wCKKEU3HrI&amp;tk=4006KvNubQMyUKPA8V02ndLZ4ZdO%2BQjmAT7GxVnjL4Rwk6ewi356PNOQKePl0ypYx1c7Dpi5RQawPXeIhjtrwJmhbK%2B7%2FP98piTEVpjpMwlZ0Es%3D&amp;_=1637907449068</span></span><br><span class="line"><span class="string">ds: II4Fg+RtgS2K3+RJnfEduZufjjdwD4QhYQhW04uf/VRDc+fFv1h7ViPEKzfVS15/YEt/aPddzytkFKhGf1Uo/XJm6pthogRlVaD8YeC68vvNq44fKPFACQkFXnHXZ8IGNQ8yrxON2Up0DT3jOOlmAxcKlI5PS3IaOp/jshcyHK+rJaRTTxZ4IXSCKZA+yQAEcAjLtuNMmB1RjOGr5SoG6fPAF+/5zUe4JogUCHg4IFDAaLsFP9Nev6IkG0mnTXJlwdIJSe6ZaTCOplnmGC0mouUychSC9/KIyV2TJiN7kJCUQKyqbp3VAbpz+e9xfaFeqtyPv+wOE1buN6QpyyJ8NvJ4edqWVwch1ebshP8gVPM5tNlcd/YWPT2n6VFtxn5ejme5wI1wDnpCBtbXY3C0+hIJaqJ9M94QNQvK4Swn6kGkrLKCgBbkaOfOdnfdMXmUrIOkIn8vCcbx8htOM9d5kNn8DmHNt4rtCcXphfQSNvf4didhsiAzwUU0ZQ1G53CTs7xMe9NqmzIRYgs/OVr4WIqK7r5cRjuBtcWtZAGKqrOWEO9QokQM225kP47RB25Vap6P4obUOTl88p4eNEGefEhqfDr7ZBqZ8p65Ht76ndP1+q8N6dpCh7CR5hv96mkIw1jfYWlQaZvgjNF/3uxqDRSnLLf9LGJ6O2qdR6IdN3XHB5DGmpdnOiu9PWA6J1qSAfeZOiPBXIsoDM+5yTqMwthRIWZK7zJ4B2vfQrqcqYgW3C9w6KglvuIEaNBZBnjrA/QuA1PU8ZMCijyrRN+3jnS1VX6lBoxVFG9oQnRLdyTaNFQbcq1o6zdcRh/zCQMcBSvZK8tKboXFXDK6oN0zYaSBdx3rJVzzEdkAL1gA8t0nqYsFbdQIQjeFi5RjIrjvDknLZ82xw0zoid7DyZ7qYTbIGxjdYK0QdKXH/u6GM446LCkG2wnSgTEHtBWSJIacz3haMH0277hPZeyGtjstPhqrm3v6wFYqSqa0feecGEGXUlFkq1vfEb/q6CQUEoRzZREiW6pEVnFUIzNG/8mqfISDRwYFqTOjNn5HAEho8qf0G9G3zi6FZgGQ8Jq4HJ9F6I4QEagJYde/A5KmPqZNgSxJc/j/vXzQ7I565b/1F6A=</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># tk 通过viewlog可以获取</span></span><br><span class="line"><span class="string">tk: 3806qsn4na7kF+g6FX58o+wfHp0rkicKoGxH7zGNq7/Nc12RpVbL51wDca2EsymyAXLwDS2oDrrP90vY/QvHD4ykyS18NmHsVx9q95E2iL5h0T4=</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">dv: tk0.53300068179592071637896306454@ssr0MBvDUOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COtyEyvJzgnkEOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COtyUitizgnkCOnmgCECw2C3ptJldEClhFtpdFQJtVI3wWM6COvkrgn2zwtHzzupohD3Q9EClFJlhDCpX~JldlI-AHDRpKG9zjny7gFk3iukqOArpvol8hDAdFCptEJyoFJiAyGJ8vB6lVukU-nSAXtr0DBvD7gukn~t9zyvkrlupohD3Q9EClFJlhDCpX~JldyQ68KLJoXvDUzukn-vmz~tkIlupohD3Q9EClFJlhDCpX~JldyQ68KLJoXnDq-nHzytDIOtDrgnmgCECw2C3ptJldEClhFtpdFIiA1M6V~FkrznkrOnyB~ukC~nSrOArpvol8hDAdFCptEJyoFJitlBRleQ2zwtSIOny7yukClvDUOArpvol8hDAdFCptEJyoFJitlBRleQpQHBJhzGJ8XBggLzgFHhpZWhFrhn9zHuk3-CrYQ5OzuSCynyqznkBjnDIwtD3HnkIgtSnivk3-nyq-tkC~hrYL2o~I2nbuHdzB6jNBRpeG2CNB-dKuHglMRoVGRVNG6E_ArnnmzzukrHvkqOtyr-ukrlnyBOvkB~ukrlnyBOnDCyt1zjnSE_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fuid: FOCoIC3q5fKa8fgJnwzbE67EJ49BGJeplOzf+4l4EOvDuu2RXBRv6R3A1AZMa49I27C0gDDLrJyxcIIeAeEhD8JYsoLTpBiaCXhLqvzbzmvy3SeAW17tKgNq/Xx+RgOdb8TWCFe62MVrDTY6lMf2GrfqL8c87KLF2qFER3obJGn1imUD9LtLDAxtUQVt00ywrFuY99+6ElzcETNlw/C1xZYD5SLH8W4d36DzPMGV4JvslIL5jj8EphieYalb6sMBbKVYW5I2LDDIlDR71IUk1mpk3Y5oUBZH/dZ3rKHJ0IrTnF2Wvu65NpvARErKLH28zZNnk8BWIbmbBo8AkexVQD4TDTFi/kJX2nM5fjwUAQ7erld8wjGrZonj8gE2rhKeVRKCfkBUbih20ajKSj+uMVvFor2HqteRD2yQbHfKX9I9NLYjYxH1YiKtJ/LZAt+jDVAzZK7r2I/XoTgOvpK/GRXi7OMMoCL+ptOvZYbHZs2EmNX6yJZWrVbkhpE/nUJFiBoWKq2H4RxmaiTcdrlpoJQCEBGWNtTsmA4KXyjd88UZ+jjahyvNzJARDVgTFkgMHmv/tVUjVExnyBvjSrtrPoNvLDs45y1J1BPSrpDADEz2WH0+/bGxWLp7WNEcuhl9zrEQCVXVz1ypT0UIooITdod7rWTX1LABi9llmt7bm09SsvyWN9Io4FIQt/daeMGulvDHd3RTujFKqjbyri7OLazNV00oTLvGOs+5nIyBMdEkYaTFoTKPDDVtdCO/a2IhGao+HBbcA3Sf1GWmuogDLSCOxH8nX4hJuc0/kKmVlWYouh6qHeKBM7Z7nT+yF0MXReV/blmoigtQ9A6GiwCQZffiSKUOp1VsS+Ly9+Iupn8y+IS13BgMJv25Rgmo9qqpVhLAbfjO5WTzIDI7m0pWOYtgRQxRCPxURvzJkLg7aW9Jm4l4Bb/STxNSv23Ru6jHCU7CX0YAw0dSweJL1vvynMwJmCcF9FlItQPQ5C9mG3VH8xRyN02ywrifxP8IF10Hg3b+Vi67h2TKPZAaOJzerzLANGXGTGd0Djy9B9kQWIoYRDDc4ujl2xJR5AN3q8OeLV68X8zvK+Yv4VQDA+ZZkVu82rr07bPfAFb6iDElQPL0gOE5uGejgPswFkbgH6k2WEZafSOKRDnPOYABKWeLHdXZe2bA0DO+1FlR8qb+PmiFcZVouo5DCvlQhjRhvpItjUh2/yNfyHjnbZ1A/hRRI7BFVDtu1KaJzjx51nTN1+yRNkC71fasq+HI3zFAuiUsfI5v5JSd2kRNZE7s6bQrA5yMI31SfUDgxDrsd6lPtUU=</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">traceid: F3DFF901</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">callback: parent.bd__pcbs__7pdpae</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">time: 1637896340</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sig: NHhDWmhlSkpXWlpoVlpTb1hZajZwaXhyYlRtSmVUM1lCRzBXa1dhOFRPWmtMZnRHd2Vmd29rdTZlUzhHKzl6eA==</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">elapsed: 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">shaOne: 00b653720afbeae6ff66615907b0e59e85a35757</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">rinfo: &#123;&quot;fuid&quot;:&quot;1d5264f920930df8d682b5cbac99c9e5&quot;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">POST2：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#通过gid获取token</span></span><br><span class="line"><span class="string">#https://passport.baidu.com/v2/api/?getapi&amp;token=&amp;tpl=netdisk&amp;subpro=netdisk_web&amp;apiver=v3&amp;tt=1637908334880&amp;class=login&amp;gid=4D970C9-672F-4002-BCD8-D71F629D9C8A&amp;loginversion=v4&amp;logintype=basicLogin&amp;traceid=&amp;time=1637908335&amp;alg=v3&amp;sig=ZHpJazJocTlTeHNMNG5LUlZjdE8yTXNLMUNjMGk3QmVTUzdDajF2Y2Q0M3pjSDBNL1k4ZHN1VHJzY0g0QWdrbw%3D%3D&amp;elapsed=10&amp;shaOne=00c2bc34cb293b92b355a01af5b33dcb0c7c0e19&amp;callback=bd__cbs__elghk8</span></span><br><span class="line"><span class="string">token: 134400c212148a1ed97cd3e14a64dec4</span></span><br><span class="line"><span class="string">#</span></span><br><span class="line"><span class="string">#通过loginv4_tangram_c32acce.js可以 看到gid的生成方法</span></span><br><span class="line"><span class="string"># 通过F12查找可以得到gid的生成方法，如下所示</span></span><br><span class="line"><span class="string"># &quot;xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&quot;.replace(/[xy]/g, function(e) &#123;</span></span><br><span class="line"><span class="string">#                     var t = 16 * Math.random() | 0,</span></span><br><span class="line"><span class="string">#                         n = &quot;x&quot; === e ? t : 3 &amp; t | 8;</span></span><br><span class="line"><span class="string">#                     return n.toString(16)</span></span><br><span class="line"><span class="string">#                 &#125;).toUpperCase()</span></span><br><span class="line"><span class="string">gid: 14893FE-D2FB-4BBC-9D6F-353A905BD43D</span></span><br><span class="line"><span class="string">username: 18370446979</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#可以通过rsa加密</span></span><br><span class="line"><span class="string">#通过F12找到password的生成方法</span></span><br><span class="line"><span class="string"># password: function() &#123; e._getRSA(function(t) &#123; e.RSA = t.RSA, e.rsakey = t.rsakey &#125;) &#125;</span></span><br><span class="line"><span class="string">password: qnmL7K9NK7pvaHhlmV/FVFjJEJsUUCmPs+aRr0jWLPSe9y033E9268hTdWCZoGGSz3fV2Q5BN7szAXd8vpH/I4BzNI0Jd72MXb1pkNEXeUnuxEy7xDPGshCWThTIID2PcCydW0dWrs/GrVjTsiwKFrOuig8uWSeJ5F5RVS2ioMc=</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">rsakey: kC02XvnT7I6uMux9cYbgVbsVqcat7gCK</span></span><br><span class="line"><span class="string">#在loginv4_tangram_c32acce.js文件可以找到ppui的生成方法，没啥影响，可以用相同的ppui</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ppui_logintime: 59338</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ds: DJv1I2ZMYFt4VNJGrEieRzX+fTxEGqp5KEKwP82Eqs0OwC/0nDO0AaGbSnOx4gLBgEwFUlmknWtM8w70bR6651jBTWR3RrN6M8athvLv0qjIh/uqJXSiudqxzvNbI8JrbRQi/L6z0yapH8OzFzgtjkCbQnKIIseJ5YN7pXQNzzfsLmwRSboSxSIMh7H+mNWlm7Jv6bipWb2Ef+mULPz+HJcsYAuYX7ZSRej5qKjg4LilTqk77vHywOzTdYX43QwjBa95tTsSQl3UdoyTyTSoP3+MI39NMnpfvc31ubL1aYBzIwtGTKBGQ6r9IMPO1/apOChCY8Ukvh5YsbuEPVZhHNrWGbwQxmPZGfwnHLxk+a4fe8Swz2GQYZhJWXhGX1zomphghvSfI0Xr0C3ofNTFL5YQ71CiRAzbbmQ/jtEHblVptfsMcO06PhUuRCioIIrvPK3f5pSnKweaOreL/stiDNRDmTc/TFc/zLbPufGyD5xEZm3ptEBPg9JO1kZVGX9hEjqGlJPTvnmgyiTZbqqYMGuU+teFEfSezySS7QbzHrNdJWklVjPbX4MaXF0vQwV3QvJ2BYaIJXbUbmi2N+MCb6uuhcCUzRXcvQcCFN+nCi8JV0rZbi8qRA6qogqITSmcoX+C2ebWRCqd1Bta5oZQDFnlNY8P1//P9TI67UuT8f3rpPwKHtCV94QXnYOPyXhxoFShURm0wEXyWfe6k74eCKJzF9WHKgqjvWNBU4oxh51ptfsMcO06PhUuRCioIIrvRnbBncAl/l2oYpW1Tl+s9vX6rw3p2kKHsJHmG/3qaQjDWN9haVBpm+CM0X/e7GoNFKcst/0sYno7ap1Hoh03dccHkMaal2c6K709YDonWpJOuWUh5Btk18pp2ofQaQue++3sNK8B3l22xvtu69xAcBbcL3DoqCW+4gRo0FkGeOsD9C4DU9TxkwKKPKtE37eOdLVVfqUGjFUUb2hCdEt3JNo0VBtyrWjrN1xGH/MJAxwFK9kry0puhcVcMrqg3TNhpIF3HeslXPMR2QAvWADy3SepiwVt1AhCN4WLlGMiuO/EHjpcfKHlM/uGqUpTxlzFg6wRAvrcujI33KeY139x4zosKQbbCdKBMQe0FZIkhpybzWnTPAJbZ2HlksG4LzZfZZxOv3Rz8OzebBbGZY2cKHzOIWo49PxA+Q/fgOWSMCg+0HmDAKY5njRPYRTBGiWmrU3+ErtPH77j3x0d0oaItNhgp6PrWJ92dgGBZTxabg4LrS+iGvesD1QIbG1EpOUu6EGco5bp1s9+m48R0KveFA==</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tk: 5886+r0RsngU7eH85qinjvKq/Ov8cTqi3758E9sFclzWzRrsTzYvBtjqRuohenHcu9EkNtVwd1ZSl1bUdta7LJyUF+N0t3lElPs4Qk2Y4E3v6mM=</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">dv: tk0.076573147928699071637897079952@ssm0MCo4JL8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGCLvj6ZvT-c8k6L8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGCLvjKeo9-yoH--0tDh4SRH6CXU9Xh4CtlZ9XwXJiAe4atYFH-~ok6cUk2-8e--0tDh4SRH6CXU9Xh4CtlZ9XwXJiAe4atYFH-x8k6yUkSi0kqLA2toDXph4AwUCtv69jDU9~AjF9poKGX30k7eo4hltm0otok7-0k8~ve-Xv12e0tDh4SRH6CXU9Xh4CtlZ9XwjRGpYI9Dlo46e0k8Xve-Xvj8Z0tDh4SRH6CXU9Xh4CtlZ9XwjRGpYI9Dlo4KX0k5x8e-Xok6y0tDh4SRH6CXU9Xh4CtlZ9XwaO~pYUkSi8e-e8j8Lv16e8mcC6CyrCStv9Xw6CXhUvtwUR9v3JSyEOGAlokKe0kSX0k2e812e0r-_CllI~lI~vtMEhImi8H-e0kSiBmpRsL-01q~v1C~8j2ZvjSeokKyo4q~84Kjvj7yvjq~o4SX87__imVIrDZJr8P0ew-KGxgKatWFrCgKiwY0ecXOaD3Fa3gFG6_zmp8m--0k2ev18Lvj2i0k2X8jKLokKZ0k2X8jKL84CjvB-x816_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fuid: FOCoIC3q5fKa8fgJnwzbE67EJ49BGJeplOzf+4l4EOvDuu2RXBRv6R3A1AZMa49I27C0gDDLrJyxcIIeAeEhD8JYsoLTpBiaCXhLqvzbzmvy3SeAW17tKgNq/Xx+RgOdb8TWCFe62MVrDTY6lMf2GrfqL8c87KLF2qFER3obJGn1imUD9LtLDAxtUQVt00ywrFuY99+6ElzcETNlw/C1xZYD5SLH8W4d36DzPMGV4JvslIL5jj8EphieYalb6sMBbKVYW5I2LDDIlDR71IUk1mpk3Y5oUBZH/dZ3rKHJ0IrTnF2Wvu65NpvARErKLH28zZNnk8BWIbmbBo8AkexVQD4TDTFi/kJX2nM5fjwUAQ7erld8wjGrZonj8gE2rhKeVRKCfkBUbih20ajKSj+uMVvFor2HqteRD2yQbHfKX9I9NLYjYxH1YiKtJ/LZAt+jDVAzZK7r2I/XoTgOvpK/GRXi7OMMoCL+ptOvZYbHZs2EmNX6yJZWrVbkhpE/nUJFiBoWKq2H4RxmaiTcdrlpoJQCEBGWNtTsmA4KXyjd88UZ+jjahyvNzJARDVgTFkgMHmv/tVUjVExnyBvjSrtrPoNvLDs45y1J1BPSrpDADEz2WH0+/bGxWLp7WNEcuhl9zrEQCVXVz1ypT0UIooITdod7rWTX1LABi9llmt7bm09SsvyWN9Io4FIQt/daeMGulvDHd3RTujFKqjbyri7OLazNV00oTLvGOs+5nIyBMdEkYaTFoTKPDDVtdCO/a2IhGao+HBbcA3Sf1GWmuogDLSCOxH8nX4hJuc0/kKmVlWYouh6qHeKBM7Z7nT+yF0MXReV/blmoigtQ9A6GiwCQZffiSKUOp1VsS+Ly9+Iupn8y+IS13BgMJv25Rgmo9qqpVhLAbfjO5WTzIDI7m0pWOYtgRQxRCPxURvzJkLg7aW9Jm4l4Bb/STxNSv23Ru6jHCU7CX0YAw0dSweJL1vvynMwJmCcF9FlItQPQ5C9mG3VH8xRyN02ywrifxP8IF10Hg3b+Vi67h2TKPZAaOJzerzLANGXGTGd0Djy9B9kQWIoYRDDc4ujl2xJR5AN3q8OeLV68X8zvK+Yv4VQDA+ZZkVu82rr07bPfAFb6iDElQPL0gOE5uGejgPswFkbgH6k2WEZafSOKRDnPOYABKWeLHdXZe2bA0DO+1FlR8qb+PmiFcZVouo5DCvlQhjRhvpItjUh2/yNfyHjnbZ1A/hRRI7BFVDtu1KaJzjx51nTN1+yRNkC71fasq+HI3zFAuiUsfI5v5JSd2kRNZE7s6bQrA5yMI31SfUDgxDrsd6lPtUU=</span></span><br><span class="line"><span class="string">traceid: B67AC501</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#baidu.phoenix._setIconsStatus</span></span><br><span class="line"><span class="string">#只是标识，不参与校验</span></span><br><span class="line"><span class="string">&quot;bd__cbs__&quot; + Math.floor(Math.random() * 2147483648).toString(36)</span></span><br><span class="line"><span class="string">callback: parent.bd__pcbs__kk48fu</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">time: 1637897139</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">alg: v3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sig: cFJLQ0ZzMWo2VmFaaUVXMFJQbk94QUhuUWFLeEVkNFllM2tjOVhFQ1hOcEg2UC9PUUltR2l2SnZOcFpQNi9FTw==</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">elapsed: 5</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">shaOne: 00a72749f19999931e2653a554991baf3f874f7b</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">rinfo: &#123;&quot;fuid&quot;:&quot;1d5264f920930df8d682b5cbac99c9e5&quot;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一般我们对登录附加信息的获取，都会通过多个POST请求进行比较，得出不变的数据和变动的数据，</span></span><br><span class="line"><span class="string">    然后进行分析处理。</span></span><br><span class="line"><span class="string">    1. 获取表单提交参数</span></span><br><span class="line"><span class="string">    2. 确定参数获取方式-&gt;可以通过F12的内容查找，然后搜索指定参数名，找到对应的代码，分析数据如何生成的</span></span><br><span class="line"><span class="string">    3. 构建post请求，进行登录</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> Crypto.PublicKey <span class="keyword">import</span> RSA</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> PKCS1_v1_5</span><br><span class="line"><span class="keyword">import</span> PyV8 <span class="comment">#pyv8引擎，可以直接执行js代码，不用转换为python语言</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    s = requests.Session()</span><br><span class="line">    s.get(<span class="string">&#x27;http://yun.baidu.com&#x27;</span>)</span><br><span class="line">    js = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    function callback()&#123;</span></span><br><span class="line"><span class="string">        return &quot;bd__cbs__&quot; + Math.floor(Math.random() * 2147483648).toString(36) ; </span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    function gid()&#123;</span></span><br><span class="line"><span class="string">    return &quot;xxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&quot;.replace(/[xy]/g, function(e) &#123;</span></span><br><span class="line"><span class="string">                    var t = 16 * Math.random() | 0,</span></span><br><span class="line"><span class="string">                        n = &quot;x&quot; === e ? t : 3 &amp; t | 8;</span></span><br><span class="line"><span class="string">                    return n.toString(16)</span></span><br><span class="line"><span class="string">                &#125;).toUpperCase()</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    ctxt = PyV8.JsContext()</span><br><span class="line">    ctxt.enter()</span><br><span class="line">    ctxt.<span class="built_in">eval</span>(js)</span><br><span class="line">    <span class="comment">### 获取gid</span></span><br><span class="line">    gid = ctxt.<span class="built_in">locals</span>.gid()</span><br><span class="line">    <span class="comment">### 获取callback</span></span><br><span class="line">    callback = ctxt.<span class="built_in">locals</span>.callback()</span><br><span class="line">    <span class="comment">### 获取token,根据需要的参数，传入相应参数，返回对应的token</span></span><br><span class="line">    tokenUrl = <span class="string">&quot;#https://passport.baidu.com/v2/api/?getapi&amp;token=&amp;tpl=netdisk&amp;subpro=netdisk_web&amp;apiver=v3&amp;tt=1637908334880&amp;class=login&amp;gid=4D970C9-672F-4002-BCD8-D71F629D9C8A&amp;loginversion=v4&amp;logintype=basicLogin&amp;traceid=&amp;time=1637908335&amp;alg=v3&amp;sig=ZHpJazJocTlTeHNMNG5LUlZjdE8yTXNLMUNjMGk3QmVTUzdDajF2Y2Q0M3pjSDBNL1k4ZHN1VHJzY0g0QWdrbw%3D%3D&amp;elapsed=10&amp;shaOne=00c2bc34cb293b92b355a01af5b33dcb0c7c0e19&amp;callback=bd__cbs__elghk8&quot;</span></span><br><span class="line">    token_response = s.get(tokenUrl)</span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&quot;token&quot;\s*:\s*&quot;(\w+)&quot;&#x27;</span>)</span><br><span class="line">    match = pattern.search(token_response.text)</span><br><span class="line">    <span class="keyword">if</span> match:</span><br><span class="line">        token = match.group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception</span><br><span class="line">    <span class="comment">#### 获取callback</span></span><br><span class="line">    callback2 = ctxt.<span class="built_in">locals</span>.callback()</span><br><span class="line">    <span class="comment">#### 获取rsakey和pubkey</span></span><br><span class="line">    rsaUrl = <span class="string">&quot;&quot;</span></span><br><span class="line">    response = s.get(rsaUrl)</span><br><span class="line">    key = response.text <span class="comment">#匹配搜索key</span></span><br><span class="line">    pubkey = response.text</span><br><span class="line">    <span class="comment">####加密password</span></span><br><span class="line">    password = <span class="string">&#x27;&#x27;</span> <span class="comment">#自己的密码</span></span><br><span class="line">    pubkey = pubkey.replace(<span class="string">&#x27;\\n&#x27;</span>,<span class="string">&#x27;\n&#x27;</span>).replace(<span class="string">&#x27;\\&#x27;</span>,<span class="string">&#x27;&#x27;</span>)<span class="comment">#处理pubkey</span></span><br><span class="line">    rsakey = RSA.importKey(pubkey) <span class="comment">#百度网盘通过的是RSA加密</span></span><br><span class="line">    cipher = PKCS1_v1_5.new(rsakey)</span><br><span class="line">    password = base64.b64decode(cipher.encrypt(password)) <span class="comment">#加密</span></span><br><span class="line">    <span class="comment">###获取callback</span></span><br><span class="line">    callback3 = ctxt.<span class="built_in">locals</span>.callback()</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&quot;charset&quot;</span>: <span class="string">&#x27;UTF - 8&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;tpl&quot;</span>: <span class="string">&quot;netdisk&quot;</span>,</span><br><span class="line">        <span class="string">&quot;subpro&quot;</span>: <span class="string">&quot;netdisk_web&quot;</span>,</span><br><span class="line">        <span class="string">&quot;apiver&quot;</span>: <span class="string">&quot;v3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;codestring&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;safeflg&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;u&quot;</span>: <span class="string">&#x27;https: // pan.baidu.com / disk / home&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;isPhone&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;quick_user&quot;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;logintype&quot;</span>: <span class="string">&quot;basicLogin&quot;</span>,</span><br><span class="line">        <span class="string">&quot;logLoginType&quot;</span>: <span class="string">&quot;pc_loginBasic&quot;</span>,</span><br><span class="line">        <span class="string">&quot;idc&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;loginmerge&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&quot;crypttype&quot;</span>: <span class="number">12</span>,</span><br><span class="line">        <span class="string">&quot;mkey&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;countrycode&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;fp_uid&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;fp_info&quot;</span>:<span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;loginversion&quot;</span>: <span class="string">&quot;v4&quot;</span>,</span><br><span class="line">        <span class="string">&quot;supportdv&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&quot;mem_pass&quot;</span>: <span class="string">&quot;on&quot;</span>,</span><br><span class="line">        <span class="string">&quot;detect&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&quot;alg&quot;</span>: <span class="string">&quot;v3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;gid&quot;</span>:gid,</span><br><span class="line">        <span class="string">&#x27;callback&#x27;</span>:callback3,</span><br><span class="line">        <span class="string">&#x27;token&#x27;</span>:token,</span><br><span class="line">        <span class="string">&#x27;password&#x27;</span>:password</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#构建好了表单数据，然后就可以进行页面登录了</span></span><br><span class="line">    post1_response = s.post(<span class="string">&#x27;https://passport.baidu.com/v2/api/?login&#x27;</span>,data=data)</span><br><span class="line">    <span class="comment">#如果页面还有其它信息需要验证，在后面进行处理即可</span></span><br><span class="line">    <span class="comment">#比如需要滑块验证等</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><blockquote><p>验证码问题</p></blockquote><p>从上面的POST表单构建引出的一个新的问题，就是关于验证码的问题，通常验证码有三种形式，一种是在post表单中的图片的验证码，一种是根据图片点击相应的区域，一种是滑块验证。</p><blockquote><blockquote><p>IP代理</p></blockquote></blockquote><p>同一个IP频繁访问网页，可能会导致IP被封禁或者需要输入验证码验证是否是真人。解决的方法可以通过加大爬虫的延时，让网页觉得你就是真人在浏览网页，毕竟通过爬虫访问网页的频率过快，容易被识别，然而通过减慢爬虫时延带来的另一个问题就是效率的降低。更好的一种做法的更换IP进行访问。</p><p>IP代理的方式可以通过设置request的proxies参数进行设置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代理设置，使用代理Proxy,可以为任意请求方法通过设置proxies参数来配置单个请求</span></span><br><span class="line">   proxies = &#123;</span><br><span class="line">       <span class="string">&quot;http&quot;</span>: <span class="string">&quot;http://0.10.1.10:3128&quot;</span>,</span><br><span class="line">       <span class="string">&quot;https&quot;</span>: <span class="string">&quot;http://10.10.1.10:1080&quot;</span>,</span><br><span class="line">       <span class="comment"># &quot;http&quot;:&quot;http://user:pass@10.10.1.10:3128&quot; #这是代理中身份认证的用户名和密码，来设置代理</span></span><br><span class="line">   &#125;</span><br><span class="line">   requests.get(<span class="string">&quot;www.baidu.com&quot;</span>, proxies=proxies)  <span class="comment"># 设置代理ip</span></span><br></pre></td></tr></table></figure><br>我们已经知道通过request可以设置代理IP，那么如何获取代理IP？有如下几种方式：</p><blockquote><ol><li><p>VPN<br>国内和国外很多厂商提供VPN服务，可以分配不同的网络线路，并可以自动更换IP，实时性很高，速度很快。但是价格一般较高，适合商用。</p></li><li><p>IP代理池<br>一些厂商将很多IP做成代理池，提供API接口，允许用户使用程序调用。稳定的IP代理池费用也是较高的，所以不适合个人学习使用。</p></li><li><p>ADSL宽带拨号<br>也就是拨号上网。ADSL每次断开重新连接时会分配新的IP地址，爬虫可以利用这个原理更换IP。但是更换IP需要断开重连，所以效率并不高，适用于实时性不高的场景。<br>Windows下通过rasdial操作可以进行拨号，Python通过os.system来执行命令行语句，进行拨号操作。代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">作为示例进行演示，如何通过ADSL进行拨号和断开</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">g_adsl_account = &#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>:<span class="string">&quot;adsl&quot;</span>,</span><br><span class="line">    <span class="string">&quot;username&quot;</span>:<span class="string">&quot;xxxxxxxx&quot;</span>,</span><br><span class="line">    <span class="string">&quot;password&quot;</span>:<span class="string">&quot;xxxxxxxx&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adsl</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.name = g_adsl_account[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">        self.username = g_adsl_account[<span class="string">&#x27;username&#x27;</span>]</span><br><span class="line">        self.password = g_adsl_account[<span class="string">&#x27;password&#x27;</span>]</span><br><span class="line">    <span class="comment">#修改asdl</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_adsl</span>(<span class="params">self,account</span>):</span></span><br><span class="line">        self.name = account[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">        self.username = account[<span class="string">&#x27;username&#x27;</span>]</span><br><span class="line">        self.password = account[<span class="string">&#x27;password&#x27;</span>]</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        拨号连接</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">self</span>):</span></span><br><span class="line">        cmd_str = <span class="string">&quot;rasdlal %s %s %s&quot;</span>%(self.name,self.username,self.password)</span><br><span class="line">        os.system(cmd_str)<span class="comment">#调用windows的控制台，通过rasdlal进行拨号</span></span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        断开连接</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">disconnect</span>(<span class="params">self</span>):</span></span><br><span class="line">        cmd_str = <span class="string">&quot;rasdial %s / disconnect&quot;</span>%self.name</span><br><span class="line">        os.system(cmd_str)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        重新拨号</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconnect</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.disconnect()</span><br><span class="line">        self.connect()</span><br></pre></td></tr></table></figure></li></ol></blockquote><p>如果我们想要找到一些免费的IP代理，可以在各个网站寻找免费的IP，然后对IP去重，检测代理有效性等操作，存放到数据库中，通过接口获取免费的IP，构建自己的IP池。</p><blockquote><blockquote><p>图片验证码识别</p></blockquote></blockquote><p>我们在浏览器登录的时候，常常需要输入图片验证码，这里给出几种方法解决图片验证码的问题。</p><blockquote><ol><li>Cookie绕过登录，我们在浏览网站的时候，常常会保留登录信息，下次登录就不要进行登录，通过Cookie即可登录。所以我们可以获取登录后的Cookie，然后设置Cookie，绕过登录。可以获取多个Cookie，构建Cookie池，绕过登录验证。示例代码如下。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过已经登陆过的Cookie进行通过Cookie进行登录。</span></span><br><span class="line"><span class="string">    我们可以在模拟登录成功后，将session的值保存到本地，后续可以通过cookie进行登录。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.cookies <span class="keyword">import</span> cookiejar_from_dict</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_session</span>(<span class="params">session</span>):</span></span><br><span class="line">    <span class="comment">#将session写入文件,session.txt</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;session.txt&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(session.headers,f) <span class="comment">#写入头</span></span><br><span class="line">        pickle.dump(session.cookes.get_dict(),f)<span class="comment">#写入Cookie值</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;将sesssion写入文件:session.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_session</span>():</span></span><br><span class="line">    <span class="comment">#加载session</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;session.txt&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = pickle.load(f)</span><br><span class="line">        cookies = pickle.load(f)</span><br><span class="line">        <span class="keyword">return</span> headers,cookies</span><br><span class="line">session  = requests.Session()</span><br><span class="line">session.headers = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#x27;</span></span><br><span class="line">dicts = <span class="built_in">dict</span>()</span><br><span class="line">dicts[<span class="string">&#x27;BDORZ&#x27;</span>] = <span class="string">&#x27;FFFB88E999055A3F8A630C64834BD6D0&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;PSINO&#x27;</span>] = <span class="string">&#x27;7&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BDRCVFR[hpwYTbhBfiY]&#x27;</span>] = <span class="string">&#x27;9xWipS8B-FspA7EnHc1QhPEUf&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BDUSS_BFESS&#x27;</span>] = <span class="string">&#x27;XFOcmlGQVhEdUZjSnJiWmJmbVgza35YV0RvZktEUEdkUXgyYkJ4S01uOER0OHRoSVFBQUFBJCQAAAAAAAAAAAEAAADprZrnsrvLxrnp1MYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMqpGEDKqRhW&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BDUSS&#x27;</span>] = <span class="string">&#x27;XFOcmlGQVhEdUZjSnJiWmJmbVgza35YV0RvZktEUEdkUXgyYkJ4S01uOER0OHRoSVFBQUFBJCQAAAAAAAAAAAEAAADprZrnsrvLxrnp1MYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMqpGEDKqRhW&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BAIDUID_BFESS&#x27;</span>] = <span class="string">&#x27;E8BBAF114F9097C4849493D68A677EC2:FG=1&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;HMACCOUNT_BFESS&#x27;</span>] = <span class="string">&#x27;13391551711E4651&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;MCITY&#x27;</span>] = <span class="string">&#x27;-%3A&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BAIDUID&#x27;</span>] = <span class="string">&#x27;A02BD18D5032DA5E3DA0339468C7AE42:FG=1&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;__yjs_duid&#x27;</span>] = <span class="string">&#x27;1_2b5acc51d9eef8efeada14364c2710b71633588836061&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_dl_fref&#x27;</span>] = <span class="string">&#x27;https://blog.csdn.net/kaida1234/article/details/89553115&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac&#x27;</span>] = <span class="string">&#x27;1638500494&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_page_id&#x27;</span>] = <span class="string">&#x27;default&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_first_page&#x27;</span>] = <span class="string">&#x27;https%3A//i.csdn.net/&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;log_Id_pv&#x27;</span>] = <span class="string">&#x27;1556&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;delPer&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;TY_SESSION_ID&#x27;</span>] = <span class="string">&#x27;1e66f339-636d-4633-845e-6afde0a1d0f8&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;PSTM&#x27;</span>] = <span class="string">&#x27;1633401440&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;dc_sid&#x27;</span>] = <span class="string">&#x27;2cee0d889d6dbf50e0ba642d35372f04&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;dc_session_id&#x27;</span>] = <span class="string">&#x27;10_1638499930521.206489&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;log_Id_click&#x27;</span>] = <span class="string">&#x27;664&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_segment&#x27;</span>] = <span class="string">&#x27;12&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_first_ref&#x27;</span>] = <span class="string">&#x27;cn.bing.com&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;ab_sr&#x27;</span>] = <span class="string">&#x27;1.0.1_MzcxMWQ4NzQzNGYyZWRmZmJhNTA2NTdiNDY5Yjc2M2I3NTg2MThlZjg4OGRhZWVjNGExYmEwYzljODc5ZmExNmJhM2RkYWQxOTI5NzcyZjhiZWM5MDExNmU3ODZjNTcxMGY1MDg0ZTA1MmE3MTc4MWZjYTcxYjQ0ODg5OGVjY2JiZDgyZjlkYjk5Nzg4M2Q4OTVkYzRiMDIwZWJkM2FmOQ==&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;firstDie&#x27;</span>] = <span class="string">&#x27;1&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;log_Id_view&#x27;</span>] = <span class="string">&#x27;4444&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_dl_um&#x27;</span>] = <span class="string">&#x27;-&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;AU&#x27;</span>] = <span class="string">&#x27;5E7&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_dl_rid&#x27;</span>] = <span class="string">&#x27;1638195457309_383576&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;H_PS_PSSID&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;uuid_tt_dd&#x27;</span>] = <span class="string">&#x27;10_37481785600-1633532465546-630652&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;Hm_up_6bcd52f51e9b3dce32bec4a3997715ac&#x27;</span>] = <span class="string">&#x27;%7B%22islogin%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isonline%22%3A%7B%22value%22%3A%221%22%2C%22scope%22%3A1%7D%2C%22isvip%22%3A%7B%22value%22%3A%220%22%2C%22scope%22%3A1%7D%2C%22uid_%22%3A%7B%22value%22%3A%22weixin_43387852%22%2C%22scope%22%3A1%7D%7D&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_dl_prid&#x27;</span>] = <span class="string">&#x27;1638100826971_946707&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;Hm_lvt_e5ef47b9f471504959267fd614d579cd&#x27;</span>] = <span class="string">&#x27;1637833430&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac&#x27;</span>] = <span class="string">&#x27;6525*1*10_37481785600-1633532465546-630652!5744*1*weixin_43387852&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BT&#x27;</span>] = <span class="string">&#x27;1633859763611&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;HMACCOUNT&#x27;</span>] = <span class="string">&#x27;13391551711E4651&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac&#x27;</span>] = <span class="string">&#x27;1638500140,1638500349,1638500394,1638500403&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;p_uid&#x27;</span>] = <span class="string">&#x27;U010000&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;UN&#x27;</span>] = <span class="string">&#x27;weixin_43387852&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;__gads&#x27;</span>] = <span class="string">&#x27;ID=20d2edb2ecff0dd8-2294b66f51cc00dd:T=1633682976:RT=1633682976:S=ALNI_MZ4Wd5yAdGLxFPr5rHe4LQE0aLwJw&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BIDUPSID&#x27;</span>] = <span class="string">&#x27;FBAA203ACF6D45F80BC31F3396736AD0&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;dc_tos&#x27;</span>] = <span class="string">&#x27;r3ir2m&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_ref&#x27;</span>] = <span class="string">&#x27;https%3A//i.csdn.net/&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;ssxmod_itna&#x27;</span>] = <span class="string">&#x27;Yq+xBDuQG=YUGkDzxAhYo=mxWqY5Y3K3qoiQD/YBmDnqD=GFDK40oE7bD7mKn5BAuDeWxrGhuqWKFixPmaAiDOh31A+=0iEdDU4i8DCTrPoD4fKGwD0eG+DD4DWDmW7DnxAQDjxGpycuTXBDi3Dbg=Di4D+zd=DmqG0DDUH/4G2D7Uy8ivyWld52ubMiir4YB=DjqTD/+qFMWRFsa5VWjnTu44DC2v1oi519poqYAiWbqGybKGunqXV4uR1pq0Z3j5vKG4oBrdoBiPet0q3ngxU6GDPARCztP4KD++P7g+dBr5DGRwx3wDxD&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;UserToken&#x27;</span>] = <span class="string">&#x27;885ba7040ab64148b8d10b4450fcc279&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;UserNick&#x27;</span>] = <span class="string">&#x27;%E5%BD%92%E6%9D%A5%E7%A9%BA%E7%A9%BA&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;BDRCVFR[C0p6oIjvx-c]&#x27;</span>] = <span class="string">&#x27;Ble67U-OKLffjRLnjc3nW6kg1IxpA7E&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;ssxmod_itna2&#x27;</span>] = <span class="string">&#x27;Yq+xBDuQG=YUGkDzxAhYo=mxWqY5Y3K3qoiG9toFxBwm47pxUxBa4Hq7GFGfoiDhPizeG7q5h7Mt7khdzCG2O7=IYjr6oTN9ebmgXvejbDDQt5Uljyh7yEuBiEMSIhcGRVeRAB+Er04T2YpCHxTHtYBrV15C0eviq+WkT8vmqGKrtFG=Mo7sk/ntYiGQI87s1SYx1=TpeYcD5OALP3OQgpKTRGpC8bLiPSMTSCodV1ddSeu1P99Niqq8dQDd8+mcI+I5FHuv=yFBy=Ui7ZguyP6CkyK9C79iTqo4RqQd7EgTbEExB=XSjr88yXeYel5TGD1iDPnnD87ZjDQAGXQH+juTKGtnYnRahRAEKiPL+T8qkYT8m+Rfrue493By+ydLytmbuKaH8EG7wRfwD1eVGar8usImSzTL8TEB2qTwYBx4U5lGftyDTa9bB0sRWl748nyt6uW0xEDBAYAm8Hxm5fDx//oaf2ZYB4txDKdNdYzlm+0Ghg+xMcWCAXV138h8iDaP8DzG482Wr72i44D7=DYIOeD=&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_pref&#x27;</span>] = <span class="string">&#x27;https%3A//cn.bing.com/&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_utm_medium&#x27;</span>] = <span class="string">&#x27;distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;UserName&#x27;</span>] = <span class="string">&#x27;weixin_43387852&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;c_dl_fpage&#x27;</span>] = <span class="string">&#x27;/download/weixin_38695452/12856957&#x27;</span></span><br><span class="line">dicts[<span class="string">&#x27;UserInfo&#x27;</span>] = <span class="string">&#x27;885ba7040ab64148b8d10b4450fcc279&#x27;</span></span><br><span class="line"><span class="comment">#不能直接将字典对象设置为cookies ，需要通过requests库的cookiejar_from_dict()方法</span></span><br><span class="line"><span class="comment">#把字段对象转换为cookiejar对象</span></span><br><span class="line">session.cookies = cookiejar_from_dict(dicts)</span><br><span class="line">r = session.post(<span class="string">&#x27;https://blog.csdn.net/&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r.status_code)</span><br><span class="line"><span class="built_in">print</span>(r.text)</span><br><span class="line">save_session(session)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(session.headers)</span></span><br><span class="line"><span class="comment"># print(session.cookies.get_dict())</span></span><br><span class="line"><span class="comment"># session.headers,session.cookies = load_session() #将cookie设置为保存的cookie</span></span><br></pre></td></tr></table></figure></li><li>借助图片识别技术识别图片文字。一种常用的是通过tesseract-ocr进行识别，一种借助百度提供的文字识别技术进行识别处理。</li></ol></blockquote><p>通过图像识别技术来获取文字信息，通常需要对图像进行预处理来提高识别准确率。这里给出一些处理流程。</p><p><pre class="mermaid">    graph LR        原始图片 --> 图片放大        图片放大 --> 灰度化        灰度化 --> 二值化        二值化 --> 去除边框        去除边框 --> 降噪        降噪 --> 结束</pre><br>示例代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#图像处理方法</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#1.放大图片</span></span><br><span class="line"><span class="comment">#DPI≥ 300的图片有更好的识别效果</span></span><br><span class="line">img_org = cv2.imread(<span class="string">&#x27;R-C.png&#x27;</span>)</span><br><span class="line">h,w = img_org.shape[:<span class="number">2</span>]</span><br><span class="line">h,w = h*<span class="number">2</span>,w*<span class="number">2</span> <span class="comment">#按比例放大两倍</span></span><br><span class="line">img_l = cv2.resize(img_org,(w,h))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(cv2.cvtColor(img_l,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#2. 图像灰度化，通过cvtColor灰度化图像</span></span><br><span class="line">img_org = cv2.imread(<span class="string">&#x27;R-C.png&#x27;</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img_org,cv2.COLOR_BGR2GRAY) <span class="comment"># 灰度化</span></span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#3.图像二值化</span></span><br><span class="line"><span class="comment">#二值化将图片转换为黑白图像，二值化可以通过cv2.threshold()方法，该方法会返回两个参数，第一个为阈值，第二个为转化后的图像</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">cv2.threshold(src,thresh,maxval,type,dst=None)</span></span><br><span class="line"><span class="string">其中thresh为阈值，maxval为高于阈值的处理，dst为二值化处理方法选择的参数。</span></span><br><span class="line"><span class="string">dst指定的不同在于处理逻辑的不同。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">img_org = cv2.imread(<span class="string">&#x27;R-C.png&#x27;</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img_org,cv2.COLOR_BGR2GRAY) <span class="comment"># 灰度化</span></span><br><span class="line"><span class="comment"># 像素点高于127的设置为255，不高于127的设置为0</span></span><br><span class="line">_,img_bin = cv2.threshold(img_gray,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY)<span class="comment">#二值化</span></span><br><span class="line">plt.subplot(<span class="number">131</span>),plt.imshow(cv2.cvtColor(img_org,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.subplot(<span class="number">132</span>),plt.imshow(cv2.cvtColor(img_gray,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.subplot(<span class="number">133</span>),plt.imshow(cv2.cvtColor(img_bin,cv2.COLOR_BGR2RGB))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#4.去除边框,当存在边框的时候去除,</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clearBorder</span>(<span class="params">img</span>):</span></span><br><span class="line">    h,w = img.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,w):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,h):</span><br><span class="line">            <span class="keyword">if</span>(y&lt;<span class="number">2</span> <span class="keyword">or</span> y&gt;w-<span class="number">2</span>):</span><br><span class="line">                img[x,y] = <span class="number">255</span></span><br><span class="line">            <span class="keyword">if</span> x &lt; <span class="number">2</span> <span class="keyword">or</span> x &gt;h-<span class="number">2</span>:</span><br><span class="line">                img[x,y] = <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="comment">#5. 降噪</span></span><br><span class="line"><span class="comment"># 噪声是图像中亮度或颜色的随机变化，降低噪声有利于准确率的提升，噪声去除的方法有很多</span></span><br><span class="line"><span class="comment"># 这里以线降噪为例</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">interfaceLine</span>(<span class="params">img</span>):</span></span><br><span class="line">    h,w = img.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,w-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,h-<span class="number">1</span>):</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">all</span>(img[x,y-<span class="number">1</span>])&gt;<span class="number">245</span>:</span><br><span class="line">                count = count+<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">all</span>(img[x,y+<span class="number">1</span>])&gt;<span class="number">245</span>:</span><br><span class="line">                count = count+<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">all</span>(img[x-<span class="number">1</span>],y)&gt;<span class="number">245</span>:</span><br><span class="line">                count = count+<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> np.<span class="built_in">all</span>(img[x+<span class="number">1</span>],y)&gt;<span class="number">245</span>:</span><br><span class="line">                count = count+<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count &gt; <span class="number">2</span> :</span><br><span class="line">                img[x,y] = <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p><p>通过tesseract-ocr进行识别，需要下载tesseract-oct,并且下载两个包。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pytesseract</span><br><span class="line">pip install pillow</span><br></pre></td></tr></table></figure><br>示例代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pyteFunction</span>():</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(<span class="string">&#x27;test.PNG&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置tesseract的安装路径,这里设置你下载的tesseract-ocr的安装路径</span></span><br><span class="line">    pytesseract.pytesseract.tesseract_cmd = <span class="string">r&#x27;C:\Program Files (x86)\Tesseract-OCR\tesseract.exe&#x27;</span></span><br><span class="line"></span><br><span class="line">    code = pytesseract.pytesseract.image_to_string(image)</span><br><span class="line">    <span class="built_in">print</span>(code)</span><br></pre></td></tr></table></figure></p><p>通过百度API调用文字识别服务。可以访问<a href="https://ai.baidu.com/">https://ai.baidu.com/</a>，然后搜索文字识别，创建一个文字识别应用，创建成功后，根据提供的APP_KEY和Secret_Key即可调用API，获取到服务。示例代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过调用百度API进行验证码的识别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identify_Verification_code</span>(<span class="params">API_Key, Secret_Key, Verification_code</span>):</span></span><br><span class="line"></span><br><span class="line">    host = <span class="string">&#x27;https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&amp;client_id=&#x27;</span> + API_Key + <span class="string">&#x27;&amp;client_secret=&#x27;</span> + Secret_Key</span><br><span class="line">    response = requests.get(host)</span><br><span class="line">    access_token = response.json()[<span class="string">&#x27;access_token&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    request_url = <span class="string">&quot;https://aip.baidubce.com/rest/2.0/ocr/v1/general_basic&quot;</span></span><br><span class="line">    <span class="comment"># 二进制方式打开图片文件,Verification_code是要识别的验证码的名字</span></span><br><span class="line">    f = <span class="built_in">open</span>(Verification_code, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">    img = base64.b64encode(f.read())</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">&quot;image&quot;</span>: img&#125;</span><br><span class="line">    access_token = access_token</span><br><span class="line">    request_url = request_url + <span class="string">&quot;?access_token=&quot;</span> + access_token</span><br><span class="line">    headers = &#123;<span class="string">&#x27;content-type&#x27;</span>: <span class="string">&#x27;application/x-www-form-urlencoded&#x27;</span>&#125;</span><br><span class="line">    response = requests.post(request_url, data=params, headers=headers)</span><br><span class="line">    shibie_result = response.json()[<span class="string">&#x27;words_result&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;words&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(shibie_result)</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过调用百度提供的接口来进行识别验证码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baiduAPI</span>():</span></span><br><span class="line">    API_Key = <span class="string">&#x27;#&#x27;</span> <span class="comment">#应用的key</span></span><br><span class="line">    Secret_Key = <span class="string">&#x27;#&#x27;</span> <span class="comment">#应用的Secret_Key</span></span><br><span class="line">    Verification_code = <span class="string">&#x27;R-C.PNG&#x27;</span> <span class="comment">#图片地址</span></span><br><span class="line">    identify_Verification_code(API_Key, Secret_Key, Verification_code)</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>滑动验证码</p></blockquote></blockquote><p>滑动验证码涉及到图片拼接方面的知识。要想解决可以有以下思路，一是通过selenium模拟行为，解决滑动验证码，二是组建Cookie池绕过验证码。</p><p>通过selenium解决滑动验证码的流程：</p><blockquote><ol><li>获取图片（不带缺口的图片，带缺口的图片）</li><li>识别缺口位置（设置一个对比阈值，遍历两张图片，找出相同位置像素RGB差距，超过此阈值的像素点，此像素点的位置就是缺口的位置）</li><li>计算滑动距离</li><li>模拟运动</li></ol><blockquote><blockquote><p>www&gt;m&gt;wap</p></blockquote></blockquote></blockquote><p>www是PC浏览器看到的网站，m和wap是移动端，大部分智能手机用的是m站，少部分旧手机用的还是wap。一般wap爬取较为简单，我们可以通过修改User-Agent模拟不同终端发送出请求，请求不同的页面。<br>如何设置不同的User-Agent,我们可以在浏览器扩展选项中下载相关的User-Agent的应用。以Edge为例：<br>下载图中的扩展应用。<br><img src="/images/user-agent.PNG" alt="A33"><br>按照如下操作，选择需要模拟的浏览器，然后通过Verify User-Agent Setting可以查看User-Agent，以Iphone为例.<br><img src="/images/step.PNG" alt="A34"><br>获取到的请求头：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mozilla/<span class="number">5.0</span> (iPhone; CPU iPhone OS <span class="number">13_2_3</span> like Mac OS X) AppleWebKit/<span class="number">605.1</span><span class="number">.15</span> (KHTML, like Gecko) Version/<span class="number">13.0</span><span class="number">.3</span> Mobile/<span class="number">15E148</span> Safari/<span class="number">604.1</span></span><br></pre></td></tr></table></figure></p><blockquote><blockquote><p>小结</p></blockquote></blockquote><p>对于网页爬取登录问题和验证码问题，需要对具体的网页进行分析，是通过API解析还是通过Cookie池登录，以及登录过程中的验证码破解问题。</p><h4 id="4-终端协议分析"><a href="#4-终端协议分析" class="headerlink" title="4.  终端协议分析"></a>4. <a id="four"></a> 终端协议分析</h4><p>一个应用不仅仅具有PC端而且还具有移动客户端。所以我们在进行爬虫开发的时候，不仅仅可以从PC端进行爬取，而且还可以从移动客户端进行爬取。我们可以借助User-Agent来伪装自己，进行数据的爬取。</p><blockquote><h3 id="1-PC端数据抓包分析"><a href="#1-PC端数据抓包分析" class="headerlink" title="1. PC端数据抓包分析"></a>1. PC端数据抓包分析</h3></blockquote><p>将爬虫伪装成PC客户端，可以对PC客户端进行抓包分析。PC抓包软件有Wireshark,Http Analyzer等。Wireshark擅长各类网络协议分析，比较重型。而Http Analizer更注重于对Http/Https协议的分析。Http Analizer可以针对某个进程进行抓包。</p><blockquote><blockquote><h4 id="Wireshark"><a href="#Wireshark" class="headerlink" title="Wireshark"></a>Wireshark</h4></blockquote></blockquote><p>Wireshark的使用方法很简单，只需要设置过滤规则进行监听即可。<br>简单的过滤规则，如指定监听的网址ip.addr == 121.40.103.238(这个地址是虾米音乐网地址，<a href="https://www.xiami.com/">https://www.xiami.com/</a>)。然后点击开始监听。<br>则Wireshark就会监听和121.40.103.238的数据。通过cmd，ping这个地址即可。</p><p><img src="/images/wireshark.PNG" alt="a41"></p><blockquote><blockquote><h4 id="http-Analyzer"><a href="#http-Analyzer" class="headerlink" title="http Analyzer"></a>http Analyzer</h4></blockquote></blockquote><p>http Analyzer的使用方法也很简单，点击开始监听即可。可以通过选择需要监听的进程进行监听。通过type可以选择需要过滤的数据类型。</p><p>通过抓包进行分析请求，进行数据爬取一般都是挺复杂的，涉及到逆向PC客户端软件和分析算法的能力。所以如果想要通过抓包进行爬取，需要了解相关技术进行爬取。</p><blockquote><h3 id="2-App抓包分析"><a href="#2-App抓包分析" class="headerlink" title="2. App抓包分析"></a>2. App抓包分析</h3></blockquote><p>移动端的抓包分析，以Android App为例。对Android应用抓包，通过下载安卓模拟器，将应用安装到模拟器中，然后进行抓包分析。</p><p>这里采用wireshake进行抓包分析，wireshake的过滤示例如下：</p><blockquote><p>过滤域名包含baidu.com的数据<br>http.host contains “baidu.com”<br>过滤指定ip地址<br>ip.addr == xxx.xxx.xxx.xxx</p></blockquote><p><img src="/images/wiresharke-1.PNG" alt="a44"><br><img src="/images/wiresharke-2.PNG" alt="a45"></p><h6 id="5-初窥Scrapy爬虫框架"><a href="#5-初窥Scrapy爬虫框架" class="headerlink" title="5. 初窥Scrapy爬虫框架"></a>5.<a id="five"></a> 初窥Scrapy爬虫框架</h6><p>Scrapy爬虫框架操作简单，是比较流行的爬虫解决方案，所以在这里对Scrapy进行学习。<br>Scraoy使用Twisted这个异步网络来处理网络通信，并且包含了各种中间件接口，可以灵活的完成各种需求。</p><blockquote><h4 id="1-Scrapy各大组件"><a href="#1-Scrapy各大组件" class="headerlink" title="1. Scrapy各大组件"></a>1. Scrapy各大组件</h4></blockquote><p>Scrapy的各个组件其实和之前学过的简单爬虫的各个模块相类似。</p><blockquote><blockquote><ol><li>Scrapy 引擎(Engine)</li></ol></blockquote></blockquote><p>Scrapy引擎负责控制数据流在系统的所有组件中流动，并在相应动作发生时触发事件。<br>这类似于爬虫调度器的作用。</p><blockquote><blockquote><ol><li>Scrapy 调度器(Scheduler)</li></ol></blockquote></blockquote><p>Scrapy调度器从引擎接收Request并将它们入队，以便之后引擎请求request时提供给引擎。这类似于URL管理器的作用。</p><blockquote><blockquote><ol><li>Scrapy 下载器(Downloader)</li></ol></blockquote></blockquote><p>Scrapy下载器负责获取页面数据并提供给引擎，而后提供给Spider。这类似于html下载器作用。</p><blockquote><blockquote><ol><li>Spider</li></ol></blockquote></blockquote><p>Spider是Scrapy用户编写用于分析Response并提取Item(即获取到的Item)或额外跟进的URL的类。每个Spider负责处理一个特定(或一些)网站。类似于html解析器作用。</p><blockquote><blockquote><ol><li>Item Pipeline</li></ol></blockquote></blockquote><p>Item Pipeline负责处理被Spider提取出来的Item。典型的处理有清理验证及持久化(例如存储到数据库中)。类似于数据存储器的作用。</p><blockquote><blockquote><ol><li>Downloader middlewares（下载器中间件）</li></ol></blockquote></blockquote><p>下载器中间件是在引擎及下载器之间的特定钩子(specific hook),处理Downloader传递给引擎的Response.<br>其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</p><blockquote><blockquote><ol><li>Scrapy中间件(Spider middlewares)</li></ol></blockquote></blockquote><p>Scrapy中间件是在引擎及Spider之间的特定钩子(specific hook),处理Spider的输入(response)和<br>输出(Items及requests)。其提供了一个渐变的机制，通过插入自定义代码来扩展Scrapy功能。</p><p><img src="/images/Scrapy.PNG" alt="a58"></p><p><img src="/public/images/Spyder流程.PNG" alt="a59"></p><blockquote><h4 id="2-Scrapy的安装"><a href="#2-Scrapy的安装" class="headerlink" title="2. Scrapy的安装"></a>2. Scrapy的安装</h4></blockquote><p>这里介绍一下Windows下如何安装Scrapy(Python 3.7版本)。<br>如果不是Python3.7版本的，需要下载对应的Pywin32。</p><blockquote><h4 id="1-Pywin32下载"><a href="#1-Pywin32下载" class="headerlink" title="1. Pywin32下载"></a>1. Pywin32下载</h4></blockquote><p><a href="https://github.com/mhammond/pywin32/">下载地址</a></p><p>各个Pywin32的版本如图所示：<br><img src="/images/pywin32.PNG" alt="a58"></p><p>找到3.7版本，64位的安装包。<br><img src="/images/pywin32-1.PNG" alt="a59"></p><p>运行安装包后，通过命令行测试，如果没报错，则安装成功。<br><img src="/images/pywin32-2.PNG" alt="a590"></p><p>安装成功后，需要通过pip下载pywin32，或者直接通过pycharm安装pywin32<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure></p><blockquote><h4 id="2-pyOpenSSL下载"><a href="#2-pyOpenSSL下载" class="headerlink" title="2. pyOpenSSL下载"></a>2. pyOpenSSL下载</h4></blockquote><p><a href="https://github.com/pyca/pyopenssl">下载地址</a> , 或者通过pycharm下载 pyOpenSSL,或者通过 pip install pyOpenSSL</p><blockquote><h4 id="3-lxml下载"><a href="#3-lxml下载" class="headerlink" title="3. lxml下载"></a>3. lxml下载</h4></blockquote><p>通过pycharm下载，或者pip install lxml</p><blockquote><h4 id="4-Scrapy下载"><a href="#4-Scrapy下载" class="headerlink" title="4. Scrapy下载"></a>4. Scrapy下载</h4></blockquote><p>通过pycahrm下载，或者通过pip install Scrapy</p><p>如果要查看是否安装成功，通过pip list命令可以查看已经安装的库。</p><blockquote><h4 id="3-Scrapy应用"><a href="#3-Scrapy应用" class="headerlink" title="3. Scrapy应用"></a>3. Scrapy应用</h4><blockquote><ol><li>创建一个新的Scrapy项目</li></ol></blockquote></blockquote><p>命令行切换到指定文件夹，运行命令 scrapy startproject xxxx ,即可创建名为xxxx的项目。</p><p><img src="/images/scrapy-4.PNG" alt="a533"></p><p>如果出现”ImportError: DLL load failed: 找不到指定的程序”错误提示，或者出现python.exe无法找到入口….pythoncom37.dll。</p><p>首先下载：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: DLL load failed: 找不到指定的程序</span><br></pre></td></tr></table></figure></p><p>然后在电脑搜索pythoncom37.dll。观察无法找到入口的路径，一般都是在C盘的System32下，然后将搜索到的pythoncom37.dll所在的文件夹的内容(选择lib目录下的pywin32下的pythoncom37.dll文件夹里面的两个dll文件放到system32下)，复制到System32下，覆盖即可。</p><p><img src="/images/scrapy-7.PNG" alt="a554"></p><p>创建成功后的目录结构如图所示：</p><p><img src="/images/scrapy目录结构.PNG" alt="a556"></p><blockquote><blockquote><blockquote><h4 id="1-创建爬虫模块"><a href="#1-创建爬虫模块" class="headerlink" title="1. 创建爬虫模块"></a>1. 创建爬虫模块</h4></blockquote></blockquote></blockquote><p>爬虫模块都放置于spiders文件夹中。爬虫模块用于从单个网站或多个网站爬取数据的类。包含初始页面的URL，网页链接，分析网页内容，提取数据。<br>需要创建一个Spider类，继承scrapy.Spider类。<br>需要定义下面三个属性：<br>name:区别Spider，名字必须唯一。<br>start_urls:Spider启动时进行爬取的入口URL列表。<br>parse():Spider的一个方法，被调用时，负责解析返回数据，提取数据。</p><p>启动爬虫模块：通过命令行，切换到项目根目录下，运行<br>scrapy crawl xxxx , 其中xxxx为Spider类的name属性。</p><blockquote><blockquote><blockquote><h4 id="2-选择器"><a href="#2-选择器" class="headerlink" title="2. 选择器"></a>2. 选择器</h4></blockquote></blockquote></blockquote><p>Scrapy选择器构建于lxml库之上。也可也通过Beautifulsoup库解析。</p><p>Selector的用法：Selector对象有四个基本方法</p><blockquote><ol><li><p>xpath(query):传入XPath表达式query，返回该表达式所有节点的selector list列表</p></li><li><p>css(query):传入CSS表达式query，返回该表达式所对应的所有节点的selector list列表</p></li><li><p>extract():序列化该节点为Unicode字符串并返回list列表</p></li><li><p>re(regex):根据传入的正则表达式对数据进行提取，返回Unicode字符串列表。</p></li></ol></blockquote><p>在spider类中的parse方法中，传入的一个参数为response,通过Selector(response)<br>即可创建一个Selector对象。<br>示例代码如下所示<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnblogsSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;cnblogs&quot;</span> <span class="comment">#爬虫名称</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;cnblogs.com&quot;</span>] <span class="comment">#允许的域名</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&quot;http://www.cnblogs.com/qiyeboy/default.html?page=1&quot;</span></span><br><span class="line">    ] <span class="comment"># Spider启动时进行爬取的入口URL列表。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#解析函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># #Selector使用</span></span><br><span class="line">        <span class="comment"># selector = Selector(response)</span></span><br><span class="line">        <span class="comment"># #调用xpath</span></span><br><span class="line">        <span class="comment"># selector.xpath()</span></span><br><span class="line">        <span class="comment"># #调用css</span></span><br><span class="line">        <span class="comment"># selector.css()</span></span><br><span class="line">        <span class="comment"># #调用re</span></span><br><span class="line">        <span class="comment"># selector.re()</span></span><br><span class="line">        <span class="comment"># #调用extract</span></span><br><span class="line">        <span class="comment"># selector.extract()</span></span><br><span class="line"></span><br><span class="line">        papers = response.xpath(<span class="string">&quot;.//*[@class=&#x27;day&#x27;]&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">            url = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            title = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            time = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            content = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(url,title,time,content)</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><h4 id="3-命令行工具"><a href="#3-命令行工具" class="headerlink" title="3. 命令行工具"></a>3. 命令行工具</h4></blockquote></blockquote></blockquote><p>这里介绍一下Scrapy命令行功能。</p><p>创建一个项目</p><blockquote><p>scrapy createproject xxxx<br>运行一个项目<br>scrapy crawl name（spider的name属性）<br>运行单个spider模块<br>scrapy runspider xxxx.py<br>列出当前项目所有Spider<br>scrapy list<br>快速创建spider模板<br>scrapy genspider -l<br>scrapy genspider -d basic<br>scrapy genspider -t basic example example.com<br>将项目部署到Scrapy服务<br>scrapy deploy</p><blockquote><blockquote><h4 id="4-定义Item"><a href="#4-定义Item" class="headerlink" title="4. 定义Item"></a>4. 定义Item</h4></blockquote></blockquote></blockquote><p>Item用于保存爬取到的数据，类似于字典类型，在之前的目录结构中，有个items.py文件用来定义存储数据的Item类。这个类需要继承scrapy.Item。示例代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnblogspiderItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like</span></span><br><span class="line">    <span class="comment"># 类似于字典类型，可以直接通过[]和get方法获取</span></span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    time = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    item = CnblogspiderItem(title=<span class="string">&#x27;爬虫&#x27;</span>,content=<span class="string">&#x27;爬虫开发&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(item.get(<span class="string">&#x27;title&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(item.keys())</span><br><span class="line">    <span class="built_in">print</span>(item.items())</span><br><span class="line">    item2 = CnblogspiderItem(item) <span class="comment">#Item复制</span></span><br><span class="line">    <span class="comment">#dict和item转化</span></span><br><span class="line">    dict_item = <span class="built_in">dict</span>(item)</span><br><span class="line">    <span class="built_in">print</span>(dict_item)</span><br><span class="line">    item = CnblogspiderItem(&#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;爬虫&#x27;</span>,<span class="string">&#x27;content&#x27;</span>:<span class="string">&#x27;开发&#x27;</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure></p><p>我们可以在原有的Item基础之上，添加更过的字段用于扩展Item。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnblogspiderItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like</span></span><br><span class="line">    <span class="comment"># 类似于字典类型，可以直接通过[]和get方法获取</span></span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    time = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">newCnblogspiderItem</span>(<span class="params">CnblogspiderItem</span>):</span></span><br><span class="line">    body = scrapy.Field()</span><br><span class="line">    title = scrapy.Field(CnblogspiderItem.fields[<span class="string">&#x27;title&#x27;</span>],body=body)</span><br></pre></td></tr></table></figure><p>如果要实现翻页功能,将提取出来的URL，构建新的Request对象，并指定解析方法。：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#解析函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">    <span class="comment"># #Selector使用</span></span><br><span class="line">    <span class="comment"># selector = Selector(response)</span></span><br><span class="line">    <span class="comment"># #调用xpath</span></span><br><span class="line">    <span class="comment"># selector.xpath()</span></span><br><span class="line">    <span class="comment"># #调用css</span></span><br><span class="line">    <span class="comment"># selector.css()</span></span><br><span class="line">    <span class="comment"># #调用re</span></span><br><span class="line">    <span class="comment"># selector.re()</span></span><br><span class="line">    <span class="comment"># #调用extract</span></span><br><span class="line">    <span class="comment"># selector.extract()</span></span><br><span class="line"></span><br><span class="line">    papers = response.xpath(<span class="string">&quot;.//*[@class=&#x27;day&#x27;]&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">        url = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        title = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        time = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        content = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        item = CnblogspiderItem(url=url,title=title,time=time,content=content)</span><br><span class="line">        <span class="keyword">yield</span> item <span class="comment">#将数据返回</span></span><br><span class="line">    next_page = Selector(response).re(<span class="string">u&#x27;&lt;a href=&quot;(\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> next_page:</span><br><span class="line">        <span class="comment">#Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由</span></span><br><span class="line">        <span class="comment">#谁来解析此项Request请求的响应</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=next_page[<span class="number">0</span>],callback=self.parse)</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><h4 id="5-定义Item-Pipeline"><a href="#5-定义Item-Pipeline" class="headerlink" title="5. 定义Item Pipeline"></a>5. 定义Item Pipeline</h4></blockquote></blockquote></blockquote><p>通过Item Pipeline可以对数据进行持久化存储，通过Item Pipeline可以对Item进行处理。<br>Item Pipeline主要有如下几个作用：</p><ol><li>清理HTML数据</li><li>验证爬取数据的合法性，检查Item是否包含某些字段</li><li>查重并丢弃</li><li>将数据结果保存到文件或数据库中</li></ol><p>Item Pipeline组件是一个独立的Python类，必须实现proocess_item方法</p><blockquote><p>process_item(self,item,spider)<br>其中itme是被爬取的item，Spider对象代表着爬取该Item的Spider</p></blockquote><p>示例代码如下所示:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnblogspiderPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.file = <span class="built_in">open</span>(<span class="string">&#x27;parpers.json&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">&#x27;title&#x27;</span>]:</span><br><span class="line">            <span class="comment">#将数据保存到文件中</span></span><br><span class="line">            line = json.dumps(<span class="built_in">dict</span>(item))+<span class="string">&quot;\n&quot;</span></span><br><span class="line">            self.file.write(line)</span><br><span class="line">            <span class="keyword">return</span> item </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#DropItem是异常类型，如果不存在就爆出异常，进行丢弃</span></span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">&quot;Missing title in %s&quot;</span>%item)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>编写好的Item Pipeline需要在settings.py(在目录结构中可以找到)中，将类添加到ITEM_PIPELINES变量中。<br>示例代码如下所示：其中的key就是Item Pipeline的类路径，value是自定义数字，当执行Item Pipeline时，按数字大小从低到高依次执行Item Pipeline。通常定义在0-1000范围内。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;spider.pipelines.SpiderPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><h4 id="6-定义Item-Pipeline内置数据存储"><a href="#6-定义Item-Pipeline内置数据存储" class="headerlink" title="6. 定义Item Pipeline内置数据存储"></a>6. 定义Item Pipeline内置数据存储</h4></blockquote></blockquote></blockquote><h4 id="内置文本格式下载方式"><a href="#内置文本格式下载方式" class="headerlink" title="内置文本格式下载方式"></a>内置文本格式下载方式</h4><p>Scrapy提供了一些简单的存储方式。生成一个带有爬取数据的输出文件(feed)。Scrapy的输出自带各种序列化格式。</p><blockquote><p>json<br>JsonItemExporter<br>JSON lines<br>JsonLinesItemExporter<br>CSV<br>CsvItemExporter<br>XML<br>XmlItemExporter<br>Pickle<br>PickleItemExporter<br>Marshal<br>MarshalItemExporter</p></blockquote><p>使用方法：通过命令行使用，例如scrapy crawl xxx -o xxx.csv</p><h4 id="内置图片和文件下载方式"><a href="#内置图片和文件下载方式" class="headerlink" title="内置图片和文件下载方式"></a>内置图片和文件下载方式</h4><p>Scrapy提供了可重用的Item Pipeline(MediaPipeline分为FilesPipeline和ImagesPipeline)，如果要使用ImagesPipeline需要下载pillow模块(pip install pillow)。这类Pipeline具有如下特性。</p><ol><li>避免重新下载最近下载过的数据</li><li>指定存储的位置和方式<br>对于ImagesPipeline还具有额外的特性：</li><li>将所有下载的图片转换为通用的格式(JPG)和模型(RGB)</li><li>缩略图生成</li><li>检测图像的宽/高，确保它们满足最小限制<br>对于要下载的Item会在内部保存一个内部对了，避免多次下载几个Item共享的同一个图片。</li></ol><p>对于FilesPipeline的工作流程：</p><ol><li>在一个爬虫里，抓取一个Item，将文件URL，放入file_urls组内。</li><li>Item从爬虫内返回，进入Item Pipeline</li><li>当Item进入FilesPipeline,file_urls组内的URL将被Scrapy的调度器和下载器安排下载。</li><li>当文件下载完后，另一个字段files将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息。信息包括：下载文件的信息比如下载路径，源抓取地址，图片校验码。如果下载失败，会记录下载错误信息。</li></ol><p>对于ImagesPipeline的工作流程：</p><ol><li>从一个爬虫中，抓取一个Item，把图片的URL放图images_url组内。</li><li>项目从爬虫内返回，进入Item Pieline</li><li>当Item进入ImagesPipelin,images_urls组内的URL将被Scrapy的调度器和下载器安排下载。</li><li>当文件下载完成之后，另一个字段(images)将别更新到结构中。信息包括：下载图片的信息比如下载路径，源抓取地址，图片校验码。如果下载失败，会记录下载错误信息。</li></ol><h4 id="使用FilesPipeline"><a href="#使用FilesPipeline" class="headerlink" title="使用FilesPipeline:"></a>使用FilesPipeline:</h4><blockquote><ol><li>在settings.py文件的ITEM_PIPELINES添加一条’scrapy.pipelines.files.FilesPipeline’:1</li><li>在item添加两个字段，比如</li></ol></blockquote><p>file_urls = scrapy.Filed()<br>files = scrapy.Filed()</p><blockquote><ol><li>在settings.py中添加下载路径FILES_STORE，文件url所在的itme字段FILE_URLS_FILED，和文件信息所在item字段FILES_RESULT_FIELD。例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FILES_STORE = <span class="string">&#x27;G:\\python&#x27;</span></span><br><span class="line">FILES_URLS_FIELD = <span class="string">&#x27;file_urls&#x27;</span></span><br><span class="line">FILES_RESULT_FIELD = <span class="string">&#x27;files&#x27;</span></span><br><span class="line">FILES_EXPIRES = <span class="number">30</span> <span class="comment"># 设置文件过期时间(天)</span></span><br></pre></td></tr></table></figure></li></ol></blockquote><h4 id="使用ImagesPipeline"><a href="#使用ImagesPipeline" class="headerlink" title="使用ImagesPipeline:"></a>使用ImagesPipeline:</h4><blockquote><ol><li>在settings.py文件的ITEM_PIPELINES添加一条’scrapy.pipelines.files.ImagesPipeline’:1</li><li>在item添加两个字段，比如<br>image_urls = scrapy.Filed()<br>images = scrapy.Filed()</li><li>在settings.py中添加下载路径IMAGES_STORE，文件url所在的itme字段IMAGES_URLS_FILED，和文件信息所在item字段IMAGES_RESULT_FIELD。IMAGES_THUMBS制作缩略图，并设置图片大小和尺寸。如果需要过滤特别小的图片可以使用IMAGES_MIN_HEIGHT和IMAGES_MIN_WIDTH来设置图片的最小高和宽。例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE = <span class="string">&#x27;G:\\python&#x27;</span></span><br><span class="line">IMAGES_URLS_FIELD = <span class="string">&#x27;file_urls&#x27;</span></span><br><span class="line">IMAGES_RESULT_FIELD = <span class="string">&#x27;files&#x27;</span></span><br><span class="line">IMAGES_THUMBS = &#123;</span><br><span class="line">    <span class="string">&#x27;small&#x27;</span>:(<span class="number">50</span>,<span class="number">50</span>),</span><br><span class="line">    <span class="string">&#x27;big&#x27;</span>:(<span class="number">270</span>,<span class="number">270</span>)</span><br><span class="line">&#125;</span><br><span class="line">IMAGES_EXPIRES = <span class="number">30</span> <span class="comment"># 设置文件过期时间(天)</span></span><br></pre></td></tr></table></figure></li></ol></blockquote><p>在之前的示例基础上，在setting.py设置如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;spider.pipelines.CnblogspiderPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">&#x27;scrapy.pipelines.images.ImagesPipeline&#x27;</span>:<span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">IMAGES_STORE = <span class="string">&#x27;G:\\python\spider\spider&#x27;</span></span><br><span class="line">IMAGES_URLS_FIELD = <span class="string">&#x27;cimage_urls&#x27;</span></span><br><span class="line">IMAGES_RESULT_FIELE = <span class="string">&#x27;cimages&#x27;</span></span><br><span class="line">IMAGES_EXPIRES = <span class="number">30</span></span><br><span class="line">IMAGES_THUMBS = &#123;</span><br><span class="line">   <span class="string">&#x27;small&#x27;</span>:(<span class="number">50</span>,<span class="number">50</span>),</span><br><span class="line">   <span class="string">&#x27;big&#x27;</span>:(<span class="number">270</span>,<span class="number">270</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>然后在CnblogspiderItem添加两个字段cimage_urls和cimages。<br>修改spider代码，用于下载图片<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def parse(self,response):</span><br><span class="line">       papers = response.xpath(&quot;.//*[@class=&#x27;day&#x27;]&quot;)</span><br><span class="line">       for paper in papers:</span><br><span class="line">           url = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;).extract()[0]</span><br><span class="line">           title = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0]</span><br><span class="line">           time = paper.xpath(&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;).extract()[0]</span><br><span class="line">           content = paper.xpath(&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;).extract()[0]</span><br><span class="line">           item = CnblogspiderItem(url=url,title=title,time=time,content=content)</span><br><span class="line">           request = scrapy.Request(url=url,callback=self.parse_body) #调用Request方法，并设置解析函数</span><br><span class="line">           request.meta[&#x27;item&#x27;] = item #将item暂存</span><br><span class="line">           yield request</span><br><span class="line">       next_page = Selector(response).re(u&#x27;&lt;a href=&quot;(\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;)</span><br><span class="line">       if next_page:</span><br><span class="line">           #Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由</span><br><span class="line">           #谁来解析此项Request请求的响应</span><br><span class="line">           yield scrapy.Request(url=next_page[0],callback=self.parse)</span><br><span class="line"></span><br><span class="line">   def parse_body(self,response):</span><br><span class="line">       item = response.meta[&#x27;item&#x27;]</span><br><span class="line">       body = response.xpath(&quot;.//*[@class=&#x27;postBody&#x27;]&quot;)</span><br><span class="line">       item[&#x27;cimage_urls&#x27;] = body.xpath(&#x27;.//img//@src&#x27;).extract()# 提取图片链接</span><br><span class="line">       yield item</span><br></pre></td></tr></table></figure></p><p>这里容易看出ImagePipeline的执行流程，首先是将图片url放入到image_urls中，然后由Scrapy下载，最后保存到指定的路径。</p><h4 id="自定义FilesPipline和ImagesPipeline"><a href="#自定义FilesPipline和ImagesPipeline" class="headerlink" title="自定义FilesPipline和ImagesPipeline"></a>自定义FilesPipline和ImagesPipeline</h4><p>如果要自定义FilesPipline和ImagesPipeline则需要继承FilesPipeline或者ImagesPipeline，重写<br>get_media_requests和item_completed()方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在工作流程中，管道会得到图片的URL并从项目中下载。需要重写get_media_requests方法.并对各个图片URL返回一个Request。</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_meida_requests</span>(<span class="params">self,item,info</span>):</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span>  <span class="title">MyImagesPipeline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 管道通过get_media_requests接收到图片的URL并从项目中进行下载</span></span><br><span class="line">    <span class="comment"># get_media_request会返回图片URL对应的Request</span></span><br><span class="line">    <span class="comment"># 对于返回的结果results会以二元素的元组列表形式传送到item_completed方法</span></span><br><span class="line">    <span class="comment"># 返回格式</span></span><br><span class="line">    <span class="comment"># success: 布尔值，成功返回True，是啊比返回False</span></span><br><span class="line">    <span class="comment"># url:图片下载的url</span></span><br><span class="line">    <span class="comment"># path:图片存储路径</span></span><br><span class="line">    <span class="comment"># checksum:图片内容的MD5 hash</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">        <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">&#x27;image_urls&#x27;</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(image_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当所有图片请求完成时，item_completed方法将被调用</span></span><br><span class="line">    <span class="comment"># results是get_media_requests下载完成之后的结果。</span></span><br><span class="line">    <span class="comment"># item_completed需要返回一个输出。这个输出会被送到随后的ItemPipelines(在Scrapy中，item的执行顺序会根据id大小依次调用)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self, results, item, info</span>):</span></span><br><span class="line">        image_paths = [x[<span class="string">&#x27;path&#x27;</span>] <span class="keyword">for</span> ok,x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">&quot;Item contains no images&quot;</span>) <span class="comment">#丢弃项目DropItem</span></span><br><span class="line">        item[<span class="string">&#x27;image_paths&#x27;</span>] = image_paths </span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><blockquote><blockquote><blockquote><h4 id="7-启动爬虫"><a href="#7-启动爬虫" class="headerlink" title="7. 启动爬虫"></a>7. 启动爬虫</h4></blockquote></blockquote><ol><li>命令行方式<br>scrapy crawl spider_name </li><li>使用CrawlProcess类<br>CrawlProcess类内部会开启Twisted reactor，配置log,设置Twisted reactor自动关闭。</li></ol></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#初始化CrawlerProcess参数</span></span><br><span class="line">    procerss = CrawlerProcess(&#123;</span><br><span class="line">        <span class="string">&#x27;USER-AGENT&#x27;</span>:<span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0 ; Windows NT 5.1)&#x27;</span></span><br><span class="line">    &#125;)</span><br><span class="line">    procerss.crawl(CnblogsSpider) <span class="comment">#运行爬虫</span></span><br><span class="line">    procerss.start()</span><br></pre></td></tr></table></figure><p>带启动参数的CrawlProcess类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">procerss = CrawlerProcess(get_project_settings())</span><br><span class="line">procerss.crawl(CnblogsSpider)  <span class="comment"># 运行爬虫</span></span><br><span class="line">procerss.start()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><blockquote><ol><li>使用CrawlerRunner</li></ol></blockquote><p>CrawlerRunner：在spider结束后，必须自行关闭Twisted reactor 需要在CrawlerRunner.crawl所返回的对象中添加回调函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">configure_logging(&#123;<span class="string">&#x27;LOG_FORMAT&#x27;</span>:<span class="string">&#x27;%(levelname)s：%(message)s&#x27;</span>&#125;)</span><br><span class="line">  runner = CrawlerRunner()</span><br><span class="line">  d = runner.crawl(CnblogsSpider)</span><br><span class="line">  d.addBoth(<span class="keyword">lambda</span> _:reactor.stop())</span><br><span class="line">  reactor.run()</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><h4 id="8-强化爬虫"><a href="#8-强化爬虫" class="headerlink" title="8. 强化爬虫"></a>8. 强化爬虫</h4></blockquote></blockquote></blockquote><p>这里主要是介绍关于Scrapy的调试方法和异常，控制运行状态等内容。</p><blockquote><h5 id="1-调试方法"><a href="#1-调试方法" class="headerlink" title="1. 调试方法"></a>1. 调试方法</h5></blockquote><h4 id="Parse命令"><a href="#Parse命令" class="headerlink" title="Parse命令"></a>Parse命令</h4><p>检查Spider输出的最基本方法(Parse命令)，可以在函数层上检查spider各个部分的效果。<br>使用方法为scrapy parse —spider==spider_name -c parse -d 2<item_url><br>例如 scrapy parse —spider==cblogs -c parse -d”<a href="https://www.baidu.com">https://www.baidu.com</a>“</p><h4 id="Scrapy-shell"><a href="#Scrapy-shell" class="headerlink" title="Scrapy shell"></a>Scrapy shell</h4><p>通过Scrapy shell 可以查看spider某个位置中被处理的response，以确定期望的response是否到达特定位置。<br>在spider类中，添加scrapy.shell.inspect_response方法。当程序运行到inspect_response方法时，会暂停，并切换进shell中，方便进行调试。如果要退出终端可以ctrl+d进行退出。代码如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self,response</span>):</span></span><br><span class="line">       papers = response.xpath(<span class="string">&quot;.//*[@class=&#x27;day&#x27;]&quot;</span>)</span><br><span class="line">       <span class="comment">#添加scrapy shell</span></span><br><span class="line"></span><br><span class="line">       <span class="keyword">from</span> scrapy.shell <span class="keyword">import</span> inspect_response</span><br><span class="line">       inspect_response(response,self)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">           url = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/@href&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">           title = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">           time = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;dayTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">           content = paper.xpath(<span class="string">&quot;.//*[@class=&#x27;postTitle&#x27;]/a/text()&quot;</span>).extract()[<span class="number">0</span>]</span><br><span class="line">           item = CnblogspiderItem(url=url,title=title,time=time,content=content)</span><br><span class="line">           request = scrapy.Request(url=url,callback=self.parse_body) <span class="comment">#调用Request方法，并设置解析函数</span></span><br><span class="line">           request.meta[<span class="string">&#x27;item&#x27;</span>] = item <span class="comment">#将item暂存</span></span><br><span class="line">           <span class="keyword">yield</span> request</span><br><span class="line">       next_page = Selector(response).re(<span class="string">u&#x27;&lt;a href=&quot;(\S*)&quot;&gt;下一页&lt;/a&gt;&#x27;</span>)</span><br><span class="line">       <span class="keyword">if</span> next_page:</span><br><span class="line">           <span class="comment">#Request对象中的URL为请求链接，callback为回调方法，回调方法用于指定由</span></span><br><span class="line">           <span class="comment">#谁来解析此项Request请求的响应</span></span><br><span class="line">           <span class="keyword">yield</span> scrapy.Request(url=next_page[<span class="number">0</span>],callback=self.parse)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse_body</span>(<span class="params">self,response</span>):</span></span><br><span class="line">       item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">       body = response.xpath(<span class="string">&quot;.//*[@class=&#x27;postBody&#x27;]&quot;</span>)</span><br><span class="line">       item[<span class="string">&#x27;cimage_urls&#x27;</span>] = body.xpath(<span class="string">&#x27;.//img//@src&#x27;</span>).extract()<span class="comment"># 提取图片链接</span></span><br><span class="line">       <span class="keyword">yield</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h4 id="loggin"><a href="#loggin" class="headerlink" title="loggin"></a>loggin</h4><p>通过运行后的记录查看爬虫的运行状态</p><h4 id="编译器"><a href="#编译器" class="headerlink" title="编译器"></a>编译器</h4><p>借助编译器提供的Debug进行断点调试，查看程序运行情况。</p><blockquote><h5 id="2-异常"><a href="#2-异常" class="headerlink" title="2. 异常"></a>2. 异常</h5></blockquote><p>之前已经使用过以恶异常也就是抛弃Item,DropItem。下图是关于Scrapy异常的介绍。<br><img src="/images/scrapy-e1.PNG" alt="566"></p><p><img src="/images/scrapy-e2.PNG" alt="567"></p><blockquote><h5 id="3-控制运行状态"><a href="#3-控制运行状态" class="headerlink" title="3. 控制运行状态"></a>3. 控制运行状态</h5></blockquote><p>通过telnet访问Scrapy终端。<br>talnet localhost 6023<br>进入终端后，通过est()可以查看scrapy运行情况。<br>通过engine/pause()暂停运行，通过engine.unpause()继续运行。<br>通过engine.stop()停止运行。<br>配置telnet，在setting.py中配置IP和端口。<br>TELNETCONSOLE_PORT:6023 #设置为0或者None动态分配端口。<br>TELNETCONSOLE_HOST:’127.0.0.1’ #y也就是本地地址ip</p><h6 id="7-实战项目：Scrapy爬虫"><a href="#7-实战项目：Scrapy爬虫" class="headerlink" title="7. 实战项目：Scrapy爬虫"></a>7.<a id="seven"></a> 实战项目：Scrapy爬虫</h6>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-数据存储&quot;&gt;&lt;a href=&quot;#1-数据存储&quot; class=&quot;headerlink&quot; title=&quot;1.数据存储&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#one&quot;&gt;1.数据存储&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;2-动态文件抓取&quot;&gt;&lt;a href=&quot;#2-动态文件抓取&quot; 
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫开发与项目实战-第二回合（实战）</title>
    <link href="http://example.com/wiki/%E7%A8%8B%E5%BA%8F%E6%8A%80%E6%9C%AF/Python/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB2.1/"/>
    <id>http://example.com/wiki/程序技术/Python/爬虫/Python爬虫2.1/</id>
    <published>2021-11-18T08:35:25.519Z</published>
    <updated>2021-11-22T02:19:31.513Z</updated>
    
    <content type="html"><![CDATA[<p>爬虫的原理十分简单，通过URL，获取网页资源，再根据网页资源进一步获取需要的信息数据。</p><p>我们很容易借助requests,beautifulSoup库等实现一个简答的爬虫。实际上，到了这基本上爬虫的大概就已经学习完毕了。对于我们现在编写的爬虫与大型爬虫的差距在于以下几点：</p><blockquote><ol><li>实现方式</li><li>优化方式</li><li>稳健性<br>我们现在仅仅只考虑功能的实现，所以还只是基础爬虫，要实现一个大型的爬虫还因该从效率，稳健性，结构化，维护等方面综合考虑。</li></ol></blockquote><p><img src="/images/基础爬虫.PNG" alt="a1"></p><p>一个爬虫的基础框架可以分为：爬虫调度器，URL管理器，HTML下载器，HTML解析器，数据存储器</p><ol><li><p>爬虫调度器  负责其他四个模块的协调工作</p></li><li><p>URL管理器主要负责URL链接，维护已经爬取的URL集合和未爬取的URL集合，提供互殴去新的URL链接的接口</p></li><li><p>HTML下载器用于从URL管理器中获取未爬取的URL链接并下载HTML页面</p></li><li><p>HTML解析器用于从HTML下载器中获取已经下载的HTML页面，并从中解析出新的URL链接交给URL管理器<br>解析出有效数据交给数据存储器。</p></li><li><p>数据存储器用于将HTML解析器解析出来的数据通过文件或数据库的形式存储起来。</p></li></ol><p>爬虫的动态流程如下所示：<br><img src="/images/爬虫时序图.PNG" alt="a2"></p><h6 id="百度百科词条爬取项目"><a href="#百度百科词条爬取项目" class="headerlink" title="百度百科词条爬取项目"></a>百度百科词条爬取项目</h6><blockquote><h3 id="URL管理器"><a href="#URL管理器" class="headerlink" title="URL管理器"></a>URL管理器</h3></blockquote><h6 id="简单分布式爬虫"><a href="#简单分布式爬虫" class="headerlink" title="简单分布式爬虫"></a>简单分布式爬虫</h6><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>这里实现简单的分布式爬虫，采用主从模式。目前大型的爬虫都采用分布式爬取，所以通过此次实践加深对分布式爬虫的理解。分布式需要考虑如何设计结构，保证各个节点稳定高效地运作。</p><h4 id="主从模式"><a href="#主从模式" class="headerlink" title="主从模式"></a>主从模式</h4><p>主从模式是指由一台主机作为控制节点，负责管理所有运行网络爬虫的主机，爬虫只需要从控制节点那里接收任务，并把新<br>生成任务提交给控制节点就可以了，在这个过程中不必与其它爬虫通信。</p><blockquote><ol><li>采用主从模式，实现简单，利于管理。而控制节点则需要与所有爬虫进行通信。</li><li>主从模式的缺陷在于，控制节点会成为整个系统的瓶颈，容易导致整个分布式网络爬虫系统性能下降。</li></ol></blockquote><p>主从模式的结构如下所示(以一台主机和两台从机为例)：控制节点（ControlNode）主要分为URL管理器，数据存储器和控制调度器。(1)控制调度器通过三个进程来协调URL管理器和数据存储器的工作：(2)一个是URL管理进程，负责URL的管理和将URL传递给爬虫节点；(3)一个是数据提取进程，负责读取爬虫节点返回的数据，将返回数据中的URL交给URL管理进程，将标题和摘要等数据交给数据存储进程；最后一个是数据存储进程，负责将数据提取进程中提交的数据进行本地存储。</p><p>对于爬虫节点(SpdierNode):包含爬虫调度器，HTML下载器，HTML解析器，主要负责对URL进行爬取，下载，然后将新的URL,data返回给控制节点(ControlNode)。<br>爬虫调度器的执行流程：</p><ol><li>爬虫调度器从控制节点中的url_q队列读取URL</li><li>爬虫调度器调用HTML下载器，HTML解析器获取网页中新的URL和标题摘要</li><li>爬虫调度器将新的URL和标题摘要传入result_q队列交给控制节点</li></ol><p><img src="/images/主从.PNG" alt="a21"><br><img src="/images/控制节点.PNG" alt="a22"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;爬虫的原理十分简单，通过URL，获取网页资源，再根据网页资源进一步获取需要的信息数据。&lt;/p&gt;
&lt;p&gt;我们很容易借助requests,beautifulSoup库等实现一个简答的爬虫。实际上，到了这基本上爬虫的大概就已经学习完毕了。对于我们现在编写的爬虫与大型爬虫的差距在于
      
    
    </summary>
    
      <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="http://example.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://example.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
