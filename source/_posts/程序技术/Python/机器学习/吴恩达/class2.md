---
title: 吴恩达-深度学习——二
categories:
    -  深度学习
tags:
    - 深度学习
---

#### 1. 逻辑回归（Logistic Regression）

逻辑回归就是二分类问题，假设y的结果为0或1，那么逻辑回归就是寻找一个函数，根据输入x确定y是1的概率。也就是f(x) = P(y=1|x) 。
其中x是$R^{n_x}$(也就是$n_x$维向量)，y是{0，1}。

通常在做法是将$w^Tx+b$作为x的线性函数，并将结果标准化到0-1之间。

>Sigmoid 函数

Sigmoid函数原型:$\sigma(z) = \frac{1}{1+e^{-z}}$。
通过Sigmoid函数就可以将结果标准化到0-1之间。

从上面可以看出，如果要实现逻辑回归，最主要的是学习参数$w^T$和b,用于更好的对结果为1的概率进行预测。

>损失函数/误差函数

损失函数用于训练参数$W^T$和b，可以衡量算法的运行情况。损失函数可以用于观察预测结果和实际结果的差距多少。
损失函数衡量的是单个训练样本的预测情况。

1. 通过预测结果和实际值差的平方作为损失函数

>$\zeta(y_{预测},y_{标准})=\frac{1}{2}(y_{预测}-y_{标准})^2$
>并不推荐使用这种方法

2. 通过 $\zeta(y_{预测},y_{标准})=-(ylogy_{预测}+(1-y)log(1-y_{预测}))$作为损失函数。

其中y_{预测}是经过Sigmoid后的结果。
通过这个损失函数，可以使得当y=1的时候，让y预测接近1，当y=0时，让y预测接近0。

>成本函数（Cost function）

成本函数衡量的是全体训练样本的情况。
成本函数J（w,b） = $\frac{1}{m}\sum^m_{i=1}\zeta(y_{预测},y_{标准}}})$

推到过程:假设 y=1的概率为a,y=0的概率为1-a，那么对于函数p(y|x)就可以写成：
p(y|x) = a^y*(1-a)^(1-y);将y=1和y=0带入即可验证。然后根据log函数的单调递增性，对p(y|x)求对数。得到 logp(y|x) = -1*$\zeta(y_{预测},y_{标准})$.由于log函数单增，所以要找到p(y|x)最值，也就是找到$\zeta(y_{预测},y_{标准})$的最小值。然后根据极大似然估计，得到成本函数：J（w,b） = $\frac{1}{m}\sum^m_{i=1}\zeta(y_{预测},y_{标准}}})$。


这个成本函数是凸的

>### tags:

>1. 如果定义损失函数为$\zeta(y_{预测},y_{标准})=\frac{1}{2}(y_{预测}-y_{标准})^2$

这样会导致无法找到全局最优解，只能找到局部的最优解。


#### 梯度下降法

逻辑回归的工作就是要找到合适的w,b使得成本函数J（w,b）达到最小值。

>凸函数

这里的凸函数和国定定义的凹凸性定义相反（也就是实际上等同于高数中的凹U的形状）。
凸函数是一个定义在某个向量空间的凸子集C上的实值函数f，而且对于凸子集C中任意两个向量$x_1,x_2$、有$f((x1+x2)/2)<=(f(x1)+f(x2))/2$成立。
凸函数有一个重要的性质：对于凸函数来说，局部最小值就是全局最小值。

logistic回归采用J（w,b）作为成本函数就是因为这个函数J（w,b）是凸函数，由于凸函数的局部最小值就是全局最小值，所以能够保证所得到的结果是最优的。
而如果不是凸函数，则可能得到的结果只是局部最优的。

>梯度下降法

梯度下降法也就是随机初始一个w,b。然后在每一次迭代的过程中，沿着下降速度最快的方向下降。由高等数学知识可知，方向导数最大的方向称为梯度。所以也就是沿着梯度方向下降，这样就能保证下降的方向最快。

>执行流程

不断执行下面的权值和偏置的更新过程，以达到最优解。

w = w - $\alpha\frac{\phi_{J(w,b)}}{\phi_{w}}$

b = b - $\alpha\frac{\phi_{J(w,b)}}{\phi_{b}}$

$\alpha$: 学习率(learning rate)
$\phi$: 偏导符号，读作round


#### 计算图

前向传播：forward
反向传播: backpropagation

简单介绍一下梯度下降求解的过程:
假设输入x = [x1,x2], w = [w1,w2],b。
逻辑回归函数：
    $z = w^T*x+b $ , $y_{预测}=\sigma(z)=a$,$L(a,y_{标准})=-(yloga+(1-y)log(1-a))$

反向传播计算梯度的过程：

>1. 计算损失函数对a的偏导

$d_a = d_{L(a,b)}/d_a$ = $-y/a+(1-y)/(1-a)$

>2. 计算损失函数对z的偏导

这里根据链式求导规则，可以得出$d_L/d_z$ = $(d_L/d_a)*(d_a/d_z)$,由此可得：$d_L/d_z$=a(1-a).

>3. 求损失函数对各个变量的偏导

$\phi_L/\phi_{w1}=d_{w_1} =x_1d_z $
以此类推可以得出对各个变量的偏导。

>4. 根据求出的偏导，进行更新参数

x = x - $\alpha*d$


#### 向量化技术

向量化技术用于加速运算速度，摆脱for循环(使用for循环，效率会很低)。

>什么是向量化技术？

简单的说就是通过矩阵进行运算(向量可以看成一个1 x n 维的矩阵)，然后进行矩阵运算就可以快速进行若干次的操作。
就以 $z = w^T*x+b $为例，正常是通过双重for循环进行遍历w和t进行求和运算，现在只需要通过矩阵的乘法运算一次性可以得出结果。
在计算机内部有专门的矩阵运算器，用于处理矩阵的运算，所以通过矩阵运算可以提高运算速度。


向量化能够加快运算速度，一般情况下，不要用显示的循环解决问题。能不用就不用，尽量寻找其他方法进行解决(通常这种方法可以并行运算，提高运行速度)。


在利用Python实现向量化技术的时候，我们可以借助numpy库的许多函数和方法，来完成向量的运算。


#### 利用python实现Logistic回归前向传播和反向传播

![梯度](/images/deep/梯度更新参数.PNG)


$Z = w^TX+b=np.dot(w^T,X)+b$
$A=\sigma(Z)$
$d_z = A-Y$
$d_w=\frac{1}{m}Xd_{z^T}$
$d_b=\frac{1}{m}np.sum(d_z)$



#### Python和Numpy中的广播（broadcasting）

简单的说，假设有一个矩阵是mXn，如果需要这个矩阵和一个实数R或者1Xn或者mX1的矩阵进行操作。Python会自动的将实数R变为mXn的矩阵，以及将1 Xn或mX1的矩阵变成mXn的矩阵(复制m或复制n次)，用于操作。

举一个简单的例子：加入有个1X3的矩阵A,有个3X1的矩阵B，在Python中A+B的结果是一个3X3的矩阵。
他会把A复制3次变为3X3的矩阵，然后将B复制3次变为3X3的矩阵。

如何避免发生由于广播而产生的错误：
>1. 不使用秩为1的数组 ， 也就是shape(s1,)的数组
尽量创建shape为(1,n)或(n,1)的列或行向量。
>2. 确保自己的向量是自己需要的shape,可以通过reshape或assert等方法来确保在程序的运行过程中，矩阵是自己想要的shape

#### tips

GPU: 图像处理单元（Graphics Processing Unit）
